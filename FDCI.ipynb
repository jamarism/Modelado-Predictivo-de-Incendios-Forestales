{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "oNMcdsqVvD90",
        "outputId": "15962d44-3128-48cf-b552-72e57f341acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✔️ Fuente registrada y activa: Palatino Linotype\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8a0f73b38b5115c497.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8a0f73b38b5115c497.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# ======================================================================\n",
        "# 0. INSTALACIÓN\n",
        "# ======================================================================\n",
        "!pip -q install earthengine-api gradio geopandas numpy matplotlib rasterio \\\n",
        "                pillow folium branca scipy scikit-learn pyarrow fastparquet\n",
        "\n",
        "# ======================================================================\n",
        "# 1. IMPORTAR LIBRERÍAS\n",
        "# ======================================================================\n",
        "import ee, os, io, base64, traceback, re, warnings, math, calendar\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "import rasterio, rasterio.warp\n",
        "from rasterio.enums import Resampling\n",
        "from rasterio import transform\n",
        "from PIL import Image\n",
        "\n",
        "import folium, branca, gradio as gr\n",
        "from branca.element import Template, MacroElement\n",
        "from google.colab import auth, drive\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# MONTAJE INICIAL DE GOOGLE DRIVE\n",
        "# ----------------------------------------------------------------------\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ======================================================================\n",
        "# 1-bis. CONFIGURAR FUENTE PERSONALIZADA (Palatino) + Seaborn (estilo unificado)\n",
        "# ======================================================================\n",
        "font_path = \"/content/drive/My Drive/fonts/palatino-linotype.ttf\"\n",
        "if not Path(font_path).exists():\n",
        "    raise FileNotFoundError(f\"No se encontró el archivo: {font_path}\")\n",
        "\n",
        "fm.fontManager.addfont(font_path)\n",
        "font_name = fm.FontProperties(fname=font_path).get_name()\n",
        "\n",
        "# Estilo global de figuras (ticks, tipografías, tamaños).\n",
        "mpl.rcParams.update({\n",
        "    \"font.family\":  \"serif\",\n",
        "    \"font.serif\":   [font_name],\n",
        "    \"pdf.fonttype\": 42,\n",
        "    \"ps.fonttype\":  42,\n",
        "    \"axes.unicode_minus\": False,\n",
        "    \"axes.titleweight\": \"semibold\",\n",
        "    \"axes.labelsize\": 12,\n",
        "    \"axes.titlesize\": 13,\n",
        "    \"xtick.labelsize\": 11,\n",
        "    \"ytick.labelsize\": 11,\n",
        "    \"figure.dpi\": 120,\n",
        "})\n",
        "\n",
        "sns.set_theme(\n",
        "    style=\"ticks\",\n",
        "    context=\"notebook\",\n",
        "    rc={\n",
        "        \"font.family\": \"serif\",\n",
        "        \"font.serif\":  [font_name],\n",
        "        \"axes.titleweight\": \"semibold\",\n",
        "        \"figure.dpi\": 120,\n",
        "    },\n",
        ")\n",
        "# Paleta de colores consistente.\n",
        "BASE_PALETTE = sns.color_palette(\"deep\")\n",
        "COLOR_ACCENT = BASE_PALETTE[7]   # para barras e histogramas + hexbin fill\n",
        "COLOR_WET    = BASE_PALETTE[0]   # azul\n",
        "COLOR_DRY    = BASE_PALETTE[3]   # rojo\n",
        "COLOR_CURVE_W= BASE_PALETTE[0]\n",
        "COLOR_CURVE_D= BASE_PALETTE[3]\n",
        "COLOR_SCATTER= \"#6e6e6e\"         # gris\n",
        "\n",
        "print(f\"✔️ Fuente registrada y activa: {font_name}\")\n",
        "\n",
        "# ======================================================================\n",
        "# 2. PARÁMETROS\n",
        "# ======================================================================\n",
        "SCALE_TARGET  = 500\n",
        "EXPORT_FOLDER = 'MODIS_Composite'\n",
        "MAX_TASKS     = 400\n",
        "\n",
        "NDVI_RANGE = (-1.0, 1.0)\n",
        "LST_RANGE  = (-50.0, 60.0)\n",
        "\n",
        "FIRMS_COLLECTION = 'FIRMS'\n",
        "\n",
        "NDVI_SENTINEL = -32768\n",
        "LST_SENTINEL  = -32768\n",
        "QC_FOLDER     = f\"/content/drive/My Drive/{EXPORT_FOLDER}\"\n",
        "\n",
        "# ======================================================================\n",
        "# 3. EARTH ENGINE\n",
        "# ======================================================================\n",
        "auth.authenticate_user()\n",
        "ee.Initialize(project='ee-example')  #añadir Cloud Project\n",
        "\n",
        "# ======================================================================\n",
        "# 4. REGIÓN DE INTERÉS (Boyacá + Cundinamarca)\n",
        "# ======================================================================\n",
        "def _get_regions():\n",
        "    fc = (ee.FeatureCollection(\"FAO/GAUL/2015/level1\")\n",
        "          .filter(ee.Filter.inList('ADM1_NAME', ['Cundinamarca', 'Boyaca'])))\n",
        "    return fc, fc.union().geometry()\n",
        "\n",
        "# ======================================================================\n",
        "# 5-a. NDVI y 5-b. LST (colecciones)\n",
        "# ======================================================================\n",
        "# Carga NDVI MODIS; filtros temporales/espaciales y dtypes.\n",
        "def _get_ndvi_comp(start, end, geom):\n",
        "    def _scale(img):\n",
        "        nd = img.select('NDVI').multiply(0.0001).rename('NDVI')\n",
        "        nd = nd.updateMask(nd.gte(-1).And(nd.lte(1)))\n",
        "        return ee.Image(nd.copyProperties(img, ['system:time_start']))\n",
        "    return (ee.ImageCollection('MODIS/061/MOD13A1')\n",
        "            .filterDate(start, end).filterBounds(geom).map(_scale))\n",
        "\n",
        "# Carga LST MODIS; normaliza y asegura máscara.\n",
        "def _get_lst_comp(start, end, geom):\n",
        "    def _scale(img):\n",
        "        return (img.multiply(0.1).rename('LST')\n",
        "                   .copyProperties(img, ['system:time_start']))\n",
        "    return (ee.ImageCollection('projects/sat-io/open-datasets/gap-filled-lst/gf_day_1km')\n",
        "              .filterDate(start, end)\n",
        "              .filterBounds(geom)\n",
        "              .map(_scale))\n",
        "\n",
        "# ======================================================================\n",
        "# 5-c. HAZARD (MCD12Q1)\n",
        "# ======================================================================\n",
        "# HAZARD: peso por clase VT desde tabla determinista\n",
        "HAZARD_WEIGHTS = { 0:0, 1:0.85, 2:0.6, 3:0.6, 4:0.6, 5:0.82, 6:0.72, 7:0.4,\n",
        "                   8:0.8, 9:0.8, 10:0.5, 11:0.1, 12:0.35, 13:0.05, 14:0.48,\n",
        "                   15:0.3, 16:0.12, 17:0 }\n",
        "def _get_hazard_img(year, geom):\n",
        "    lc = (ee.ImageCollection('MODIS/061/MCD12Q1')\n",
        "          .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
        "          .first().select('LC_Type1'))\n",
        "    keys, vals = list(HAZARD_WEIGHTS.keys()), list(HAZARD_WEIGHTS.values())\n",
        "    return (lc.remap(keys, vals)\n",
        "              .rename('HAZARD')\n",
        "              .updateMask(lc.mask())\n",
        "              .clip(geom))\n",
        "\n",
        "# ======================================================================\n",
        "# 5-c-bis. VT (clase de cobertura anual) desde MCD12Q1 · LC_Type1 (1..17)\n",
        "# ======================================================================\n",
        "def _get_vt_img(year:int, geom):\n",
        "    \"\"\"\n",
        "    Devuelve la imagen anual LC_Type1 (1..17) para el año dado, recortada a geom.\n",
        "    No aplica pesos; es la clase cruda (VT).\n",
        "    \"\"\"\n",
        "    vt = (ee.ImageCollection('MODIS/061/MCD12Q1')\n",
        "          .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
        "          .first()\n",
        "          .select('LC_Type1')\n",
        "          .clip(geom))\n",
        "    vt = vt.toInt16().updateMask(vt.mask())\n",
        "    return vt.rename('VT')\n",
        "\n",
        "# ======================================================================\n",
        "# 5-c-ter. Nombres oficiales GEE para LC_Type1 (MCD12Q1, esquema IGBP)\n",
        "# ======================================================================\n",
        "VT_CLASS_NAMES = {\n",
        "    1:  \"Evergreen Needleleaf Forest\",\n",
        "    2:  \"Evergreen Broadleaf Forest\",\n",
        "    3:  \"Deciduous Needleleaf Forest\",\n",
        "    4:  \"Deciduous Broadleaf Forest\",\n",
        "    5:  \"Mixed Forests\",\n",
        "    6:  \"Closed Shrublands\",\n",
        "    7:  \"Open Shrublands\",\n",
        "    8:  \"Woody Savannas\",\n",
        "    9:  \"Savannas\",\n",
        "    10: \"Grasslands\",\n",
        "    11: \"Permanent Wetlands\",\n",
        "    12: \"Croplands\",\n",
        "    13: \"Urban and Built-Up\",\n",
        "    14: \"Cropland/Natural Vegetation Mosaics\",\n",
        "    15: \"Permanent Snow and Ice\",\n",
        "    16: \"Barren\",\n",
        "    17: \"Water Bodies\"\n",
        "}\n",
        "VT_VALID = set(VT_CLASS_NAMES.keys())\n",
        "\n",
        "# ======================================================================\n",
        "# 5-c-quater. Tabla HAZARD por cobertura (editable en UI) + utilidades\n",
        "# ======================================================================\n",
        "HAZARD_TABLE_DEFAULT = pd.DataFrame({\n",
        "    \"Code\":        [ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
        "    \"LandCover\":   [\"Evergreen Broadleaf Forest\",\"Deciduous Broadleaf Forest\",\"Mixed Forests\",\n",
        "                    \"Closed Shrublands\",\"Open Shrublands\",\"Woody Savannas\",\"Savannas\",\n",
        "                    \"Grasslands\",\"Permanent Wetlands\",\"Croplands\",\"Urban and Built-Up\",\n",
        "                    \"Cropland/Natural Vegetation Mosaics\",\"Permanent Snow and Ice\",\"Barren\",\"Water Bodies\"],\n",
        "    \"FireDensity\": [0.1288,0.0000,0.0000,0.1279,0.0000,0.2046,0.2032,0.3430,0.1943,1.8119,0.3716,1.1816,0.0000,0.0000,0.0000],\n",
        "    \"HazardIndex\": [0.0711,0.0000,0.0000,0.0706,0.0000,0.1129,0.1122,0.1893,0.1072,1.0000,0.2051,0.6521,0.0000,0.0000,0.0000],\n",
        "}).astype({\"Code\":\"int32\"})\n",
        "\n",
        "def vt_table_to_map(df: pd.DataFrame) -> dict:\n",
        "    \"\"\"Convierte la tabla editable a dict {code: hazard in [0,1]}.\"\"\"\n",
        "    d = {}\n",
        "    for _, r in df.iterrows():\n",
        "        try:\n",
        "            c = int(r[\"Code\"])\n",
        "            v = float(r[\"HazardIndex\"])\n",
        "            d[c] = float(np.clip(v, 0.0, 1.0))\n",
        "        except Exception:\n",
        "            continue\n",
        "    for k in VT_CLASS_NAMES:\n",
        "        d.setdefault(k, 0.0)\n",
        "    return d\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# 5-d. Exportar extremos — año único + paso 0.01\n",
        "# ======================================================================\n",
        "# Exportación estable (Parquet/CSV)\n",
        "def _export_extreme_pairs_adaptive_fast(year,\n",
        "                                        step=0.01,\n",
        "                                        p_low=2,\n",
        "                                        p_high=98,\n",
        "                                        ndvi_lag_days=8,\n",
        "                                        scale_m=SCALE_TARGET,\n",
        "                                        add_coords=False):\n",
        "    try:\n",
        "        print(f\"[{year}] Iniciando exportaciones (step={step})…\")\n",
        "\n",
        "        start, end = f\"{year}-01-01\", f\"{year}-12-31\"\n",
        "        _, geom = _get_regions()\n",
        "        nd_ic = _get_ndvi_comp(start, end, geom)\n",
        "        ls_ic = _get_lst_comp(start, end, geom)\n",
        "\n",
        "        diff_ms = int(ndvi_lag_days * 86_400_000)\n",
        "        joined = ee.Join.saveBest('LST', 'tDiff').apply(\n",
        "            nd_ic, ls_ic,\n",
        "            ee.Filter.maxDifference(diff_ms,\n",
        "                leftField='system:time_start',\n",
        "                rightField='system:time_start'))\n",
        "\n",
        "        def _stack(img):\n",
        "            ls = ee.Image(img.get('LST')).select('LST')\n",
        "            return (ee.Image(img).select('NDVI').addBands(ls)\n",
        "                    .copyProperties(img, ['system:time_start']))\n",
        "        pairs = ee.ImageCollection(joined.map(_stack))\n",
        "\n",
        "        dec = len(str(step).split('.')[1]) if '.' in str(step) else 0\n",
        "        n_bins = int(round((1.0 + 0.2) / step))\n",
        "        bins = [(round(i*step - 0.2, dec), round((i+1)*step - 0.2, dec))\n",
        "                for i in range(n_bins)]\n",
        "        if bins[-1][1] < 1.0:\n",
        "            bins[-1] = (bins[-1][0], round(1.0, dec))\n",
        "\n",
        "        for lo_nd, hi_nd in bins:\n",
        "            sub = pairs.map(lambda im: im.updateMask(\n",
        "                im.select('NDVI').gte(lo_nd).And(im.select('NDVI').lt(hi_nd))))\n",
        "\n",
        "            stats  = sub.select('LST').toBands().reduceRegion(\n",
        "                        ee.Reducer.percentile([p_low, p_high]),\n",
        "                        geom, scale_m, bestEffort=True, maxPixels=1e13)\n",
        "\n",
        "            vals   = stats.values()\n",
        "            lst_lo = ee.Number(vals.get(0))\n",
        "            lst_hi = ee.Number(vals.get(1))\n",
        "\n",
        "            def _to_tbl(im):\n",
        "                m = im.select('LST').lte(lst_lo).Or(im.select('LST').gte(lst_hi))\n",
        "                im2 = im.updateMask(m)\n",
        "                date = ee.Date(im.get('system:time_start')).format('YYYY-MM-dd')\n",
        "                bands = ['NDVI','LST']\n",
        "                if add_coords:\n",
        "                    im2 = im2.addBands(ee.Image.pixelLonLat()); bands+=['longitude','latitude']\n",
        "                return (im2.select(bands)\n",
        "                           .sample(region=geom, scale=scale_m, geometries=False)\n",
        "                           .map(lambda f: f.set('date', date)))\n",
        "\n",
        "            tbl = sub.map(_to_tbl).flatten()\n",
        "\n",
        "            tag = (f'EXT_{year}_BIN{\"m\" if lo_nd<0 else \"p\"}'\n",
        "                   f'{abs(lo_nd):.{dec}f}-{\"m\" if hi_nd<0 else \"p\"}'\n",
        "                   f'{abs(hi_nd):.{dec}f}')\n",
        "            ee.batch.Export.table.toDrive(\n",
        "                collection=tbl, description=tag, folder=EXPORT_FOLDER,\n",
        "                fileNamePrefix=tag, fileFormat='CSV').start()\n",
        "            print(f\"[{year}] ➜ Tarea iniciada: {tag}.csv\")\n",
        "\n",
        "        return f\"[{year}] Todas las tareas fueron lanzadas.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        msg = f\"[{year}] Error general: {e}\\n{traceback.format_exc(limit=1)}\"\n",
        "        print(msg); return msg\n",
        "\n",
        "def export_extremes_single_year(year_sel):\n",
        "    year = int(year_sel)\n",
        "    if year < 2003 or year > 2020:\n",
        "        return \"Error: el año debe estar entre 2003 y 2020.\"\n",
        "    return _export_extreme_pairs_adaptive_fast(year, step=0.01, add_coords=False)\n",
        "\n",
        "# ======================================================================\n",
        "# 5-f. CONCATENAR TODOS LOS CSV “EXT_*” DE UN AÑO\n",
        "# ======================================================================\n",
        "# Alineación temporal/espacial; preserva llaves y máscaras.\n",
        "def merge_extreme_csv(year_sel):\n",
        "    try:\n",
        "        year = int(year_sel)\n",
        "        folder = f'/content/drive/My Drive/{EXPORT_FOLDER}'\n",
        "        pattern = re.compile(rf'^EXT_{year}_BIN.*\\.csv$')\n",
        "        parts = [os.path.join(folder, f) for f in os.listdir(folder)\n",
        "                 if pattern.match(f)]\n",
        "        if not parts:\n",
        "            return f\"No se encontraron archivos EXT_{year}_BIN*.csv\"\n",
        "        dfs = [pd.read_csv(p) for p in sorted(parts)]\n",
        "        all_df = pd.concat(dfs, ignore_index=True)\n",
        "        out_name = f'EXT_ALL_NDVI_LST_{year}.csv'\n",
        "        out_path = os.path.join(folder, out_name)\n",
        "        all_df.to_csv(out_path, index=False)\n",
        "        if all_df.empty:\n",
        "            return \"Falló la concatenación: DataFrame vacío.\"\n",
        "        for p in parts:\n",
        "            os.remove(p)\n",
        "        return (f\"✅ {len(parts)} archivos ➜ {out_name} \"\n",
        "                f\"({len(all_df)} filas). Partes eliminadas.\")\n",
        "    except Exception as e:\n",
        "        return f\"Error unificando CSV: {e}\\n{traceback.format_exc(limit=1)}\"\n",
        "\n",
        "# ======================================================================\n",
        "# 5-g. EXPORTAR SERIE DIARIA COMPLETA (NDVI/LST/FIRMS/VT)\n",
        "# ======================================================================\n",
        "def _get_ndvi_raw(start, end, geom):\n",
        "    return (ee.ImageCollection('MODIS/061/MOD13A1')\n",
        "            .filterDate(start, end)\n",
        "            .filterBounds(geom)\n",
        "            .select('NDVI'))\n",
        "\n",
        "def _get_lst_raw(start, end, geom):\n",
        "    return (ee.ImageCollection('projects/sat-io/open-datasets/gap-filled-lst/gf_day_1km')\n",
        "            .filterDate(start, end)\n",
        "            .filterBounds(geom))\n",
        "\n",
        "def _firms_conf_for_date(date_str, nd_proj, geom):\n",
        "    d0 = ee.Date(date_str)\n",
        "    d1 = d0.advance(1, 'day')\n",
        "    col = (ee.ImageCollection(FIRMS_COLLECTION)\n",
        "           .filterDate(d0, d1)\n",
        "           .filterBounds(geom)\n",
        "           .select('confidence'))\n",
        "\n",
        "    def _nonempty(ic):\n",
        "        img = ic.max().toUint8()\n",
        "        proj375 = ee.Projection('EPSG:4326').atScale(375)\n",
        "        img375  = img.setDefaultProjection(proj375)\n",
        "        img_aggr = (img375\n",
        "                    .reduceResolution(reducer=ee.Reducer.max(), bestEffort=True, maxPixels=4096)\n",
        "                    .reproject(nd_proj)\n",
        "                    .unmask(0)\n",
        "                    .clip(geom))\n",
        "        return img_aggr.rename('CONF')\n",
        "\n",
        "    return ee.Image(ee.Algorithms.If(\n",
        "        col.size().gt(0),\n",
        "        _nonempty(col),\n",
        "        ee.Image(0).toUint8().rename('CONF').reproject(nd_proj).clip(geom)\n",
        "    ))\n",
        "\n",
        "def _export_full_year_cells_raw(year_sel, ndvi_lag_days=8):\n",
        "    try:\n",
        "        year = int(year_sel)\n",
        "        if year < 2003 or year > 2020:\n",
        "            return \"Error: el año debe estar entre 2003 y 2020.\"\n",
        "\n",
        "        _, geom   = _get_regions()\n",
        "        start_y   = f\"{year}-01-01\"\n",
        "        end_y     = f\"{year}-12-31\"\n",
        "        nd_ic     = _get_ndvi_raw(start_y, end_y, geom)\n",
        "        ls_ic     = _get_lst_raw (start_y, end_y, geom)\n",
        "        vt_static = _get_vt_img(year, geom)  # ← VT anual (LC_Type1)\n",
        "\n",
        "        # Emparejar LST con NDVI (Δ t)\n",
        "        diff_ms = int(ndvi_lag_days * 86_400_000)\n",
        "        joined = ee.Join.saveBest('NDVI_best', 'tDiff').apply(\n",
        "            ls_ic, nd_ic,\n",
        "            ee.Filter.maxDifference(\n",
        "                diff_ms, leftField='system:time_start', rightField='system:time_start'\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Exportar por mes\n",
        "        for m in range(1, 12 + 1):\n",
        "            m0 = ee.Date.fromYMD(year, m, 1)\n",
        "            m1 = m0.advance(1, 'month')\n",
        "            lst_m = ee.ImageCollection(joined).filterDate(m0, m1)\n",
        "\n",
        "            def _per_day(ls_img):\n",
        "                nd_img  = ee.Image(ls_img.get('NDVI_best')).select('NDVI')\n",
        "                nd_proj = nd_img.projection()\n",
        "\n",
        "                # Reproyectar a la malla NDVI para evitar remuestreo posterior\n",
        "                lst_raw = ee.Image(ls_img).reproject(nd_proj).rename('LST_raw')\n",
        "                vt_img  = vt_static.reproject(nd_proj).rename('VT')\n",
        "                lonlat  = ee.Image.pixelLonLat().reproject(nd_proj)\n",
        "\n",
        "                date_str = ee.Date(ls_img.get('system:time_start')).format('YYYY-MM-dd')\n",
        "                conf     = _firms_conf_for_date(date_str, nd_proj, geom)\n",
        "\n",
        "                nd_unm   = nd_img.unmask(NDVI_SENTINEL).rename('NDVI_raw')\n",
        "                lst_unm  = lst_raw.unmask(LST_SENTINEL)\n",
        "                conf_u8  = conf.unmask(0).toUint8().rename('CONF')\n",
        "                vt_i16   = vt_img.unmask(0).toInt16().rename('VT')\n",
        "\n",
        "                stack = (ee.Image.cat([nd_unm, lst_unm, conf_u8, vt_i16, lonlat])\n",
        "                         .updateMask(ee.Image(1).reproject(nd_proj))\n",
        "                         .clip(geom))\n",
        "\n",
        "                tbl = stack.sample(\n",
        "                    region=geom, projection=nd_proj, geometries=False\n",
        "                ).map(lambda f: f.set('date', date_str))\n",
        "                return tbl\n",
        "\n",
        "            fc_month = ee.FeatureCollection(lst_m.map(_per_day)).flatten()\n",
        "\n",
        "            tag = f\"FULL_{year}_{str(m).zfill(2)}_CELLS_RAW\"\n",
        "            ee.batch.Export.table.toDrive(\n",
        "                collection     = fc_month,\n",
        "                description    = tag,\n",
        "                folder         = EXPORT_FOLDER,\n",
        "                fileNamePrefix = tag,\n",
        "                fileFormat     = 'CSV',\n",
        "                selectors      = ['date','longitude','latitude','NDVI_raw','LST_raw','CONF','VT']\n",
        "            ).start()\n",
        "\n",
        "            print(f\"[{year}] ➜ Tarea iniciada: {tag}.csv\")\n",
        "\n",
        "        return f\"[{year}] Exportaciones mensuales iniciadas (12 CSV en '{EXPORT_FOLDER}').\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error exportando FULL RAW: {e}\\n{traceback.format_exc(limit=1)}\"\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# 5-h. CSV ➜ PARQUET (BORRAR CSV)\n",
        "# ======================================================================\n",
        "def _convert_full_csvs_to_parquet(year_sel, compression='zstd'):\n",
        "    try:\n",
        "        year   = int(year_sel)\n",
        "        folder = f\"/content/drive/My Drive/{EXPORT_FOLDER}\"\n",
        "        pat    = re.compile(rf\"^FULL_{year}_(\\d{{2}})_CELLS_RAW\\.csv$\")\n",
        "        files  = [f for f in os.listdir(folder) if pat.match(f)]\n",
        "        if not files:\n",
        "            return f\"No hay CSV FULL_{year}_MM_CELLS_RAW.csv en {folder}\"\n",
        "\n",
        "        files.sort()\n",
        "        total_rows, converted = 0, 0\n",
        "\n",
        "        for fname in files:\n",
        "            csv_path = os.path.join(folder, fname)\n",
        "            pq_path  = os.path.join(folder, fname.replace(\".csv\", \".parquet\"))\n",
        "\n",
        "            df = pd.read_csv(csv_path, na_filter=True)\n",
        "\n",
        "            if 'CONF' in df.columns:\n",
        "                df['CONF'] = df['CONF'].fillna(0).astype('uint8')\n",
        "\n",
        "            if 'NDVI_raw' in df.columns:\n",
        "                try:\n",
        "                    df['NDVI_raw'] = df['NDVI_raw'].astype('int16')\n",
        "                except Exception:\n",
        "                    df['NDVI_raw'] = df['NDVI_raw'].astype('int32')\n",
        "\n",
        "            if 'LST_raw' in df.columns:\n",
        "                try:\n",
        "                    df['LST_raw'] = df['LST_raw'].astype('int16')\n",
        "                except Exception:\n",
        "                    df['LST_raw'] = df['LST_raw'].astype('int32')\n",
        "\n",
        "            if 'VT' in df.columns:\n",
        "                # LC_Type1 usa 1..17; 0 como “sin dato”\n",
        "                try:\n",
        "                    df['VT'] = df['VT'].fillna(0).astype('uint8')\n",
        "                except Exception:\n",
        "                    df['VT'] = df['VT'].fillna(0).astype('int16')\n",
        "\n",
        "            if 'date' in df.columns:\n",
        "                df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
        "\n",
        "            df.to_parquet(pq_path, index=False, compression=compression)\n",
        "            nrows = len(df)\n",
        "            total_rows += nrows\n",
        "            converted  += 1\n",
        "\n",
        "            if os.path.exists(pq_path) and os.path.getsize(pq_path) > 0:\n",
        "                os.remove(csv_path)\n",
        "                print(f\"✔️ {fname} ➜ {os.path.basename(pq_path)} ({nrows} filas) — CSV eliminado\")\n",
        "            else:\n",
        "                print(f\"⚠️ {fname}: Parquet no creado o vacío; CSV conservado\")\n",
        "\n",
        "        return (f\"Convertidos {converted} CSV a Parquet (≈ {total_rows:,} filas). \"\n",
        "                f\"Compresión='{compression}'. CSV originales eliminados.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error en conversión CSV➜Parquet: {e}\\n{traceback.format_exc(limit=1)}\"\n",
        "\n",
        "# ======================================================================\n",
        "# 7. UTILIDADES PARA GEOTIFF NDVI–LST EXPORTADOS\n",
        "# ======================================================================\n",
        "def _tag2date(name):\n",
        "    m = re.search(r'\\d{8}', name)\n",
        "    if not m:\n",
        "        raise ValueError(f\"No se encontró fecha en: {name}\")\n",
        "    return datetime.strptime(m.group(0), '%Y%m%d')\n",
        "\n",
        "def _norm_ndvi(nd):  # ya en [-1,1]\n",
        "    return nd\n",
        "\n",
        "def _list_pairs(max_delta_days=8):\n",
        "    folder = f'/content/drive/My Drive/{EXPORT_FOLDER}'\n",
        "    if not os.path.isdir(folder):\n",
        "        raise Exception(\"Aún no existe la carpeta de GeoTIFF en tu Drive.\")\n",
        "    nd_files = sorted(f for f in os.listdir(folder) if f.startswith('NDVI_'))\n",
        "    ls_files = sorted(f for f in os.listdir(folder) if f.startswith('LST_'))\n",
        "    haz_path = None\n",
        "    ls_dict = {_tag2date(f): f for f in ls_files}\n",
        "    pairs = []\n",
        "    for nd_f in nd_files:\n",
        "        d_nd = _tag2date(nd_f)\n",
        "        d_closest = min(ls_dict, key=lambda d: abs((d - d_nd).days))\n",
        "        if abs((d_closest - d_nd).days) <= max_delta_days:\n",
        "            nd_path = os.path.join(folder, nd_f)\n",
        "            ls_path = os.path.join(folder, ls_dict[d_closest])\n",
        "            with rasterio.open(nd_path) as src:\n",
        "                tr, crs = src.transform, src.crs\n",
        "            pairs.append((nd_path, ls_path, tr, crs, haz_path))\n",
        "    if not pairs:\n",
        "        raise Exception(\"Sin pares NDVI–LST dentro de ±4 días.\")\n",
        "    return pairs\n",
        "\n",
        "def _pair_by_date(date_str, max_delta_days=15):\n",
        "    target = datetime.strptime(date_str, '%Y-%m-%d')\n",
        "    pairs = _list_pairs(max_delta_days=max_delta_days)\n",
        "    best = min(pairs, key=lambda p: abs((_tag2date(os.path.basename(p[0])) - target).days))\n",
        "    d_best = _tag2date(os.path.basename(best[0]))\n",
        "    if abs((d_best - target).days) > max_delta_days:\n",
        "        raise RuntimeError(f\"No hay NDVI dentro de ±{max_delta_days} días de {date_str}.\")\n",
        "    return best\n",
        "\n",
        "_PAIRS = None\n",
        "def get_pairs():\n",
        "    global _PAIRS\n",
        "    if _PAIRS is None:\n",
        "        _PAIRS = _list_pairs()\n",
        "    return _PAIRS\n",
        "\n",
        "# ======================================================================\n",
        "# 11. DENSIDAD NDVI–LST POR AÑO (HEXBIN + MARGINALES)\n",
        "#     • Fuente: Parquet FULL_{YYYY}_MM_CELLS_RAW.parquet\n",
        "# ======================================================================\n",
        "def _list_full_parquets_for_year(year:int):\n",
        "    pat = re.compile(rf\"^FULL_{year}_(\\d{{2}})_CELLS_RAW\\.parquet$\")\n",
        "    files = [f for f in os.listdir(QC_FOLDER) if pat.match(f)]\n",
        "    files.sort()\n",
        "    return [os.path.join(QC_FOLDER, f) for f in files]\n",
        "\n",
        "def _sample_year_points(year:int,\n",
        "                        max_points:int = 400_000,\n",
        "                        max_per_file:int = 40_000,\n",
        "                        seed:int = 42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    paths = _list_full_parquets_for_year(year)\n",
        "    if not paths:\n",
        "        raise FileNotFoundError(\n",
        "            f\"No hay Parquet FULL_{year}_MM_CELLS_RAW.parquet en {QC_FOLDER}\"\n",
        "        )\n",
        "    parts = []\n",
        "    use_cols = ['NDVI_raw','LST_raw','latitude','longitude']\n",
        "    for p in paths:\n",
        "        df = pd.read_parquet(p, columns=use_cols)\n",
        "        nd = df['NDVI_raw'].to_numpy()\n",
        "        lr = df['LST_raw'].to_numpy()\n",
        "        mask = (nd != NDVI_SENTINEL) & (lr != LST_SENTINEL)\n",
        "        if not mask.any():\n",
        "            continue\n",
        "        ndv = np.clip(nd[mask].astype(np.float32) / 10000.0, -1.0, 1.0)\n",
        "        lst = lr[mask].astype(np.float32) / 10.0\n",
        "        lat = df.loc[mask, 'latitude'].to_numpy(dtype=np.float32)\n",
        "        lon = df.loc[mask, 'longitude'].to_numpy(dtype=np.float32)\n",
        "\n",
        "        m = (ndv >= -1.0) & (ndv <= 1.0) & (lst >= -20.0) & (lst <= 60.0)\n",
        "        if not m.any():\n",
        "            continue\n",
        "        ndv, lst, lat, lon = ndv[m], lst[m], lat[m], lon[m]\n",
        "\n",
        "        if len(ndv) > max_per_file:\n",
        "            idx = rng.choice(len(ndv), size=max_per_file, replace=False)\n",
        "            ndv, lst, lat, lon = ndv[idx], lst[idx], lat[idx], lon[idx]\n",
        "\n",
        "        parts.append(pd.DataFrame({'NDVI': ndv, 'LST': lst, 'lat': lat, 'lon': lon}))\n",
        "        del df\n",
        "\n",
        "    if not parts:\n",
        "        raise RuntimeError(f\"Sin datos válidos tras filtrar en {year}.\")\n",
        "    out = pd.concat(parts, ignore_index=True)\n",
        "    if len(out) > max_points:\n",
        "        out = out.sample(n=max_points, random_state=seed)\n",
        "    return out\n",
        "\n",
        "def _apply_palatino_to_fig(fig, family):\n",
        "    for ax in fig.axes:\n",
        "        ax.title.set_fontfamily(family)\n",
        "        ax.xaxis.label.set_fontfamily(family)\n",
        "        ax.yaxis.label.set_fontfamily(family)\n",
        "        for lab in ax.get_xticklabels() + ax.get_yticklabels():\n",
        "            lab.set_fontfamily(family)\n",
        "\n",
        "def _hexbin_year(year:int, mode:str=\"densidad\", gridsize:int=45):\n",
        "    \"\"\"\n",
        "    densidad  -> jointplot hexbin + marginales\n",
        "    lat / lon -> scatter con marcador 'x' y barra de color (sin marginales)\n",
        "    Estilo y colores consistentes Seaborn/Palatino.\n",
        "    \"\"\"\n",
        "    df = _sample_year_points(year).replace([np.inf, -np.inf], np.nan).dropna(subset=[\"NDVI\",\"LST\"])\n",
        "    if df.empty:\n",
        "        fig, ax = plt.subplots(figsize=(6,2))\n",
        "        ax.axis(\"off\"); ax.text(.5,.5,f\"Sin datos para {year}\",ha=\"center\",va=\"center\")\n",
        "        return fig\n",
        "\n",
        "    xlim = (float(np.nanmin(df[\"NDVI\"])), float(np.nanmax(df[\"NDVI\"])))\n",
        "    ylim = (float(np.nanmin(df[\"LST\"])),  float(np.nanmax(df[\"LST\"])))\n",
        "\n",
        "    # --------- MODO DENSIDAD: HEXBIN + MARGINALES ----------\n",
        "    if mode == \"densidad\":\n",
        "        with sns.axes_style(\"ticks\"), mpl.rc_context({\"font.family\": \"serif\", \"font.serif\": [font_name]}):\n",
        "            g = sns.jointplot(\n",
        "                data=df, x=\"NDVI\", y=\"LST\",\n",
        "                kind=\"hex\",\n",
        "                color=COLOR_ACCENT,\n",
        "                height=8.0, ratio=3, space=0,\n",
        "                joint_kws=dict(gridsize=int(gridsize), mincnt=1),\n",
        "                marginal_kws=dict(bins=50, color=COLOR_ACCENT, stat=\"count\")\n",
        "            )\n",
        "            g.ax_joint.set_xlim(*xlim); g.ax_joint.set_ylim(*ylim)\n",
        "            g.set_axis_labels(\"NDVI\", \"LST (°C)\")\n",
        "\n",
        "            g.ax_marg_x.cla()\n",
        "            cnt_x, bins_x = np.histogram(df[\"NDVI\"].to_numpy(), bins=50, range=xlim)\n",
        "            centers_x = 0.5 * (bins_x[:-1] + bins_x[1:]); width_x = (bins_x[1] - bins_x[0]) * 0.9\n",
        "            g.ax_marg_x.bar(centers_x, cnt_x, width=width_x, color=COLOR_ACCENT, edgecolor=\"none\")\n",
        "            g.ax_marg_x.set_xlim(*xlim); g.ax_marg_x.set_ylabel(\"Frecuencia\")\n",
        "            g.ax_marg_x.yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=4, integer=True))\n",
        "            g.ax_marg_x.ticklabel_format(style=\"sci\", axis=\"y\", scilimits=(0,0))\n",
        "            g.ax_marg_x.tick_params(axis=\"y\", labelleft=True)\n",
        "\n",
        "            g.ax_marg_y.cla()\n",
        "            cnt_y, bins_y = np.histogram(df[\"LST\"].to_numpy(), bins=50, range=ylim)\n",
        "            centers_y = 0.5 * (bins_y[:-1] + bins_y[1:]); height_y = (bins_y[1] - bins_y[0]) * 0.9\n",
        "            g.ax_marg_y.barh(centers_y, cnt_y, height=height_y, color=COLOR_ACCENT, edgecolor=\"none\")\n",
        "            g.ax_marg_y.set_ylim(*ylim); g.ax_marg_y.set_xlabel(\"Frecuencia\")\n",
        "            g.ax_marg_y.xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=4, integer=True))\n",
        "            g.ax_marg_y.ticklabel_format(style=\"sci\", axis=\"x\", scilimits=(0,0))\n",
        "            g.ax_marg_y.tick_params(axis=\"x\", labelbottom=True)\n",
        "\n",
        "            g.ax_joint.set_title(f\"NDVI–LST · {year} · Densidad\", pad=12)\n",
        "            sns.despine(fig=g.fig); g.fig.tight_layout()\n",
        "\n",
        "            for ax in g.fig.axes:\n",
        "                ax.title.set_fontfamily(font_name)\n",
        "                ax.xaxis.label.set_fontfamily(font_name)\n",
        "                ax.yaxis.label.set_fontfamily(font_name)\n",
        "                for lab in ax.get_xticklabels() + ax.get_yticklabels():\n",
        "                    lab.set_fontfamily(font_name)\n",
        "            return g.fig\n",
        "\n",
        "    # --------- MODO LAT/LON: SCATTER 'X' + COLORBAR, SIN MARGINALES ----------\n",
        "    var = \"lat\" if mode == \"lat\" else \"lon\"\n",
        "    vals = df[var].to_numpy()\n",
        "    try:\n",
        "        cmap = sns.color_palette(\"cubehelix_r\", as_cmap=True)\n",
        "    except Exception:\n",
        "        cmap = plt.cm.viridis\n",
        "    norm = mpl.colors.Normalize(vmin=float(np.nanmin(vals)), vmax=float(np.nanmax(vals)))\n",
        "\n",
        "    n = len(df)\n",
        "    s = 6 if n > 200_000 else (8 if n > 100_000 else 12)\n",
        "\n",
        "    with sns.axes_style(\"ticks\"), mpl.rc_context({\"font.family\": \"serif\", \"font.serif\": [font_name]}):\n",
        "        fig, ax = plt.subplots(figsize=(8.5, 6))\n",
        "        sc = ax.scatter(df[\"NDVI\"], df[\"LST\"],\n",
        "                        c=vals, cmap=cmap, norm=norm,\n",
        "                        marker=\"x\", s=s, linewidths=0.7, alpha=0.9)\n",
        "        ax.set_xlim(*xlim); ax.set_ylim(*ylim)\n",
        "        ax.set_xlabel(\"NDVI\"); ax.set_ylabel(\"LST (°C)\")\n",
        "        title = \"Color = latitud\" if mode == \"lat\" else \"Color = longitud\"\n",
        "        ax.set_title(f\"NDVI–LST · {year} · {title}\", pad=12)\n",
        "        ax.grid(alpha=.3)\n",
        "        sns.despine(fig=fig)\n",
        "        cbar = fig.colorbar(sc, ax=ax, pad=0.02)\n",
        "        cbar.set_label(\"Latitud (°)\" if mode == \"lat\" else \"Longitud (°)\")\n",
        "\n",
        "        for a in fig.axes + [cbar.ax]:\n",
        "            try: a.title.set_fontfamily(font_name)\n",
        "            except Exception: pass\n",
        "            if hasattr(a, \"xaxis\") and a.xaxis and a.xaxis.label: a.xaxis.label.set_fontfamily(font_name)\n",
        "            if hasattr(a, \"yaxis\") and a.yaxis and a.yaxis.label: a.yaxis.label.set_fontfamily(font_name)\n",
        "            for lab in a.get_xticklabels() + a.get_yticklabels():\n",
        "                lab.set_fontfamily(font_name)\n",
        "\n",
        "        fig.tight_layout()\n",
        "        return fig\n",
        "\n",
        "# ======================================================================\n",
        "# 12-bis. ENVOLVENTES HÚMEDA/SECA (HISTGB + HUBER)\n",
        "# ======================================================================\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "\n",
        "def _prep_xy_split_by_bin(df, step=0.01, n_min=5):\n",
        "    d = df[['NDVI','LST']].replace([np.inf, -np.inf], np.nan).dropna().copy()\n",
        "    d = d[(d['NDVI'] >= -1.0) & (d['NDVI'] <= 1.0)]\n",
        "    d['bin'] = (np.floor(d['NDVI']/step)*step + step/2).round(3)\n",
        "\n",
        "    Xw, yw, Xd, yd = [], [], [], []\n",
        "    for b, g in d.groupby('bin'):\n",
        "        s = np.sort(g['LST'].to_numpy(float))\n",
        "        if s.size < 2:\n",
        "            continue\n",
        "        j = int(np.argmax(np.diff(s))) if s.size > 1 else 0\n",
        "        lower, upper = s[:j+1], s[j+1:]\n",
        "        if lower.size < n_min or upper.size < n_min:\n",
        "            t = np.median(s); lower = s[s<=t]; upper = s[s>=t]\n",
        "            if lower.size == 0 or upper.size == 0:\n",
        "                continue\n",
        "        nd_bin = float(b)\n",
        "        Xw.append(np.full(lower.shape, nd_bin).reshape(-1,1));  yw.append(lower)\n",
        "        Xd.append(np.full(upper.shape, nd_bin).reshape(-1,1));  yd.append(upper)\n",
        "\n",
        "    if not Xw or not Xd:\n",
        "        raise ValueError(\"No hay suficientes bins para separar bloques.\")\n",
        "    return (np.vstack(Xw), np.concatenate(yw)), (np.vstack(Xd), np.concatenate(yd))\n",
        "\n",
        "def _fit_quantile_hgbr(X, y, q, monotone):\n",
        "    model = HistGradientBoostingRegressor(\n",
        "        loss=\"quantile\", quantile=q, learning_rate=0.05, max_iter=600,\n",
        "        min_samples_leaf=80, l2_regularization=1e-2, monotonic_cst=[monotone],\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X, y)\n",
        "    xg = np.linspace(-1.0, 1.0, 401).reshape(-1,1)\n",
        "    yg = model.predict(xg)\n",
        "    return xg.ravel(), yg\n",
        "\n",
        "def _line_from_curve(xg, yg, fit_range=(0.15, 0.95)):\n",
        "    mask = (xg >= fit_range[0]) & (xg <= fit_range[1])\n",
        "    Xr = xg[mask].reshape(-1,1); yr = yg[mask]\n",
        "    hr = HuberRegressor(alpha=1e-4).fit(Xr, yr)\n",
        "    return float(hr.coef_[0]), float(hr.intercept_)\n",
        "\n",
        "def envelope_curves_hgbr(df, q_wet=0.02, q_dry=0.98, step=0.01, n_min=5):\n",
        "    (Xw, yw), (Xd, yd) = _prep_xy_split_by_bin(df, step=step, n_min=n_min)\n",
        "    xg_w, wet_g = _fit_quantile_hgbr(Xw, yw, q=q_wet, monotone=+1)\n",
        "    xg_d, dry_g = _fit_quantile_hgbr(Xd, yd, q=q_dry, monotone=-1)\n",
        "    xg = np.linspace(-1.0, 1.0, 401)\n",
        "    wet_curve = np.interp(xg, xg_w, wet_g)\n",
        "    dry_curve = np.interp(xg, xg_d, dry_g)\n",
        "    return xg, wet_curve, dry_curve\n",
        "\n",
        "def _fit_edges_from_extremes(df,\n",
        "                             q_wet=0.02, q_dry=0.98,\n",
        "                             step=0.01, n_min=5,\n",
        "                             fit_range=(0.15, 0.95)):\n",
        "    xg, wet_g, dry_g = envelope_curves_hgbr(df, q_wet=q_wet, q_dry=q_dry,\n",
        "                                            step=step, n_min=n_min)\n",
        "    m_w, c_w = _line_from_curve(xg, wet_g, fit_range=fit_range)\n",
        "    m_d, c_d = _line_from_curve(xg, dry_g, fit_range=fit_range)\n",
        "\n",
        "    xm = xg[(xg>=fit_range[0]) & (xg<=fit_range[1])].mean()\n",
        "    if m_w < 0:\n",
        "        m_w = abs(m_w); c_w = float(np.mean(wet_g[(xg>=fit_range[0])&(xg<=fit_range[1])])) - m_w*xm\n",
        "    if m_d >= 0:\n",
        "        m_d = -abs(m_d) if m_d != 0 else -1e-6\n",
        "        c_d = float(np.mean(dry_g[(xg>=fit_range[0])&(xg<=fit_range[1])])) - m_d*xm\n",
        "\n",
        "    return (m_w, c_w), (m_d, c_d)\n",
        "\n",
        "def _make_year_scatter_figure(df, year, m_w, c_w, m_d, c_d,\n",
        "                              sample_max=200_000,\n",
        "                              show_curves=True,\n",
        "                              xlim=(-0.2, 1.0),\n",
        "                              ylim=(-20, 60)):\n",
        "    df = df[['NDVI','LST']].replace([np.inf, -np.inf], np.nan).dropna()\n",
        "    df = df[(df['NDVI'] >= xlim[0]) & (df['NDVI'] <= xlim[1])]\n",
        "    n = len(df)\n",
        "    if n > sample_max:\n",
        "        rng = np.random.default_rng(42)\n",
        "        dfp = df.iloc[rng.choice(n, sample_max, replace=False)]\n",
        "    else:\n",
        "        dfp = df\n",
        "\n",
        "    with sns.axes_style(\"ticks\"), mpl.rc_context({\"font.family\": \"serif\", \"font.serif\": [font_name]}):\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "        if not dfp.empty:\n",
        "            ax.scatter(dfp['NDVI'], dfp['LST'], s=4, alpha=0.25,\n",
        "                       color=COLOR_SCATTER, edgecolors='none')\n",
        "\n",
        "        xs = np.linspace(xlim[0], xlim[1], 200)\n",
        "        yw = m_w*xs + c_w\n",
        "        yd = m_d*xs + c_d\n",
        "        ax.plot(xs, yw, color=COLOR_WET,  lw=2.6, label=f\"Húmeda  y={m_w:.2f}·x+{c_w:.2f}\")\n",
        "        ax.plot(xs, yd, color=COLOR_DRY,  lw=2.6, label=f\"Seca    y={m_d:.2f}·x+{c_d:.2f}\")\n",
        "\n",
        "        if show_curves:\n",
        "            try:\n",
        "                xg, wet_g, dry_g = envelope_curves_hgbr(df)\n",
        "                mask = (xg >= xlim[0]) & (xg <= xlim[1])\n",
        "                ax.plot(xg[mask], wet_g[mask], color=COLOR_CURVE_W, lw=3, alpha=.50, label='Húmeda (curva)')\n",
        "                ax.plot(xg[mask], dry_g[mask], color=COLOR_CURVE_D, lw=3, alpha=.50, label='Seca (curva)')\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        ax.set_xlim(*xlim); ax.set_ylim(*ylim)\n",
        "        ax.set_xlabel('NDVI'); ax.set_ylabel('LST (°C)')\n",
        "        ax.set_title(f'NDVI–LST {year}: nube (gris) + rectas')\n",
        "        ax.grid(alpha=.3); ax.legend(loc='best', fontsize=9)\n",
        "        sns.despine(fig=fig)\n",
        "        fig.tight_layout()\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        fig.savefig(buf, format='png', dpi=150); plt.close(fig)\n",
        "        buf.seek(0); img = Image.open(buf).convert('RGB'); arr = np.array(img); buf.close()\n",
        "        return arr\n",
        "\n",
        "def plot_multi_year_edges(edge_list, alpha=.40):\n",
        "    if not edge_list:\n",
        "        raise ValueError(\"edge_list vacío\")\n",
        "\n",
        "    with sns.axes_style(\"ticks\"), mpl.rc_context({\"font.family\": \"serif\", \"font.serif\": [font_name]}):\n",
        "        fig, ax_plot = plt.subplots(figsize=(9, 6))\n",
        "        x = np.linspace(-1, 1, 200)\n",
        "\n",
        "        m_ws, c_ws, m_ds, c_ds = [], [], [], []\n",
        "        for ed in edge_list:\n",
        "            y_w = ed['m_w']*x + ed['c_w']\n",
        "            y_d = ed['m_d']*x + ed['c_d']\n",
        "            ax_plot.plot(x, y_w, color=COLOR_WET, alpha=alpha)\n",
        "            ax_plot.plot(x, y_d, color=COLOR_DRY, alpha=alpha)\n",
        "            m_ws.append(ed['m_w']); c_ws.append(ed['c_w'])\n",
        "            m_ds.append(ed['m_d']); c_ds.append(ed['c_d'])\n",
        "\n",
        "        m_wm, c_wm = np.mean(m_ws), np.mean(c_ws)\n",
        "        m_dm, c_dm = np.mean(m_ds), np.mean(c_ds)\n",
        "\n",
        "        ax_plot.plot(x, m_wm*x + c_wm, color=COLOR_WET,  lw=3,\n",
        "                     label=f\"Húmeda ⌀  y = {m_wm:.2f}·x + {c_wm:.2f}\")\n",
        "        ax_plot.plot(x, m_dm*x + c_dm, color=COLOR_DRY,  lw=3,\n",
        "                     label=f\"Seca   ⌀  y = {m_dm:.2f}·x + {c_dm:.2f}\")\n",
        "\n",
        "        ax_plot.set_xlabel('NDVI'); ax_plot.set_ylabel('LST (°C)')\n",
        "        ax_plot.set_title('Rectas húmeda y seca por año + promedio')\n",
        "        ax_plot.grid(alpha=.3); ax_plot.legend(fontsize=9, loc='upper left')\n",
        "        ax_plot.set_xlim(-1, 1)\n",
        "        sns.despine(fig=fig)\n",
        "        fig.tight_layout()\n",
        "\n",
        "    header = [\"Año\", \"a₁\", \"b₁\", \"a₂\", \"b₂\"]\n",
        "    rows = [[str(ed['year']),\n",
        "             f\"{ed['m_w']:.4f}\", f\"{ed['c_w']:.4f}\",\n",
        "             f\"{ed['m_d']:.4f}\", f\"{ed['c_d']:.4f}\"]\n",
        "            for ed in edge_list]\n",
        "    table_text = '\\t'.join(header) + '\\n' + '\\n'.join(['\\t'.join(row) for row in rows])\n",
        "    return fig, table_text\n",
        "\n",
        "# ======================================================================\n",
        "# 13. Mapa con pesos calibrados usando VT desde Parquet diario\n",
        "#      • Toma VT de FULL_YYYY_MM_CELLS_RAW.parquet (columna VT) del día indicado\n",
        "#      • Rasteriza VT -> HAZARD usando la tabla editable (vt_table_df)\n",
        "#      • Calcula TVDI con rectas a1,b1 (húmeda) y a2,b2 (seca)\n",
        "#      • FDCI = (w1*LST_SER + w2*ND_DRY + w3*TVDI + w4*HAZ + 1)/4\n",
        "# ======================================================================\n",
        "\n",
        "def _parquet_path_for_date(date_str: str) -> str:\n",
        "    \"\"\"Devuelve la ruta del Parquet mensual correspondiente a date_str.\"\"\"\n",
        "    try:\n",
        "        y = int(date_str[0:4])\n",
        "        m = int(date_str[5:7])\n",
        "    except Exception:\n",
        "        raise ValueError(f\"Fecha inválida: {date_str} (esperado AAAA-MM-DD)\")\n",
        "    fname = f\"FULL_{y}_{str(m).zfill(2)}_CELLS_RAW.parquet\"\n",
        "    path  = os.path.join(QC_FOLDER, fname)\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"No existe {fname} en {QC_FOLDER}\")\n",
        "    return path\n",
        "\n",
        "def _load_vt_points_for_date(date_str: str):\n",
        "    \"\"\"\n",
        "    Carga (lon, lat, vt) SOLO del día solicitado desde el parquet mensual.\n",
        "    Devuelve tres arrays (float64, float64, int16). Filtra VT válidos (1..17).\n",
        "    \"\"\"\n",
        "    path = _parquet_path_for_date(date_str)\n",
        "    df = pd.read_parquet(path, columns=[\"date\", \"longitude\", \"latitude\", \"VT\"])\n",
        "    if not pd.api.types.is_object_dtype(df[\"date\"]):\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "    df = df[df[\"date\"] == date_str].dropna(subset=[\"longitude\", \"latitude\", \"VT\"])\n",
        "    if df.empty:\n",
        "        return np.array([]), np.array([]), np.array([], dtype=np.int16)\n",
        "    vt_arr = pd.to_numeric(df[\"VT\"], errors=\"coerce\").astype(\"Int16\").to_numpy()\n",
        "    mask   = np.isin(vt_arr, list(VT_VALID))\n",
        "    if not mask.any():\n",
        "        return np.array([]), np.array([]), np.array([], dtype=np.int16)\n",
        "    lon = df.loc[mask, \"longitude\"].to_numpy()\n",
        "    lat = df.loc[mask, \"latitude\"].to_numpy()\n",
        "    vt  = vt_arr[mask].astype(np.int16)\n",
        "    return lon, lat, vt\n",
        "\n",
        "def _rasterize_hazard_from_points(lon, lat, vt, vt_map, nd_tr, nd_crs, out_shape):\n",
        "    \"\"\"\n",
        "    Crea raster HAZARD (float32) alineado a la malla NDVI/LST usando:\n",
        "      - puntos (lon,lat,vt)\n",
        "      - vt_map: dict {code: hazard[0..1]}\n",
        "      - nd_tr, nd_crs: transform y CRS del NDVI\n",
        "      - out_shape: (rows, cols)\n",
        "    \"\"\"\n",
        "    hz = np.zeros(out_shape, dtype=np.float32)\n",
        "\n",
        "    if lon.size == 0:\n",
        "        return hz\n",
        "    try:\n",
        "        dst = nd_crs if isinstance(nd_crs, str) else nd_crs.to_string()\n",
        "    except Exception:\n",
        "        dst = nd_crs\n",
        "    if dst and str(dst).upper() not in (\"EPSG:4326\", \"OGC:CRS84\", \"WGS84\"):\n",
        "        xs, ys = rasterio.warp.transform(\"EPSG:4326\", nd_crs, lon.tolist(), lat.tolist())\n",
        "        xs = np.asarray(xs); ys = np.asarray(ys)\n",
        "    else:\n",
        "        xs, ys = lon, lat\n",
        "\n",
        "    rr, cc = rasterio.transform.rowcol(nd_tr, xs, ys)\n",
        "    rr = np.asarray(rr); cc = np.asarray(cc)\n",
        "\n",
        "    rows, cols = out_shape\n",
        "    inb = (rr >= 0) & (rr < rows) & (cc >= 0) & (cc < cols)\n",
        "    if not inb.any():\n",
        "        return hz\n",
        "\n",
        "    rr = rr[inb]; cc = cc[inb]; vt = vt[inb]\n",
        "\n",
        "    max_code = max(VT_VALID)\n",
        "    lut = np.zeros(max_code + 1, dtype=np.float32)\n",
        "    for k, v in vt_map.items():\n",
        "        if 0 <= int(k) <= max_code:\n",
        "            lut[int(k)] = float(np.clip(v, 0.0, 1.0))\n",
        "    hz_vals = lut[np.clip(vt, 0, max_code)]\n",
        "\n",
        "    hz[rr, cc] = hz_vals.astype(np.float32)\n",
        "    return hz\n",
        "\n",
        "def _map_from_parquet(date_map_str,\n",
        "                      a1, b1, a2, b2,               # rectas húmeda/seca (NDVI en [-1,1])\n",
        "                      w_lst, w_ndvi, w_tvdi, w_haz,  # pesos FDCI\n",
        "                      vt_table_df,\n",
        "                      add_firms_points=False,        # dibuja puntos CONF>0 desde parquet\n",
        "                      grid_deg=0.0045,               # ~500 m en el ecuador\n",
        "                      lst_norm_min=None,             # LST min (°C) para normalizar (None = auto p02)\n",
        "                      lst_norm_max=None):            # LST max (°C) para normalizar (None = auto p98)\n",
        "    \"\"\"\n",
        "    Genera un mapa FDCI a partir del Parquet diario.\n",
        "\n",
        "    Correcciones clave:\n",
        "      • Se filtran explícitamente los centinelas: NDVI_raw == -32768, LST_raw == -32768.\n",
        "      • NDVI se usa crudo en [-1, 1] (no 0–1).\n",
        "      • LST serie se normaliza con [min, max] provistos o, si no, con p02–p98 del DÍA,\n",
        "        calculados DESPUÉS de filtrar centinelas y no finitos.\n",
        "      • FIRMS: se pintan puntos con CONF>0 correspondientes a las filas válidas (tras m_ok).\n",
        "    \"\"\"\n",
        "    import numpy as np, pandas as pd, folium\n",
        "    from matplotlib import cm, colors\n",
        "\n",
        "    # -------- 1) Cargar SOLO el día desde el Parquet del mes correspondiente --------\n",
        "    path = _parquet_path_for_date(date_map_str)\n",
        "    use_cols = [\"date\",\"longitude\",\"latitude\",\"NDVI\",\"NDVI_raw\",\n",
        "                \"LST\",\"LST_raw\",\"CONF\",\"VT\"]\n",
        "\n",
        "    try:\n",
        "        import pyarrow.parquet as pq\n",
        "        cols_avail = set(pq.ParquetFile(path).schema.names)\n",
        "        keep = [c for c in use_cols if c in cols_avail]\n",
        "        df = pd.read_parquet(path, columns=keep)\n",
        "    except Exception:\n",
        "        df = pd.read_parquet(path)\n",
        "        keep = [c for c in use_cols if c in df.columns]\n",
        "        df = df[keep]\n",
        "\n",
        "    if 'date' not in df.columns:\n",
        "        return \"<p style='color:red'><b>El parquet no tiene columna 'date'.</b></p>\"\n",
        "\n",
        "    if not pd.api.types.is_object_dtype(df[\"date\"]):\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "    df = df[df[\"date\"] == date_map_str]\n",
        "\n",
        "    req_cols = {\"longitude\",\"latitude\",\"VT\"}\n",
        "    if not req_cols.issubset(df.columns):\n",
        "        faltan = \", \".join(sorted(req_cols - set(df.columns)))\n",
        "        return f\"<p style='color:red'><b>Faltan columnas en el parquet: {faltan}</b></p>\"\n",
        "    df = df.dropna(subset=[\"longitude\",\"latitude\",\"VT\"])\n",
        "    if df.empty:\n",
        "        return \"<p style='color:red'><b>No hay filas para ese día en el Parquet.</b></p>\"\n",
        "\n",
        "    # -------- 2) NDVI (crudo [-1,1]) y LST (°C) con FILTRO de centinelas --------\n",
        "    # NDVI\n",
        "    if \"NDVI_raw\" in df.columns:\n",
        "        nd_raw = pd.to_numeric(df[\"NDVI_raw\"], errors=\"coerce\").to_numpy(np.int32)\n",
        "        mask_nd = nd_raw != NDVI_SENTINEL\n",
        "        nd = nd_raw.astype(np.float32) / 10000.0\n",
        "    elif \"NDVI\" in df.columns:\n",
        "        nd = pd.to_numeric(df[\"NDVI\"], errors=\"coerce\").to_numpy(np.float32)\n",
        "        mask_nd = np.isfinite(nd) & (nd >= -1.5) & (nd <= 1.5)\n",
        "    else:\n",
        "        return \"<p style='color:red'><b>No se encontró NDVI ni NDVI_raw en el parquet.</b></p>\"\n",
        "\n",
        "    # LST\n",
        "    if \"LST_raw\" in df.columns:\n",
        "        ls_raw = pd.to_numeric(df[\"LST_raw\"], errors=\"coerce\").to_numpy(np.int32)\n",
        "        mask_ls = ls_raw != LST_SENTINEL\n",
        "        ls = ls_raw.astype(np.float32) / 10.0\n",
        "    elif \"LST\" in df.columns:\n",
        "        ls = pd.to_numeric(df[\"LST\"], errors=\"coerce\").to_numpy(np.float32)\n",
        "        mask_ls = np.isfinite(ls) & (ls > -150.0) & (ls < 100.0)\n",
        "    else:\n",
        "        return \"<p style='color:red'><b>No se encontró LST ni LST_raw en el parquet.</b></p>\"\n",
        "\n",
        "    lon = pd.to_numeric(df[\"longitude\"], errors=\"coerce\").to_numpy(np.float64)\n",
        "    lat = pd.to_numeric(df[\"latitude\"],  errors=\"coerce\").to_numpy(np.float64)\n",
        "    vt  = pd.to_numeric(df[\"VT\"],        errors=\"coerce\").to_numpy(np.int16)\n",
        "\n",
        "    # Para FIRMS:\n",
        "    conf_vals = None\n",
        "    if \"CONF\" in df.columns:\n",
        "        conf_vals = pd.to_numeric(df[\"CONF\"], errors=\"coerce\").to_numpy(np.float32)\n",
        "    else:\n",
        "        conf_vals = None\n",
        "\n",
        "    # Máscara final: centinelas + finitos + VT válido\n",
        "    m_ok = (mask_nd & mask_ls &\n",
        "            np.isfinite(nd) & np.isfinite(ls) &\n",
        "            np.isfinite(lon) & np.isfinite(lat) &\n",
        "            (vt > 0))\n",
        "\n",
        "    if not np.any(m_ok):\n",
        "        return \"<p style='color:red'><b>Sin datos válidos (centinelas/NaN/VT) tras filtrar.</b></p>\"\n",
        "\n",
        "    nd = np.clip(nd[m_ok], -1.0, 1.0)\n",
        "    ls = ls[m_ok]\n",
        "    lon = lon[m_ok]; lat = lat[m_ok]; vt = vt[m_ok]\n",
        "    if conf_vals is not None:\n",
        "        conf_vals = conf_vals[m_ok]  # alinear FIRMS con las filas válidas\n",
        "\n",
        "    # -------- 3) Features por píxel --------\n",
        "    # LST serie normalizada por límites provistos o p02–p98 del día\n",
        "    if (lst_norm_min is not None) and (lst_norm_max is not None):\n",
        "        try:\n",
        "            lmin = float(lst_norm_min); lmax = float(lst_norm_max)\n",
        "        except Exception:\n",
        "            lmin, lmax = np.nan, np.nan\n",
        "        if (not np.isfinite(lmin)) or (not np.isfinite(lmax)) or (lmax <= lmin):\n",
        "            lmin, lmax = np.percentile(ls, [2, 98])\n",
        "    else:\n",
        "        lmin, lmax = np.percentile(ls, [2, 98])\n",
        "\n",
        "    lst_ser = np.clip((ls - lmin) / max(1e-6, (lmax - lmin)), 0.0, 1.0)\n",
        "\n",
        "    # NDVI crudo como feature ([-1,1])\n",
        "    nd_feat = nd\n",
        "\n",
        "    # TVDI con rectas sobre NDVI en [-1,1]\n",
        "    a1, b1, a2, b2 = map(float, (a1, b1, a2, b2))\n",
        "    lst_w = a1*nd_feat + b1\n",
        "    lst_d = a2*nd_feat + b2\n",
        "    span  = np.maximum(1e-6, lst_d - lst_w)\n",
        "    tvdi  = np.clip((ls - lst_w) / span, 0.0, 1.0)\n",
        "\n",
        "    # HAZARD desde VT (tabla editable)\n",
        "    vt_map = vt_table_to_map(vt_table_df)\n",
        "    haz = np.array([vt_map.get(int(c), 0.0) for c in vt], dtype=np.float32)\n",
        "    haz = np.clip(haz, 0.0, 1.0)\n",
        "\n",
        "    # FDCI final\n",
        "    w_lst, w_ndvi, w_tvdi, w_haz = map(float, (w_lst, w_ndvi, w_tvdi, w_haz))\n",
        "    fdci = np.clip((w_lst*lst_ser + w_ndvi*nd_feat + w_tvdi*tvdi + w_haz*haz + 1.0) / 4.0, 0.0, 1.0)\n",
        "\n",
        "    # -------- 4) Rasterizar a rejilla WGS84 --------\n",
        "    pad = 0.01\n",
        "    min_lon, max_lon = float(lon.min()) - pad, float(lon.max()) + pad\n",
        "    min_lat, max_lat = float(lat.min()) - pad, float(lat.max()) + pad\n",
        "    ncols = int(np.ceil((max_lon - min_lon) / grid_deg))\n",
        "    nrows = int(np.ceil((max_lat - min_lat) / grid_deg))\n",
        "    if ncols <= 0 or nrows <= 0:\n",
        "        return \"<p style='color:red'><b>Extensión inválida para rasterizar.</b></p>\"\n",
        "\n",
        "    cc = ((lon - min_lon) / grid_deg).astype(np.int64)\n",
        "    rr = ((max_lat - lat) / grid_deg).astype(np.int64)\n",
        "    m  = (rr >= 0) & (rr < nrows) & (cc >= 0) & (cc < ncols)\n",
        "    rr, cc, val = rr[m], cc[m], fdci[m]\n",
        "\n",
        "    idx = rr * ncols + cc\n",
        "    acc = np.bincount(idx, weights=val, minlength=nrows*ncols).astype(np.float32)\n",
        "    cnt = np.bincount(idx,              minlength=nrows*ncols).astype(np.float32)\n",
        "    grid = (acc / np.maximum(cnt, 1)).reshape(nrows, ncols)\n",
        "    grid[cnt.reshape(nrows, ncols) == 0] = np.nan\n",
        "\n",
        "    # -------- 5) Folium --------\n",
        "    cmap = cm.get_cmap(\"turbo\")\n",
        "    norm = colors.Normalize(vmin=0.0, vmax=1.0)\n",
        "    rgba = (cmap(norm(np.nan_to_num(grid, nan=0.0))) * 255).astype(np.uint8)\n",
        "    rgba[..., 3] = (np.isfinite(grid)).astype(np.uint8) * 255\n",
        "\n",
        "    fmap = folium.Map(location=[(min_lat+max_lat)/2.0, (min_lon+max_lon)/2.0],\n",
        "                      zoom_start=7, tiles=\"cartodbpositron\")\n",
        "    bounds = [[min_lat, min_lon], [max_lat, max_lon]]\n",
        "    folium.raster_layers.ImageOverlay(\n",
        "        image=rgba, bounds=bounds, name=f\"FDCI (Parquet {date_map_str})\",\n",
        "        opacity=0.85, mercator_project=False\n",
        "    ).add_to(fmap)\n",
        "\n",
        "    # ---- Puntos FIRMS (CONF>0) ----\n",
        "    n_pts = 0\n",
        "    if add_firms_points and (conf_vals is not None):\n",
        "        mask_fire = np.isfinite(conf_vals) & (conf_vals > 0)\n",
        "        if np.any(mask_fire):\n",
        "            lo_fire = lon[mask_fire]; la_fire = lat[mask_fire]; cf_fire = conf_vals[mask_fire]\n",
        "            # Colorear por nivel de confianza: baja ≤30, media 31–60, alta >60\n",
        "            def _color(c):\n",
        "                if c <= 30: return \"#FFB37A\"   # naranja claro\n",
        "                if c <= 60: return \"#FF7F0E\"   # naranja\n",
        "                return \"#E62300\"               # rojo\n",
        "            for lo, la, c in zip(lo_fire, la_fire, cf_fire):\n",
        "                folium.CircleMarker(\n",
        "                    location=[float(la), float(lo)],\n",
        "                    radius=2.2, color=_color(float(c)),\n",
        "                    fill=True, fill_opacity=0.95, opacity=0.9, weight=0.7,\n",
        "                    tooltip=f\"CONF={float(c):.0f}\"\n",
        "                ).add_to(fmap)\n",
        "            n_pts = int(mask_fire.sum())\n",
        "\n",
        "    folium.LayerControl().add_to(fmap)\n",
        "\n",
        "    # Cabecera con info de normalización y NDVI\n",
        "    header = (f\"<div style='font-family:{font_name}; font-size:13px; margin:6px 0;'>\"\n",
        "              f\"<b>Normalización LST (°C)</b>: min={lmin:.2f} · max={lmax:.2f} &nbsp;|&nbsp; \"\n",
        "              f\"<b>NDVI</b>: sin normalizar [-1, 1] &nbsp;|&nbsp; \"\n",
        "              f\"<b>FIRMS</b>: {n_pts} puntos con CONF&gt;0 (filtrados con datos válidos)</div>\")\n",
        "    return header + fmap._repr_html_()\n",
        "\n",
        "# ======================================================================\n",
        "# 15. HISTOGRAMAS MULTIANUALES\n",
        "# ======================================================================\n",
        "NDVI_BINS_F = np.linspace(0.0, 1.0, 81)     # paso 0.025\n",
        "LST_BINS_F  = np.arange(-20, 60 + 1, 1.0)   # -20..60 °C\n",
        "CONF_BINS_F = np.arange(0, 100 + 5, 5.0)    # 0..100, paso 5\n",
        "\n",
        "def _accum_year_hist_and_stats(year:int):\n",
        "    paths = _list_full_parquets_for_year(year)\n",
        "    if not paths:\n",
        "        raise FileNotFoundError(f\"No hay Parquet FULL_{year}_MM_CELLS_RAW.parquet en {QC_FOLDER}\")\n",
        "\n",
        "    ndvi_counts = np.zeros(len(NDVI_BINS_F)-1, dtype=np.int64)\n",
        "    lst_counts  = np.zeros(len(LST_BINS_F)-1,  dtype=np.int64)\n",
        "    conf_counts = np.zeros(len(CONF_BINS_F)-1, dtype=np.int64)\n",
        "\n",
        "    total_rows = 0\n",
        "    ndvi_miss = 0\n",
        "    lst_miss  = 0\n",
        "    conf_pos  = 0\n",
        "\n",
        "    by_day_count = {}\n",
        "    use_cols = ['date','longitude','latitude','NDVI_raw','LST_raw','CONF']\n",
        "    for p in paths:\n",
        "        df = pd.read_parquet(p, columns=use_cols)\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "\n",
        "        total_rows += len(df)\n",
        "        vc = df['date'].dt.date.value_counts(dropna=True)\n",
        "        for d, c in vc.items():\n",
        "            by_day_count[d] = by_day_count.get(d, 0) + int(c)\n",
        "\n",
        "        nd = df['NDVI_raw'].to_numpy()\n",
        "        ndvi_miss += int((nd == NDVI_SENTINEL).sum())\n",
        "        mask_nd = (nd != NDVI_SENTINEL)\n",
        "        if mask_nd.any():\n",
        "            ndv = (nd[mask_nd].astype(np.float32) / 10000.0)\n",
        "            ndv = np.clip(ndv, -1.0, 1.0)\n",
        "            h, _ = np.histogram(ndv, bins=NDVI_BINS_F)\n",
        "            ndvi_counts += h\n",
        "\n",
        "        lr = df['LST_raw'].to_numpy()\n",
        "        lst_miss += int((lr == LST_SENTINEL).sum())\n",
        "        mask_lr = (lr != LST_SENTINEL)\n",
        "        if mask_lr.any():\n",
        "            lstc = lr[mask_lr].astype(np.float32) / 10.0\n",
        "            h, _ = np.histogram(lstc, bins=LST_BINS_F)\n",
        "            lst_counts += h\n",
        "\n",
        "        cf = df['CONF'].to_numpy()\n",
        "        conf_pos += int((cf > 0).sum())\n",
        "        mconf = (cf > 0) & (cf <= 100)\n",
        "        if mconf.any():\n",
        "            h, _ = np.histogram(cf[mconf].astype(np.float32), bins=CONF_BINS_F)\n",
        "            conf_counts += h\n",
        "        del df\n",
        "\n",
        "    days_obs = len(by_day_count)\n",
        "    days_exp = 366 if calendar.isleap(year) else 365\n",
        "    days_missing = max(days_exp - days_obs, 0)\n",
        "    cells_per_day = np.array(list(by_day_count.values()), dtype=np.int64)\n",
        "    cells_min = int(cells_per_day.min()) if days_obs else 0\n",
        "    cells_max = int(cells_per_day.max()) if days_obs else 0\n",
        "    cells_avg = float(cells_per_day.mean()) if days_obs else 0.0\n",
        "\n",
        "    stats = {\n",
        "        'year': year, 'files': len(paths), 'rows_total': int(total_rows),\n",
        "        'days_expected': int(days_exp), 'days_observed': int(days_obs),\n",
        "        'days_missing': int(days_missing), 'cells_day_min': cells_min,\n",
        "        'cells_day_max': cells_max, 'cells_day_avg': cells_avg,\n",
        "        'pct_ndvi_missing': (ndvi_miss / total_rows * 100.0) if total_rows else 0.0,\n",
        "        'pct_lst_missing' : (lst_miss  / total_rows * 100.0) if total_rows else 0.0,\n",
        "        'pct_conf_positive': (conf_pos  / total_rows * 100.0) if total_rows else 0.0,\n",
        "    }\n",
        "    hists = {'ndvi': ndvi_counts, 'lst': lst_counts, 'conf': conf_counts}\n",
        "    return stats, hists\n",
        "\n",
        "def _plot_hist_grid(years, all_hists, bins, title, xlabel, xlim=None,\n",
        "                    outer_axes_only=True):\n",
        "    from math import ceil\n",
        "    n = len(years); ncols = 2; nrows = ceil(n / ncols)\n",
        "\n",
        "    with sns.axes_style(\"ticks\"), mpl.rc_context({\"font.family\": \"serif\", \"font.serif\": [font_name]}):\n",
        "        fig, axes = plt.subplots(\n",
        "            nrows, ncols, figsize=(12, 2.2*nrows),\n",
        "            sharex=True, sharey=False, squeeze=False\n",
        "        )\n",
        "        centers = 0.5*(bins[:-1] + bins[1:]); width = (bins[1]-bins[0]) * 0.9\n",
        "\n",
        "        for i, y in enumerate(years):\n",
        "            r, c = divmod(i, ncols)\n",
        "            ax = axes[r, c]\n",
        "            counts = all_hists[y]\n",
        "            ax.bar(centers, counts, width=width, align='center',\n",
        "                   color=COLOR_ACCENT, edgecolor=\"none\")\n",
        "            ax.set_title(str(y))\n",
        "            if xlim: ax.set_xlim(*xlim)\n",
        "            ax.grid(alpha=.3)\n",
        "\n",
        "            if outer_axes_only:\n",
        "                if r < nrows - 1:\n",
        "                    ax.set_xlabel(\"\")\n",
        "                    ax.tick_params(labelbottom=False)\n",
        "                if c > 0:\n",
        "                    ax.set_ylabel(\"\")\n",
        "                    ax.tick_params(labelleft=False)\n",
        "                else:\n",
        "                    ax.set_ylabel(\"Frecuencia\")\n",
        "\n",
        "        for j in range(n, nrows*ncols):\n",
        "            r, c = divmod(j, ncols)\n",
        "            axes[r, c].axis('off')\n",
        "\n",
        "        if outer_axes_only:\n",
        "            fig.supxlabel(xlabel)\n",
        "            fig.supylabel(\"Frecuencia\")\n",
        "\n",
        "        fig.suptitle(title, y=0.995, fontsize=14, fontfamily=font_name)\n",
        "        sns.despine(fig=fig)\n",
        "        fig.tight_layout(rect=[0,0,1,0.97])\n",
        "        return fig\n",
        "\n",
        "def _run_compact_histograms(years_txt):\n",
        "    years = [int(y.strip()) for y in years_txt.split(',') if y.strip()]\n",
        "    if not years: raise ValueError(\"No se proporcionaron años.\")\n",
        "    years = sorted(years)\n",
        "\n",
        "    ndvi_h, lst_h, conf_h = {}, {}, {}\n",
        "    rows = []\n",
        "    for y in years:\n",
        "        stats, hists = _accum_year_hist_and_stats(y)\n",
        "        rows.append(stats); ndvi_h[y] = hists['ndvi']; lst_h[y] = hists['lst']; conf_h[y] = hists['conf']\n",
        "\n",
        "    fig_ndvi = _plot_hist_grid(years, ndvi_h, NDVI_BINS_F, \"NDVI ([-1,1])\", \"NDVI\")\n",
        "    fig_lst  = _plot_hist_grid(years, lst_h,  LST_BINS_F,  \"LST (°C)\", \"°C\", xlim=(LST_BINS_F[0], LST_BINS_F[-1]))\n",
        "    fig_conf = _plot_hist_grid(years, conf_h, CONF_BINS_F, \"CONF (>0, bins de 5)\", \"Confianza\")\n",
        "\n",
        "    cols = [\"year\",\"files\",\"rows_total\",\"days_expected\",\"days_observed\",\"days_missing\",\n",
        "            \"cells_day_min\",\"cells_day_max\",\"cells_day_avg\",\n",
        "            \"pct_ndvi_missing\",\"pct_lst_missing\",\"pct_conf_positive\"]\n",
        "    summary_df = pd.DataFrame(rows)[cols]\n",
        "    summary_df[\"cells_day_avg\"] = summary_df[\"cells_day_avg\"].round(0).astype(\"int64\")\n",
        "    for col in [\"pct_ndvi_missing\",\"pct_lst_missing\",\"pct_conf_positive\"]:\n",
        "        summary_df[col] = summary_df[col].round(2)\n",
        "\n",
        "    return fig_ndvi, fig_lst, fig_conf, summary_df\n",
        "\n",
        "# ======================================================================\n",
        "# 15-bis. Histogramas por VT con incendios (diarios y netos)\n",
        "#         • Lee FULL_YYYY_MM_CELLS_RAW.parquet (usa columnas VT y CONF)\n",
        "#         • \"Diarios\": cada celda-día con CONF>0 cuenta 1\n",
        "#         • \"Netos\": corridas consecutivas (por celda) cuentan 1\n",
        "#         • Opción: apilar por niveles de confianza (baja/med/alta)\n",
        "# ======================================================================\n",
        "\n",
        "def _conf_tier_series(conf: pd.Series) -> pd.Series:\n",
        "    # Baja ≤30, Media 31–60, Alta >60\n",
        "    return pd.cut(conf.astype('float32'),\n",
        "                  bins=[0, 30, 60, 100],\n",
        "                  labels=[\"baja\",\"media\",\"alta\"],\n",
        "                  include_lowest=True, right=True)\n",
        "\n",
        "def _iter_event_rows_for_year(year:int):\n",
        "    \"\"\"Devuelve un DataFrame SOLO con filas de eventos (CONF>0) de todo el año,\n",
        "    con columnas mínimas: ['date','lat_q','lon_q','VT','CONF']\"\"\"\n",
        "    paths = _list_full_parquets_for_year(year)\n",
        "    parts = []\n",
        "    use_cols = ['date','longitude','latitude','VT','CONF']\n",
        "    for p in paths:\n",
        "        df = pd.read_parquet(p, columns=use_cols)\n",
        "        if df.empty:\n",
        "            continue\n",
        "        # Filtro de eventos\n",
        "        m = (df['CONF'] > 0)\n",
        "        if not m.any():\n",
        "            continue\n",
        "        sub = df.loc[m, ['date','longitude','latitude','VT','CONF']].copy()\n",
        "        # Normalizar tipos\n",
        "        if not pd.api.types.is_datetime64_any_dtype(sub['date']):\n",
        "            sub['date'] = pd.to_datetime(sub['date'], errors='coerce')\n",
        "        sub = sub.dropna(subset=['date'])\n",
        "        # Redondeo a rejilla\n",
        "        sub['lat_q'] = sub['latitude'].round(5)\n",
        "        sub['lon_q'] = sub['longitude'].round(5)\n",
        "        # Limitar a clases VT válidas\n",
        "        sub['VT'] = pd.to_numeric(sub['VT'], errors='coerce').astype('Int16')\n",
        "        sub = sub[sub['VT'].isin(VT_VALID)]\n",
        "        if not sub.empty:\n",
        "            parts.append(sub[['date','lat_q','lon_q','VT','CONF']])\n",
        "        del df, sub\n",
        "    if not parts:\n",
        "        return pd.DataFrame(columns=['date','lat_q','lon_q','VT','CONF'])\n",
        "    events = pd.concat(parts, ignore_index=True)\n",
        "    events.sort_values(['VT','lat_q','lon_q','date'], inplace=True)\n",
        "    return events\n",
        "\n",
        "def _count_daily_by_vt(events: pd.DataFrame, with_tiers: bool):\n",
        "    \"\"\"Cuenta celda-día por VT. Si with_tiers=True, apila por nivel de confianza.\"\"\"\n",
        "    if events.empty:\n",
        "        cols = ['VT','total','baja','media','alta'] if with_tiers else ['VT','total']\n",
        "        return pd.DataFrame(columns=cols)\n",
        "\n",
        "    if with_tiers:\n",
        "        events['tier'] = _conf_tier_series(events['CONF'])\n",
        "        # tamaño por VT y tier\n",
        "        t = events.groupby(['VT','tier'], observed=True).size().unstack('tier', fill_value=0)\n",
        "        t = t.reindex(columns=['baja','media','alta'], fill_value=0)\n",
        "        t['total'] = t.sum(axis=1)\n",
        "        t = t.reset_index()\n",
        "    else:\n",
        "        t = (events.groupby('VT', observed=True).size()\n",
        "                      .rename('total').reset_index())\n",
        "    return t\n",
        "\n",
        "def _count_net_by_vt(events: pd.DataFrame, with_tiers: bool):\n",
        "    \"\"\"Cuenta eventos netos (corridas de días consecutivos por celda) por VT.\n",
        "       La confianza del evento es el MÁXIMO CONF dentro de la corrida.\"\"\"\n",
        "    if events.empty:\n",
        "        cols = ['VT','total','baja','media','alta'] if with_tiers else ['VT','total']\n",
        "        return pd.DataFrame(columns=cols)\n",
        "\n",
        "    # Identificar corridas por (VT, celda)\n",
        "    def _runs_for_group(g):\n",
        "        # g: filas de una celda- VT, ordenadas por fecha\n",
        "        d = g['date'].to_numpy()\n",
        "        # Inicio de corrida si: diff>1 día o es la primera\n",
        "        start = np.ones(len(d), dtype=bool)\n",
        "        if len(d) > 1:\n",
        "            delta = (pd.Series(d).diff().dt.days.to_numpy())\n",
        "            start[1:] = (delta[1:] > 1) | np.isnan(delta[1:])\n",
        "        # Label por corrida\n",
        "        run_id = np.cumsum(start)\n",
        "        out = g.copy()\n",
        "        out['run_id'] = run_id\n",
        "        return out\n",
        "\n",
        "    grouped = events.groupby(['VT','lat_q','lon_q'], sort=False, observed=True, group_keys=False)\n",
        "    runs_df = grouped.apply(_runs_for_group)\n",
        "\n",
        "    # Confianza por corrida (máximo)\n",
        "    agg = (runs_df.groupby(['VT','lat_q','lon_q','run_id'], observed=True)\n",
        "                    .agg(CONF_max=('CONF','max'))\n",
        "                    .reset_index())\n",
        "\n",
        "    if with_tiers:\n",
        "        agg['tier'] = _conf_tier_series(agg['CONF_max'])\n",
        "        t = (agg.groupby(['VT','tier'], observed=True).size()\n",
        "                 .unstack('tier', fill_value=0)\n",
        "                 .reindex(columns=['baja','media','alta'], fill_value=0))\n",
        "        t['total'] = t.sum(axis=1)\n",
        "        t = t.reset_index()\n",
        "    else:\n",
        "        t = (agg.groupby('VT', observed=True).size()\n",
        "                    .rename('total').reset_index())\n",
        "    return t\n",
        "\n",
        "def _format_vt_table(df_counts: pd.DataFrame, with_tiers: bool):\n",
        "    df = df_counts.copy()\n",
        "    df['VT_name'] = df['VT'].map(VT_CLASS_NAMES)\n",
        "    if with_tiers:\n",
        "        cols = ['VT','VT_name','total','baja','media','alta']\n",
        "    else:\n",
        "        cols = ['VT','VT_name','total']\n",
        "    df = df[cols].sort_values('total', ascending=False)\n",
        "    return df\n",
        "\n",
        "# ======================================================================\n",
        "# 15-ter. Plot de barras por VT (simple o apilado por confianza)\n",
        "# ======================================================================\n",
        "def _plot_vt_bars(df_vt: pd.DataFrame, title: str, show_conf: bool):\n",
        "    if df_vt.empty:\n",
        "        fig, ax = plt.subplots(figsize=(7, 2))\n",
        "        ax.axis('off')\n",
        "        ax.text(.5,.5,\"Sin eventos con CONF>0 en los años seleccionados\",\n",
        "                ha='center', va='center', color='red')\n",
        "        return fig\n",
        "\n",
        "    with sns.axes_style(\"ticks\"), mpl.rc_context({\"font.family\":\"serif\", \"font.serif\":[font_name]}):\n",
        "        labels = [f\"{int(v)} · {n}\" for v, n in zip(df_vt['VT'], df_vt['VT_name'])]\n",
        "        x = np.arange(len(labels))\n",
        "        height = max(5, 0.4*len(labels))\n",
        "        fig, ax = plt.subplots(figsize=(min(14, 1.1*len(labels)+6), height))\n",
        "\n",
        "        if show_conf and all(c in df_vt.columns for c in ['baja','media','alta']):\n",
        "            b1 = ax.bar(x, df_vt['baja'],  label=\"Conf. baja (≤30)\")\n",
        "            b2 = ax.bar(x, df_vt['media'], bottom=df_vt['baja'],  label=\"Conf. media (31–60)\")\n",
        "            b3 = ax.bar(x, df_vt['alta'],  bottom=(df_vt['baja']+df_vt['media']), label=\"Conf. alta (>60)\")\n",
        "            totals = df_vt['total']\n",
        "        else:\n",
        "            b1 = ax.bar(x, df_vt['total'], color=COLOR_ACCENT, edgecolor=\"none\", label=\"Total\")\n",
        "            totals = df_vt['total']\n",
        "\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(labels, rotation=35, ha='right')\n",
        "        ax.set_ylabel(\"Conteo\")\n",
        "        ax.set_title(title, pad=10)\n",
        "        ax.grid(axis='y', alpha=.3)\n",
        "        ax.margins(x=0.01)\n",
        "\n",
        "        for idx, val in enumerate(totals):\n",
        "            ax.text(x[idx], val + (0.01*totals.max()), f\"{val:,}\",\n",
        "                    ha='center', va='bottom', fontsize=9, fontfamily=font_name)\n",
        "\n",
        "        if show_conf:\n",
        "            ax.legend(frameon=False, ncols=3, loc='upper right', fontsize=9)\n",
        "\n",
        "        sns.despine(fig=fig)\n",
        "        fig.tight_layout()\n",
        "        return fig\n",
        "\n",
        "# ======================================================================\n",
        "# 15-quater. Ejecutar histogramas VT para varios años (diarios y netos)\n",
        "# ======================================================================\n",
        "def _run_vt_histograms(years_txt: str, show_conf: bool):\n",
        "    years = [int(y.strip()) for y in years_txt.split(',') if y.strip()]\n",
        "    if not years:\n",
        "        raise ValueError(\"No se proporcionaron años.\")\n",
        "    years = sorted(years)\n",
        "\n",
        "    # Agregar TODOS los eventos (CONF>0) de los años seleccionados\n",
        "    all_events = []\n",
        "    for y in years:\n",
        "        ev_y = _iter_event_rows_for_year(y)\n",
        "        if not ev_y.empty:\n",
        "            all_events.append(ev_y)\n",
        "    if not all_events:\n",
        "        empty = pd.DataFrame(columns=['VT','VT_name','total','baja','media','alta'] if show_conf else ['VT','VT_name','total'])\n",
        "        fig1 = _plot_vt_bars(empty, \"VT con incendios · eventos diarios\", show_conf)\n",
        "        fig2 = _plot_vt_bars(empty, \"VT con incendios · eventos netos (no consecutivos)\", show_conf)\n",
        "        return fig1, fig2\n",
        "\n",
        "    events = pd.concat(all_events, ignore_index=True)\n",
        "\n",
        "    daily_counts = _count_daily_by_vt(events, with_tiers=show_conf)\n",
        "    net_counts   = _count_net_by_vt(events,   with_tiers=show_conf)\n",
        "\n",
        "    daily_df = _format_vt_table(daily_counts, with_tiers=show_conf)\n",
        "    net_df   = _format_vt_table(net_counts,   with_tiers=show_conf)\n",
        "\n",
        "    rng = f\"{years[0]}–{years[-1]}\" if len(years) > 1 else f\"{years[0]}\"\n",
        "    tit1 = f\"VT con incendios · eventos diarios (CONF>0) · Años {rng}\"\n",
        "    tit2 = f\"VT con incendios · eventos netos (no consecutivos) · Años {rng}\"\n",
        "\n",
        "    fig_daily = _plot_vt_bars(daily_df, tit1, show_conf=show_conf)\n",
        "    fig_net   = _plot_vt_bars(net_df,   tit2, show_conf=show_conf)\n",
        "    return fig_daily, fig_net\n",
        "\n",
        "# ======================================================================\n",
        "# 16. Calibración y Validación de pesos FDCI (w_lst, w_ndvi, w_tvdi, w_haz)\n",
        "#      - Carga parquet FULL_YYYY_MM_CELLS_RAW.parquet\n",
        "#      - PR-AUC como objetivo\n",
        "# ======================================================================\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_curve, roc_curve\n",
        "\n",
        "def _years_from_text(txt: str) -> list[int]:\n",
        "    return [int(y.strip()) for y in txt.split(\",\") if y.strip()]\n",
        "\n",
        "def _iter_full_parquets(year: int):\n",
        "    for p in _list_full_parquets_for_year(year):\n",
        "        yield p\n",
        "\n",
        "def _estimate_lst_percentiles(years: list[int], sample_per_file:int=5000, seed:int=42):\n",
        "    \"\"\"Estimación robusta de p02 y p98 de LST (°C) para normalizar la serie completa.\"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    sample = []\n",
        "    for y in years:\n",
        "        for p in _iter_full_parquets(y):\n",
        "            try:\n",
        "                df = pd.read_parquet(p, columns=[\"LST_raw\"])\n",
        "                arr = df[\"LST_raw\"].to_numpy()\n",
        "                m = (arr != LST_SENTINEL)\n",
        "                vals = (arr[m].astype(np.float32)/10.0)\n",
        "                if vals.size == 0:\n",
        "                    continue\n",
        "                if vals.size > sample_per_file:\n",
        "                    idx = rng.choice(vals.size, sample_per_file, replace=False)\n",
        "                    vals = vals[idx]\n",
        "                sample.append(vals)\n",
        "            except Exception:\n",
        "                continue\n",
        "    if not sample:\n",
        "        return -5.0, 45.0\n",
        "    vec = np.concatenate(sample)\n",
        "    p2, p98 = np.percentile(vec, [2, 98])\n",
        "    if p98 <= p2:\n",
        "        p2, p98 = np.min(vec), np.max(vec)\n",
        "    return float(p2), float(p98)\n",
        "\n",
        "def _build_dataset(years: list[int],\n",
        "                   vt_map: dict,\n",
        "                   a1: float, b1: float, a2: float, b2: float,\n",
        "                   neg_pos_ratio: float = 3.0,\n",
        "                   max_rows: int = 800_000,\n",
        "                   seed: int = 42,\n",
        "                   p2p98: tuple[float, float] | None = None):\n",
        "    \"\"\"\n",
        "    Devuelve X (n,4): [LST_SER, NDVI, TVDI, HAZ] y y (n,).\n",
        "    Si p2p98 se provee, usa esos límites globales (p02, p98) para normalizar LST_SER\n",
        "    en lugar de estimarlos sobre 'years'.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    if p2p98 is None:\n",
        "        p2, p98 = _estimate_lst_percentiles(years, seed=seed)\n",
        "    else:\n",
        "        p2, p98 = float(p2p98[0]), float(p2p98[1])\n",
        "\n",
        "    feats, labels = [], []\n",
        "    taken = 0\n",
        "    for y in years:\n",
        "        for path in _iter_full_parquets(y):\n",
        "            use_cols = [\"date\",\"NDVI_raw\",\"LST_raw\",\"CONF\",\"VT\"]\n",
        "            try:\n",
        "                df = pd.read_parquet(path, columns=use_cols)\n",
        "            except Exception:\n",
        "                continue\n",
        "            if df.empty:\n",
        "                continue\n",
        "\n",
        "            nd = df[\"NDVI_raw\"].to_numpy()\n",
        "            ls = df[\"LST_raw\"].to_numpy()\n",
        "            cf = df[\"CONF\"].to_numpy()\n",
        "            vt = pd.to_numeric(df[\"VT\"], errors=\"coerce\").to_numpy()\n",
        "            m_valid = (nd != NDVI_SENTINEL) & (ls != LST_SENTINEL) & (~np.isnan(vt)) & (vt > 0)\n",
        "            if not m_valid.any():\n",
        "                continue\n",
        "\n",
        "            nd = (nd[m_valid].astype(np.float32)/10000.0)   # [-1,1]\n",
        "            ls = (ls[m_valid].astype(np.float32)/10.0)      # °C\n",
        "            cf = (cf[m_valid].astype(np.float32))\n",
        "            vt = (vt[m_valid].astype(np.int16))\n",
        "\n",
        "            y_pos = (cf > 0).astype(np.uint8)\n",
        "            n_pos = int(y_pos.sum())\n",
        "            if n_pos == 0:\n",
        "                continue\n",
        "\n",
        "            idx_all = np.arange(len(y_pos))\n",
        "            idx_pos = idx_all[y_pos == 1]\n",
        "            idx_neg = idx_all[y_pos == 0]\n",
        "            n_neg_take = min(int(neg_pos_ratio * n_pos), idx_neg.size)\n",
        "            take_idx = np.concatenate([idx_pos, rng.choice(idx_neg, size=n_neg_take, replace=False)]) if n_neg_take > 0 else idx_pos\n",
        "\n",
        "            nd = nd[take_idx]; ls = ls[take_idx]; vt = vt[take_idx]; y_lab = y_pos[take_idx]\n",
        "\n",
        "            # 1) LST_SER con percentiles FIJOS\n",
        "            lst_ser = (ls - p2) / max(1e-6, (p98 - p2))\n",
        "            lst_ser = np.clip(lst_ser, 0.0, 1.0)\n",
        "\n",
        "            # 2) NDVI crudo\n",
        "            nd_feat = nd\n",
        "\n",
        "            # 3) TVDI desde rectas\n",
        "            lst_wet = a1*nd_feat + b1\n",
        "            lst_dry = a2*nd_feat + b2\n",
        "            span = np.maximum(1e-6, (lst_dry - lst_wet))\n",
        "            tvdi = np.clip((ls - lst_wet) / span, 0.0, 1.0)\n",
        "\n",
        "            # 4) HAZ desde VT\n",
        "            haz = np.array([vt_map.get(int(c), 0.0) for c in vt], dtype=np.float32)\n",
        "            haz = np.clip(haz, 0.0, 1.0)\n",
        "\n",
        "            X_part = np.stack([lst_ser, nd_feat, tvdi, haz], axis=1)\n",
        "            feats.append(X_part); labels.append(y_lab)\n",
        "\n",
        "            if sum(x.shape[0] for x in feats) >= max_rows:\n",
        "                break\n",
        "        if sum(x.shape[0] for x in feats) >= max_rows:\n",
        "            break\n",
        "\n",
        "    if not feats:\n",
        "        raise RuntimeError(\"No se pudo construir el dataset.\")\n",
        "    X = np.vstack(feats).astype(np.float32)\n",
        "    y = np.concatenate(labels).astype(np.uint8)\n",
        "    return X, y, (p2, p98)\n",
        "\n",
        "\n",
        "# 2) USAR percentiles de TRAIN para ambos splits en _time_split_by_date\n",
        "def _time_split_by_date(years_train: list[int], years_val: list[int],\n",
        "                        vt_map: dict, a1,b1,a2,b2, neg_pos_ratio, max_rows, seed):\n",
        "    # Estimar p02–p98 SOLO sobre TRAIN\n",
        "    p2p98_train = _estimate_lst_percentiles(years_train, seed=seed)\n",
        "\n",
        "    # Construir TRAIN y VALID usando los mismos límites\n",
        "    Xtr, ytr, _ = _build_dataset(years_train, vt_map, a1,b1,a2,b2,\n",
        "                                 neg_pos_ratio=neg_pos_ratio, max_rows=max_rows,\n",
        "                                 seed=seed, p2p98=p2p98_train)\n",
        "    Xva, yva, _ = _build_dataset(years_val,   vt_map, a1,b1,a2,b2,\n",
        "                                 neg_pos_ratio=neg_pos_ratio, max_rows=max_rows,\n",
        "                                 seed=seed+1, p2p98=p2p98_train)\n",
        "    # Devolver los percentiles históricos\n",
        "    return (Xtr, ytr, p2p98_train), (Xva, yva, p2p98_train)\n",
        "\n",
        "def _fdci_from_X(X: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
        "    if w is None:\n",
        "        w = np.ones(4, dtype=np.float32)\n",
        "    s = (X @ w.astype(np.float32) + 1.0) / 4.0\n",
        "    s = np.nan_to_num(s, nan=0.0, posinf=1.0, neginf=0.0)\n",
        "    return np.clip(s, 0.0, 1.0)\n",
        "\n",
        "def _rand_search_weights(X: np.ndarray, y: np.ndarray,\n",
        "                         n_candidates:int=200, seed:int=42,\n",
        "                         bounds=(-1.0, 1.0), metric:str=\"pr\"):\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    if np.unique(y).size < 2:\n",
        "        return np.array([1,1,1,1], dtype=np.float32), float(\"nan\")\n",
        "\n",
        "    best_s, best_w = -np.inf, None\n",
        "    cand = [np.array([1,1,1,1], dtype=np.float32),\n",
        "            np.array([1,0.5,1,0.5], dtype=np.float32),\n",
        "            np.array([0.7,0.7,0.7,0.7], dtype=np.float32)]\n",
        "    for _ in range(max(0, n_candidates - len(cand))):\n",
        "        cand.append(rng.uniform(bounds[0], bounds[1], size=4).astype(np.float32))\n",
        "\n",
        "    for w in cand:\n",
        "        s = _fdci_from_X(X, w)\n",
        "        try:\n",
        "            score = average_precision_score(y, s) if metric == \"pr\" else roc_auc_score(y, s)\n",
        "        except Exception:\n",
        "            score = -np.inf\n",
        "        if np.isfinite(score) and score > best_s:\n",
        "            best_s, best_w = score, w.copy()\n",
        "\n",
        "    if best_w is None:\n",
        "        best_w = np.array([1,1,1,1], dtype=np.float32)\n",
        "    return best_w, float(best_s)\n",
        "\n",
        "def _evaluate_scores(y_true: np.ndarray, scores: np.ndarray):\n",
        "    roc = roc_auc_score(y_true, scores)\n",
        "    pr  = average_precision_score(y_true, scores)\n",
        "\n",
        "    # Umbral óptimo por F1\n",
        "    P, R, T = precision_recall_curve(y_true, scores)\n",
        "    f1 = np.where((P+R)>0, 2*P*R/(P+R), 0.0)\n",
        "    i  = int(np.nanargmax(f1))\n",
        "    thr = float(T[i-1]) if i>0 and i-1 < len(T) else 0.5\n",
        "    return {\"roc_auc\": float(roc), \"pr_auc\": float(pr), \"f1_best\": float(np.max(f1)), \"thr_opt\": thr}\n",
        "\n",
        "def calibrate_weights(years_train_txt: str,\n",
        "                      years_val_txt: str,\n",
        "                      vt_table_df: pd.DataFrame,\n",
        "                      a1: float, b1: float, a2: float, b2: float,\n",
        "                      neg_pos_ratio: float = 3.0,\n",
        "                      max_rows: int = 800_000,\n",
        "                      n_candidates: int = 300,\n",
        "                      metric: str = \"pr\",\n",
        "                      seed:int = 42):\n",
        "    years_tr = _years_from_text(years_train_txt)\n",
        "    years_va = _years_from_text(years_val_txt)\n",
        "    if not years_tr or not years_va:\n",
        "        return None, \"Error: especifica años de train y validación.\", None, None\n",
        "\n",
        "    vt_map = vt_table_to_map(vt_table_df)\n",
        "    (Xtr, ytr, p_train), (Xva, yva, p_valid) = _time_split_by_date(\n",
        "        years_tr, years_va, vt_map, a1,b1,a2,b2,\n",
        "        neg_pos_ratio, max_rows, seed\n",
        "    )\n",
        "\n",
        "    if np.unique(ytr).size < 2:\n",
        "        return None, (\"Error: el TRAIN no tiene 0s y 1s tras el muestreo. \"\n",
        "                      \"Sube 'Negativos por positivo', amplía años TRAIN o incluye meses/días sin incendio.\"), None, None\n",
        "    if np.unique(yva).size < 2:\n",
        "        return None, (\"Error: la VALID no tiene 0s y 1s. Asegura que haya días sin incendio en VALID.\"), None, None\n",
        "\n",
        "    w_best, s_best = _rand_search_weights(Xtr, ytr, n_candidates=n_candidates, seed=seed, metric=metric)\n",
        "    scr_tr = _evaluate_scores(ytr, _fdci_from_X(Xtr, w_best))\n",
        "    scr_va = _evaluate_scores(yva, _fdci_from_X(Xva, w_best))\n",
        "\n",
        "    scores_tr = _fdci_from_X(Xtr, w_best)\n",
        "    scores_va = _fdci_from_X(Xva, w_best)\n",
        "    fpr_tr, tpr_tr, _ = roc_curve(ytr, scores_tr)\n",
        "    fpr_va, tpr_va, _ = roc_curve(yva, scores_va)\n",
        "    auc_tr = roc_auc_score(ytr, scores_tr)\n",
        "    auc_va = roc_auc_score(yva, scores_va)\n",
        "    fig_roc_tr = _plot_roc_same_style(fpr_tr, tpr_tr, auc_tr, tag=\"(train)\")\n",
        "    fig_roc_va = _plot_roc_same_style(fpr_va, tpr_va, auc_va, tag=\"(valid)\")\n",
        "\n",
        "    summary = (\n",
        "        f\"Normalización LST (°C) por serie:\\n\"\n",
        "        f\"  TRAIN  p02={p_train[0]:.2f}, p98={p_train[1]:.2f}\\n\"\n",
        "        f\"  VALID  p02={p_valid[0]:.2f}, p98={p_valid[1]:.2f}\\n\\n\"\n",
        "        f\"Pesos óptimos (train, métrica={metric.upper()}):\\n\"\n",
        "        f\"  w_lst={w_best[0]:+.3f} | w_ndvi={w_best[1]:+.3f} | w_tvdi={w_best[2]:+.3f} | w_haz={w_best[3]:+.3f}\\n\\n\"\n",
        "        f\"Train → PR-AUC={scr_tr['pr_auc']:.3f} · ROC-AUC={scr_tr['roc_auc']:.3f} · F1*={scr_tr['f1_best']:.3f} @thr≈{scr_tr['thr_opt']:.2f}\\n\"\n",
        "        f\"Valid → PR-AUC={scr_va['pr_auc']:.3f} · ROC-AUC={scr_va['roc_auc']:.3f} · F1*={scr_va['f1_best']:.3f} @thr≈{scr_va['thr_opt']:.2f}\"\n",
        "    )\n",
        "    return w_best, summary, scr_tr, scr_va, fig_roc_tr, fig_roc_va\n",
        "\n",
        "def _plot_roc_same_style(fpr, tpr, auc_roc, tag: str = \"\"):\n",
        "    fig, axr = plt.subplots(figsize=(6, 5))\n",
        "    axr.plot(fpr, tpr, lw=2, color=\"navy\", label=f\"AUC = {auc_roc:.3f}\")\n",
        "    axr.plot([0, 1], [0, 1], \"--\", color=\"gray\", label=\"aleatorio\")\n",
        "    axr.set_xlabel(\"FPR\")\n",
        "    axr.set_ylabel(\"TPR\")\n",
        "    ttl = \"Curva ROC — presencia/no-presencia\"\n",
        "    if tag:\n",
        "        ttl += f\" {tag}\"\n",
        "    axr.set_title(ttl)\n",
        "    axr.grid(ls=\":\")\n",
        "    axr.legend()\n",
        "    fig.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# ======================================================================\n",
        "# 14. INTERFAZ GRADIO – organización por pestañas\n",
        "# ======================================================================\n",
        "with gr.Blocks(title=\"MODIS 500 m — NDVI / LST / FDCI + HAZARD + FIRMS\") as demo:\n",
        "    gr.Markdown(r\"\"\"\n",
        "### Flujo de trabajo\n",
        "1) **Serie completa RAW anual (todas las celdas/día) + Parquet**\n",
        "2) **Explorar: Densidad (hexbin) · Mapa**\n",
        "3) **Extremos (0.01) y Rectas multi-año**\n",
        "4) **Histogramas multianuales (compactos)**\n",
        "\"\"\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        # ---------------------- TAB 1 · SERIE COMPLETA RAW -------------------------\n",
        "        with gr.TabItem(\"1) Serie completa RAW + Parquet\"):\n",
        "            gr.Markdown(\"#### Serie completa anual (RAW enteros) — NDVI/LST/FIRMS/VT (12 CSV mensuales)\")\n",
        "            with gr.Row():\n",
        "                year_full = gr.Number(2019, label=\"Año (2003–2020)\", precision=0)\n",
        "                lag_full  = gr.Number(16, label=\"Δ días NDVI vs LST (join)\", precision=0)\n",
        "                full_btn  = gr.Button(\"Exportar FULL\")\n",
        "            full_out = gr.Textbox(label=\"Estado exportación FULL\")\n",
        "\n",
        "            gr.Markdown(\"#### CSV ➜ Parquet (y eliminar CSV)\")\n",
        "            with gr.Row():\n",
        "                year_pq = gr.Number(2019, label=\"Año a convertir\", precision=0)\n",
        "                pq_btn  = gr.Button(\"Convertir a Parquet\")\n",
        "            pq_out = gr.Textbox(label=\"Estado conversión\")\n",
        "\n",
        "        # ---------------------- TAB 2 · EXPLORAR (SOLO DENSIDAD) -------------------\n",
        "        with gr.TabItem(\"2) Explorar (Densidad)\"):\n",
        "            gr.Markdown(\"#### Densidad NDVI–LST por año (hexbin + marginales)\")\n",
        "            with gr.Row():\n",
        "                year_hex   = gr.Number(2019, label=\"Año (2003–2020)\", precision=0)\n",
        "                mode_hex   = gr.Radio([\"densidad\", \"lat\", \"lon\"], value=\"densidad\", label=\"Color por\")\n",
        "                grid_hex   = gr.Slider(20, 120, value=50, step=2, label=\"Resolución (gridsize)\")\n",
        "                hex_btn    = gr.Button(\"Generar densidad\")\n",
        "            hex_fig = gr.Plot(label=\"Hexbin + marginales (Seaborn)\")\n",
        "\n",
        "            def _hex_wrapper(y, m, gsz):\n",
        "                try:\n",
        "                    return _hexbin_year(int(y), str(m), int(gsz))\n",
        "                except Exception as e:\n",
        "                    fig, ax = plt.subplots(figsize=(6, 2))\n",
        "                    ax.axis('off'); ax.text(.5,.5,str(e),ha='center',va='center',color='red')\n",
        "                    return fig\n",
        "\n",
        "            hex_btn.click(_hex_wrapper, [year_hex, mode_hex, grid_hex], hex_fig)\n",
        "\n",
        "        # ---------------------- TAB 3 · EXTREMOS & MULTI-AÑO -----------------------\n",
        "        with gr.TabItem(\"3) Extremos (0.01) y Multi-año\"):\n",
        "            gr.Markdown(\"#### Exportación extremos NDVI-LST (step = 0.01)\")\n",
        "            with gr.Row():\n",
        "                year_ext = gr.Number(2019, label=\"Año a procesar (2003–2020)\", precision=0)\n",
        "                ext_btn  = gr.Button(\"Exportar extremos\")\n",
        "            ext_out = gr.Textbox(label=\"Estado exportación extremos\")\n",
        "\n",
        "            gr.Markdown(\"#### Unificar CSV de extremos en uno solo\")\n",
        "            with gr.Row():\n",
        "                year_merge = gr.Number(2019, label=\"Año a unificar\", precision=0)\n",
        "                merge_btn  = gr.Button(\"Unificar y limpiar CSV\")\n",
        "            merge_out = gr.Textbox(label=\"Estado de unificación\")\n",
        "\n",
        "            gr.Markdown(\"#### Rectas húmeda/seca multi-año (EXT_ALL_NDVI_LST_YYYY.csv)\")\n",
        "            with gr.Row():\n",
        "                years_txt = gr.Textbox(\n",
        "                    \"2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020\",\n",
        "                    label=\"Años (separados por coma)\")\n",
        "                multi_btn = gr.Button(\"Generar rectas multi-año\")\n",
        "            multi_fig = gr.Plot(label=\"Rectas multi-año\")\n",
        "            multi_txt = gr.Textbox(label=\"Tabla de coeficientes\", lines=20)\n",
        "            multi_gallery = gr.Gallery(label=\"Por año: nube + rectas\", show_label=True)\n",
        "\n",
        "            def _multi_wrapper(yrs):\n",
        "                try:\n",
        "                    years = [int(y.strip()) for y in yrs.split(',') if y.strip()]\n",
        "                    if not years:\n",
        "                        raise ValueError(\"No se especificaron años.\")\n",
        "                    edge_list, gallery_imgs = [], []\n",
        "                    for y in years:\n",
        "                        path = f\"/content/drive/My Drive/{EXPORT_FOLDER}/EXT_ALL_NDVI_LST_{y}.csv\"\n",
        "                        if not os.path.exists(path):\n",
        "                            raise FileNotFoundError(f\"No existe: {path}\")\n",
        "                        df_y = pd.read_csv(path)\n",
        "                        (m_w, c_w), (m_d, c_d) = _fit_edges_from_extremes(df_y)\n",
        "                        edge_list.append({'year': y, 'm_w': m_w, 'c_w': c_w, 'm_d': m_d, 'c_d': c_d})\n",
        "                        img_arr = _make_year_scatter_figure(\n",
        "                            df_y, y, m_w, c_w, m_d, c_d, sample_max=200_000, show_curves=True\n",
        "                        )\n",
        "                        gallery_imgs.append((img_arr, f\"{y}\"))\n",
        "                    fig_all, table_txt = plot_multi_year_edges(edge_list)\n",
        "                    return fig_all, table_txt, gallery_imgs\n",
        "                except Exception as e:\n",
        "                    fig, ax = plt.subplots(figsize=(6, 2))\n",
        "                    ax.text(.5, .5, str(e), ha='center', va='center', color='red')\n",
        "                    ax.axis('off'); return fig, str(e), []\n",
        "\n",
        "        # ---------------------- TAB 4 · HISTOGRAMAS MULTIANUALES ------------------\n",
        "        with gr.TabItem(\"4) Histogramas multianuales (compactos)\"):\n",
        "            gr.Markdown(\"### Histogramas multianuales (doble columna) y resumen por año\")\n",
        "            years_compact = gr.Textbox(\n",
        "                \"2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020\",\n",
        "                label=\"Años (separados por coma)\"\n",
        "            )\n",
        "            compact_btn = gr.Button(\"Generar\")\n",
        "\n",
        "            fig_ndvi_all = gr.Plot(label=\"NDVI [-1,1] · todos los años\")\n",
        "            fig_lst_all  = gr.Plot(label=\"LST (°C) · todos los años\")\n",
        "            fig_conf_all = gr.Plot(label=\"CONF (>0) · todos los años\")\n",
        "            summary_tbl  = gr.Dataframe(label=\"Resumen por año\",\n",
        "                                        interactive=False,\n",
        "                                        wrap=True,\n",
        "                                        col_count=(12, \"fixed\"))\n",
        "\n",
        "            def _compact_wrapper(yrs):\n",
        "                try:\n",
        "                    return _run_compact_histograms(yrs)\n",
        "                except Exception as e:\n",
        "                    fig, ax = plt.subplots(figsize=(6, 2))\n",
        "                    ax.axis('off'); ax.text(.5,.5,str(e),ha='center',va='center',color='red')\n",
        "                    err_df = pd.DataFrame({\"error\":[str(e)]})\n",
        "                    return (fig, fig, fig, err_df)\n",
        "\n",
        "            compact_btn.click(_compact_wrapper, [years_compact],\n",
        "                              [fig_ndvi_all, fig_lst_all, fig_conf_all, summary_tbl])\n",
        "\n",
        "            # ----- VT con incendios (diarios y netos) -----\n",
        "            gr.Markdown(\"### VT con incendios · Histogramas por cobertura (LC_Type1)\")\n",
        "            with gr.Row():\n",
        "                years_vt = gr.Textbox(\n",
        "                    \"2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020\",\n",
        "                    label=\"Años (separados por coma) — para VT\"\n",
        "                )\n",
        "                show_conf_ck = gr.Checkbox(False, label=\"Mostrar confianza FIRMS (barras apiladas)\")\n",
        "                vt_btn = gr.Button(\"Generar VT con incendios\")\n",
        "\n",
        "            fig_vt_daily = gr.Plot(label=\"VT con incendios · eventos diarios (CONF>0)\")\n",
        "            fig_vt_net   = gr.Plot(label=\"VT con incendios · eventos netos (no consecutivos)\")\n",
        "\n",
        "            def _vt_wrapper(yrs, show_conf):\n",
        "                try:\n",
        "                    return _run_vt_histograms(yrs, bool(show_conf))\n",
        "                except Exception as e:\n",
        "                    fig, ax = plt.subplots(figsize=(6, 2))\n",
        "                    ax.axis('off'); ax.text(.5,.5,str(e),ha='center',va='center',color='red')\n",
        "                    return (fig, fig)\n",
        "\n",
        "            vt_btn.click(_vt_wrapper, [years_vt, show_conf_ck], [fig_vt_daily, fig_vt_net])\n",
        "\n",
        "        # ---------------------- TAB 5 · CALIBRACIÓN & VALIDACIÓN -----------------------\n",
        "        with gr.TabItem(\"5) Calibración & Validación\"):\n",
        "            gr.Markdown(\"### Calibración de pesos FDCI (w_lst, w_ndvi, w_tvdi, w_haz) y validación\")\n",
        "\n",
        "            with gr.Row():\n",
        "                years_tr_txt = gr.Textbox(\"2015,2016,2017,2018\", label=\"Años TRAIN (coma)\")\n",
        "                years_va_txt = gr.Textbox(\"2019,2020\",            label=\"Años VALID (coma)\")\n",
        "\n",
        "            with gr.Row():\n",
        "                a1_in = gr.Number(8.06, label=\"a₁ (húmeda)\")\n",
        "                b1_in = gr.Number( 0.22, label=\"b₁ (húmeda)\")\n",
        "                a2_in = gr.Number(-11.41, label=\"a₂ (seca)\")\n",
        "                b2_in = gr.Number(48.03, label=\"b₂ (seca)\")\n",
        "\n",
        "            gr.Markdown(\"#### Tabla HAZARD por cobertura (edita **HazardIndex**). Doble click para cambiar.\")\n",
        "            vt_table = gr.Dataframe(value=HAZARD_TABLE_DEFAULT.copy(),\n",
        "                                    headers=list(HAZARD_TABLE_DEFAULT.columns),\n",
        "                                    datatype=[\"number\",\"str\",\"number\",\"number\"],\n",
        "                                    row_count=(len(HAZARD_TABLE_DEFAULT), \"fixed\"),\n",
        "                                    interactive=True)\n",
        "\n",
        "            gr.Markdown(\"#### Pesos actuales (puedes fijarlos manualmente o dejar que el calibrador los busque)\")\n",
        "            with gr.Row():\n",
        "                w_lst_in  = gr.Slider(-1.0, 1.0, value=+1.0, step=0.01, label=\"w_lst  (LST serie)\")\n",
        "                w_ndvi_in = gr.Slider(-1.0, 1.0, value=-0.5, step=0.01, label=\"w_ndvi (NDVI crudo)\")\n",
        "                w_tvdi_in = gr.Slider(-1.0, 1.0, value=+1.0, step=0.01, label=\"w_tvdi (TVDI)\")\n",
        "                w_haz_in  = gr.Slider(-1.0, 1.0, value=+1.0, step=0.01, label=\"w_haz  (HAZARD)\")\n",
        "\n",
        "            with gr.Row():\n",
        "                neg_ratio = gr.Slider(0.0, 100.0, value=3.0, step=1.0, label=\"Negativos por positivo (ratio)\")\n",
        "                max_rows  = gr.Number(800_000, label=\"Máx. filas a usar\", precision=0)\n",
        "                n_cand    = gr.Number(300, label=\"Candidatos aleatorios\", precision=0)\n",
        "                metric_sel= gr.Radio([\"pr\",\"roc\"], value=\"pr\", label=\"Métrica objetivo (train)\")\n",
        "                seed_in   = gr.Number(42, label=\"Seed\", precision=0)\n",
        "\n",
        "            with gr.Row():\n",
        "                btn_cal = gr.Button(\"🔧 Calibrar pesos\")\n",
        "                btn_val = gr.Button(\"✅ Validar con pesos actuales\")\n",
        "            out_txt = gr.Textbox(label=\"Resumen calibración/validación\", lines=10)\n",
        "\n",
        "            with gr.Row():\n",
        "                roc_tr_plot = gr.Plot(label=\"ROC — TRAIN\")\n",
        "                roc_va_plot = gr.Plot(label=\"ROC — VALID\")\n",
        "\n",
        "            gr.Markdown(\"#### Mapa con pesos (elige LST min/max para normalizar)\")\n",
        "            with gr.Row():\n",
        "                map_date5 = gr.Textbox(\"2019-08-15\", label=\"Fecha mapa (AAAA-MM-DD)\")\n",
        "                lst_min5  = gr.Textbox(\"\", label=\"LST min (°C) · vacío = auto p02\")\n",
        "                lst_max5  = gr.Textbox(\"\", label=\"LST max (°C) · vacío = auto p98\")\n",
        "                add_firms5= gr.Checkbox(False, label=\"Añadir FIRMS VIIRS\")\n",
        "                btn_map5  = gr.Button(\"Mostrar mapa con pesos\")\n",
        "            map_html5 = gr.HTML(label=\"Mapa FDCI calibrado\")\n",
        "\n",
        "            # ---------- Callbacks ----------\n",
        "            def do_calibrate(yrs_tr, yrs_va, vt_df, a1, b1, a2, b2, ratio, mx, nc, met, seed):\n",
        "                try:\n",
        "                    w_best, summary, scr_tr, scr_va, fig_tr, fig_va = calibrate_weights(\n",
        "                        yrs_tr, yrs_va, vt_df, float(a1), float(b1), float(a2), float(b2),\n",
        "                        neg_pos_ratio=float(ratio), max_rows=int(mx), n_candidates=int(nc),\n",
        "                        metric=str(met), seed=int(seed)\n",
        "                    )\n",
        "                    if w_best is None:\n",
        "                        return summary, gr.update(), gr.update(), gr.update(), gr.update(), gr.update(), gr.update()\n",
        "                    return summary, float(w_best[0]), float(w_best[1]), float(w_best[2]), float(w_best[3]), fig_tr, fig_va\n",
        "                except Exception as e:\n",
        "                    return f\"Error calibrando: {e}\", gr.update(), gr.update(), gr.update(), gr.update(), gr.update(), gr.update()\n",
        "\n",
        "            def do_validate(yrs_tr, yrs_va, vt_df, a1, b1, a2, b2, ratio, mx, wlst, wnd, wtv, whz, seed):\n",
        "                try:\n",
        "                    vt_map = vt_table_to_map(vt_df)\n",
        "                    years_val = years_from_text(yrs_va)\n",
        "                    Xv, yv, _ = build_dataset(years_val, vt_map, float(a1), float(b1), float(a2), float(b2),\n",
        "                                              neg_pos_ratio=float(ratio), max_rows=int(mx), seed=int(seed)+9)\n",
        "                    w = np.array([wlst, wnd, wtv, whz], dtype=np.float32)\n",
        "                    scores = _fdci_from_X(Xv, w)\n",
        "                    txt = (f\"Validación con pesos actuales\\n\"\n",
        "                          f\"  w_lst={w[0]:.3f} | w_ndvi={w[1]:.3f} | w_tvdi={w[2]:.3f} | w_haz={w[3]:.3f}\\n\")\n",
        "                    roc_auc = roc_auc_score(yv, scores)\n",
        "                    pr_auc = average_precision_score(yv, scores)\n",
        "                    fpr, tpr, thr = precision_recall_curve(yv, scores)\n",
        "                    txt += f\"Valid • PR-AUC={pr_auc:.3f} • ROC-AUC={roc_auc:.3f}\\n\"\n",
        "                    fpr_v, tpr_v, _ = roc_curve(yv, scores)\n",
        "                    fig_roc_v = _plot_roc_same_style(fpr_v, tpr_v, roc_auc, tag=\"(valid)\")\n",
        "                    return txt, fig_roc_v\n",
        "                except Exception as e:\n",
        "                    return f\"Error validando: {e}\", None\n",
        "\n",
        "            def _do_map(date_str, a1,b1,a2,b2, wlst,wnd,wtv,whz, vt_df, lst_min_txt, lst_max_txt, add_f):\n",
        "                lm = None if str(lst_min_txt).strip()==\"\" else lst_min_txt\n",
        "                lx = None if str(lst_max_txt).strip()==\"\" else lst_max_txt\n",
        "                return _map_from_parquet(date_str, a1,b1,a2,b2, wlst,wnd,wtv,whz, vt_df,\n",
        "                                         add_firms_points=add_f, grid_deg=0.0045,\n",
        "                                         lst_norm_min=lm, lst_norm_max=lx)\n",
        "\n",
        "            btn_cal.click(\n",
        "                do_calibrate,\n",
        "                inputs=[years_tr_txt, years_va_txt, vt_table, a1_in, b1_in, a2_in, b2_in, neg_ratio, max_rows, n_cand, metric_sel, seed_in],\n",
        "                outputs=[out_txt, w_lst_in, w_ndvi_in, w_tvdi_in, w_haz_in, roc_tr_plot, roc_va_plot]\n",
        "            )\n",
        "\n",
        "            btn_val.click(do_validate,\n",
        "                          inputs=[years_tr_txt, years_va_txt, vt_table, a1_in,b1_in,a2_in,b2_in,\n",
        "                                  neg_ratio, max_rows, w_lst_in, w_ndvi_in, w_tvdi_in, w_haz_in, seed_in],\n",
        "                          outputs=[out_txt])\n",
        "\n",
        "            btn_map5.click(\n",
        "                _do_map,\n",
        "                inputs=[map_date5, a1_in, b1_in, a2_in, b2_in,\n",
        "                        w_lst_in, w_ndvi_in, w_tvdi_in, w_haz_in, vt_table, lst_min5, lst_max5, add_firms5],\n",
        "                outputs=[map_html5]\n",
        "            )\n",
        "\n",
        "    # ---------------------- CONEXIONES GLOBALES ----------------------\n",
        "    # Tab 1\n",
        "    def _full_wrapper(y, lag): return _export_full_year_cells_raw(y, ndvi_lag_days=int(lag))\n",
        "    full_btn.click(_full_wrapper, [year_full, lag_full], full_out)\n",
        "    pq_btn.click(_convert_full_csvs_to_parquet, [year_pq], pq_out)\n",
        "\n",
        "    # Tab 2\n",
        "    def _hex_wrapper(y, m, gsz):\n",
        "        try:\n",
        "            return _hexbin_year(int(y), str(m), int(gsz))\n",
        "        except Exception as e:\n",
        "            fig, ax = plt.subplots(figsize=(6, 2))\n",
        "            ax.axis('off'); ax.text(.5,.5,str(e),ha='center',va='center',color='red')\n",
        "            return fig\n",
        "    hex_btn.click(_hex_wrapper, [year_hex, mode_hex, grid_hex], hex_fig)\n",
        "\n",
        "    # Tab 3\n",
        "    ext_btn.click(export_extremes_single_year, [year_ext], ext_out)\n",
        "    merge_btn.click(merge_extreme_csv, [year_merge], merge_out)\n",
        "    multi_btn.click(_multi_wrapper, [years_txt], [multi_fig, multi_txt, multi_gallery])\n",
        "\n",
        "demo.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhAW1fW6YxsvJqambPkhL+"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}