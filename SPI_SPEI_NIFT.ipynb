{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "BnbLlRUH2E2M",
        "outputId": "befa6808-9f58-46bf-e884-2d26497eaa3e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "            <style>\n",
              "                .geemap-dark {\n",
              "                    --jp-widgets-color: white;\n",
              "                    --jp-widgets-label-color: white;\n",
              "                    --jp-ui-font-color1: white;\n",
              "                    --jp-layout-color2: #454545;\n",
              "                    background-color: #383838;\n",
              "                }\n",
              "                    \n",
              "                .geemap-dark .jupyter-button {\n",
              "                    --jp-layout-color3: #383838;\n",
              "                }\n",
              "                \n",
              "                .geemap-colab {\n",
              "                    background-color: var(--colab-primary-surface-color, white);\n",
              "                }\n",
              "                    \n",
              "                .geemap-colab .jupyter-button {\n",
              "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
              "                }\n",
              "            </style>\n",
              "            "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://cb2afe488ded0dda60.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cb2afe488ded0dda60.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://cb2afe488ded0dda60.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SUITE UNIFICADA (CHIRPS + MODIS PET) — SPI + SPEI + Análisis\n",
        "#   • SPI (k=1,3,6,12) sobre PR mensual CHIRPS\n",
        "#   • SPEI (k=1,3,6,12) usando balance hídrico (CHIRPS − MODIS PET)\n",
        "#   • Validación SPEI (CSIC/GEE vs Local)\n",
        "#   • SPEI(Local) vs ENSO mensual (ONI)\n",
        "#   • FIRMS VIIRS 375 m: export mensual (grilla CHIRPS) + visualización CSV\n",
        "#   • Métricas globales (punto-biserial + AUC/ROC)\n",
        "#   • Correlación geográficamente ponderada (GWSS) — unificado (ENSO opcional)\n",
        "#   • Grilla 1:1 alineada a CHIRPS (~0.05° ≈ 5 km)\n",
        "#   • Interfaz Gradio (2 pestañas)\n",
        "# ============================================================\n",
        "\n",
        "# =================== 0) INSTALACIÓN =========================\n",
        "!pip -q install \"gradio==5.35.0\" pandas numpy scipy matplotlib \\\n",
        "                 earthengine-api geemap==0.30.2 rpy2 rasterio \\\n",
        "                 geopandas shapely pyproj fiona scikit-learn\n",
        "\n",
        "# =================== 1) IMPORTS =============================\n",
        "import os, re, io, time, shutil, pathlib, traceback, itertools, warnings, datetime as dt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from matplotlib import colors\n",
        "from matplotlib.colors import TwoSlopeNorm\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from shapely.ops import unary_union\n",
        "\n",
        "# GIS / EE / Raster\n",
        "import ee, geemap, geopandas as gpd, rasterio\n",
        "from rasterio.features import rasterize\n",
        "from rasterio.transform import from_origin\n",
        "\n",
        "# R (rpy2)\n",
        "import rpy2.robjects as ro\n",
        "\n",
        "# ML/Stats\n",
        "from scipy.stats import pointbiserialr\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.neighbors import BallTree\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# =================== 1b) FUENTE (opcional Palatino) =========\n",
        "import matplotlib.font_manager as fm\n",
        "PALATINO_PATH = \"/content/drive/My Drive/fonts/palatino-linotype.ttf\"\n",
        "try:\n",
        "    from google.colab import auth as gcolab_auth, drive\n",
        "    gcolab_auth.authenticate_user()\n",
        "    drive.mount(\"/content/drive\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    if os.path.exists(PALATINO_PATH):\n",
        "        fm.fontManager.addfont(PALATINO_PATH)\n",
        "        PAL = fm.FontProperties(fname=PALATINO_PATH).get_name()\n",
        "    else:\n",
        "        PAL = \"Palatino Linotype\"\n",
        "except Exception:\n",
        "    PAL = \"serif\"\n",
        "\n",
        "mpl.rcParams.update({\n",
        "    \"font.family\":  \"serif\",\n",
        "    \"font.serif\":   [PAL],\n",
        "    \"axes.titleweight\": \"semibold\",\n",
        "    \"axes.labelsize\": 12,\n",
        "    \"axes.titlesize\": 13,\n",
        "    \"xtick.labelsize\": 11,\n",
        "    \"ytick.labelsize\": 11,\n",
        "    \"figure.dpi\": 120,\n",
        "    \"mathtext.fontset\": \"stix\",\n",
        "})\n",
        "\n",
        "# =================== 1c) EE INIT ============================\n",
        "try:\n",
        "    PROJECT_ID = \"ee-example\"  # añadir Cloud Project\n",
        "    ee.Initialize(project=PROJECT_ID)\n",
        "except Exception:\n",
        "    ee.Initialize()\n",
        "\n",
        "# =================== 2) CONSTANTES Y DIRS ===================\n",
        "DRIVE_DIR_SPI  = \"/content/drive/My Drive/SPI_NIFT_outputs\"\n",
        "DRIVE_DIR_SPEI = \"/content/drive/My Drive/SPEI_outputs_CHIRPS_MODIS\"\n",
        "os.makedirs(DRIVE_DIR_SPI, exist_ok=True)\n",
        "os.makedirs(DRIVE_DIR_SPEI, exist_ok=True)\n",
        "\n",
        "CSV_SPI_PR     = \"Grid5k_Mean_Prec.csv\"            # PR mensual (CHIRPS)\n",
        "CSV_SPEI_BAL   = \"Grid5k_WaterBalance_PRmPET.csv\"  # (PR − PET) mensual (CHIRPS−MODIS)\n",
        "FIRMS_COLLECTION = 'FIRMS'                         # VIIRS 375 m (en tu cuenta)\n",
        "EXPORT_FOLDER    = 'SPEI_outputs_CHIRPS_MODIS'     # carpeta en Drive\n",
        "\n",
        "# =================== 3) REGIÓN + GRILLA (CHIRPS) =============\n",
        "def _get_regions():\n",
        "    fc = (ee.FeatureCollection(\"FAO/GAUL/2015/level1\")\n",
        "            .filter(ee.Filter.inList(\"ADM1_NAME\", [\"Cundinamarca\", \"Boyaca\"])))\n",
        "    return fc, fc.union().geometry()\n",
        "\n",
        "def _chirps_proj():\n",
        "    return ee.ImageCollection(\"UCSB-CHG/CHIRPS/DAILY\").first().projection()\n",
        "\n",
        "def _make_grid(region):\n",
        "    \"\"\"Malla 1:1 con píxeles nativos CHIRPS (0.05°). cell_id = lon*1e4_lat*1e4\"\"\"\n",
        "    chirps_proj = _chirps_proj()\n",
        "    grid = region.coveringGrid(chirps_proj)\n",
        "    def _set_id(f):\n",
        "        cent = f.geometry().centroid(1).coordinates()\n",
        "        lon  = ee.Number(cent.get(0)).multiply(1e4).round()\n",
        "        lat  = ee.Number(cent.get(1)).multiply(1e4).round()\n",
        "        cid  = lon.format('%08d').cat('_').cat(lat.format('%08d'))\n",
        "        return f.set('cell_id', cid)\n",
        "    return grid.map(_set_id)\n",
        "\n",
        "def _chirps_deg_res():\n",
        "    tr = _chirps_proj().getInfo()['transform']\n",
        "    a, _, _, _, e, _ = tr\n",
        "    return abs(a), abs(e)\n",
        "\n",
        "# =================== 4) COLECCIONES MENSUALES =================\n",
        "def _chirps_monthly_ic(start_date, end_date):\n",
        "    start = ee.Date(start_date)\n",
        "    end   = ee.Date(end_date).advance(1, 'month')\n",
        "    n_months = end.difference(start, 'month').subtract(1).int()\n",
        "    months = ee.List.sequence(0, n_months)\n",
        "    def _month_img(m):\n",
        "        mstart = start.advance(m, 'month')\n",
        "        mend   = mstart.advance(1, 'month')\n",
        "        daily  = ee.ImageCollection(\"UCSB-CHG/CHIRPS/DAILY\").filterDate(mstart, mend)\n",
        "        img = daily.select('precipitation').sum()\n",
        "        return img.rename('pr').set('system:time_start', mstart.millis())\n",
        "    return ee.ImageCollection.fromImages(months.map(_month_img))\n",
        "\n",
        "_MODIS_PET_ID = None\n",
        "def _pick_modis_pet_id():\n",
        "    global _MODIS_PET_ID\n",
        "    if _MODIS_PET_ID: return _MODIS_PET_ID\n",
        "    for cid in [\"MODIS/061/MOD16A2GF\",\"MODIS/061/MOD16A2\",\"MODIS/006/MOD16A2GF\",\"MODIS/006/MOD16A2\"]:\n",
        "        try:\n",
        "            _ = ee.ImageCollection(cid).first().getInfo()\n",
        "            _MODIS_PET_ID = cid; break\n",
        "        except Exception:\n",
        "            continue\n",
        "    if not _MODIS_PET_ID:\n",
        "        raise ValueError(\"No se pudo abrir MOD16A2* en EE.\")\n",
        "    return _MODIS_PET_ID\n",
        "\n",
        "def _modis_pet_monthly_ic(start_date, end_date, scale_factor=0.1):\n",
        "    cid   = _pick_modis_pet_id()\n",
        "    start = ee.Date(start_date)\n",
        "    end   = ee.Date(end_date).advance(1, 'month')\n",
        "    n_months = end.difference(start, 'month').subtract(1).int()\n",
        "    months = ee.List.sequence(0, n_months)\n",
        "    ic_pet = ee.ImageCollection(cid).select('PET')  # 0.1 mm/8-d\n",
        "    def _month_img(m):\n",
        "        mstart = start.advance(m, 'month')\n",
        "        mend   = mstart.advance(1, 'month')\n",
        "        img = ic_pet.filterDate(mstart, mend).sum().multiply(scale_factor)\n",
        "        return img.rename('pet').set('system:time_start', mstart.millis())\n",
        "    return ee.ImageCollection.fromImages(months.map(_month_img))\n",
        "\n",
        "def _stack_ic(ic, band):\n",
        "    return (ic.select(band)\n",
        "              .map(lambda i: i.select([0], [i.date().format(\"yyyyMM\")]))\n",
        "              .toBands())\n",
        "\n",
        "# =================== 5) EXPORTS A DRIVE =======================\n",
        "def export_chirps_pr(start_date, end_date):\n",
        "    \"\"\"Paso SPI-1: Exporta precipitación mensual CHIRPS (mm/mes) a CSV (grilla CHIRPS).\"\"\"\n",
        "    try:\n",
        "        sd, ed = map(lambda s: pd.to_datetime(s).to_pydatetime(), (start_date, end_date))\n",
        "        if sd > ed: return \"❌ La fecha inicial es posterior a la final.\"\n",
        "        region_fc, ROI = _get_regions()\n",
        "        grid_fc = _make_grid(ROI)\n",
        "        ic_pr   = _chirps_monthly_ic(start_date, end_date)\n",
        "        pr_img  = _stack_ic(ic_pr, 'pr')\n",
        "        chirps_proj = _chirps_proj()\n",
        "        cell_res    = chirps_proj.nominalScale()\n",
        "        ee.batch.Export.table.toDrive(\n",
        "            collection     = pr_img.reduceRegions(grid_fc, ee.Reducer.mean(), scale=cell_res, crs=chirps_proj),\n",
        "            description    = \"Export_PR_CHIRPS\",\n",
        "            fileNamePrefix = CSV_SPI_PR[:-4],\n",
        "            fileFormat     = \"CSV\"\n",
        "        ).start()\n",
        "        return (\"⏳ Exportación iniciada (CHIRPS mensual).\\n\"\n",
        "                f\"Al finalizar tendrás **{CSV_SPI_PR}** en tu Drive.\")\n",
        "    except Exception:\n",
        "        return f\"❌ Error exportando PR:\\n{traceback.format_exc()}\"\n",
        "\n",
        "def export_pr_minus_pet(start_date, end_date):\n",
        "    \"\"\"Paso SPEI-1: Exporta (PR_mean − PET_mean) mensual por celda (grilla CHIRPS).\"\"\"\n",
        "    try:\n",
        "        sd, ed = map(lambda s: pd.to_datetime(s).to_pydatetime(), (start_date, end_date))\n",
        "        if sd > ed: return \"❌ La fecha inicial es posterior a la final.\"\n",
        "        _, ROI  = _get_regions(); grid_fc = _make_grid(ROI)\n",
        "        ic_pr  = _chirps_monthly_ic(start_date, end_date)          # 'pr'\n",
        "        ic_pet = _modis_pet_monthly_ic(start_date, end_date, 0.1)  # 'pet'\n",
        "        pr_img  = _stack_ic(ic_pr,  'pr');  pet_img = _stack_ic(ic_pet, 'pet')\n",
        "\n",
        "        pr_bands  = pr_img.bandNames()\n",
        "        pet_bands = pet_img.bandNames()\n",
        "        common_bands = ee.List(pr_bands.map(lambda b: ee.Algorithms.If(pet_bands.contains(b), b, None))).removeAll([None])\n",
        "\n",
        "        chirps_proj  = _chirps_proj();    chirps_scale = chirps_proj.nominalScale()\n",
        "        modis_proj   = ee.ImageCollection(_pick_modis_pet_id()).first().projection()\n",
        "        modis_scale  = modis_proj.nominalScale()\n",
        "\n",
        "        fc_pr = pr_img.reduceRegions(grid_fc, ee.Reducer.mean(), scale=chirps_scale, crs=chirps_proj, tileScale=2)\n",
        "        fc_pet = pet_img.reduceRegions(grid_fc, ee.Reducer.mean(), scale=modis_scale,  crs=modis_proj,  tileScale=2)\n",
        "\n",
        "        joined = ee.Join.inner().apply(primary=fc_pr, secondary=fc_pet,\n",
        "                                       condition=ee.Filter.equals(leftField='cell_id', rightField='cell_id'))\n",
        "\n",
        "        def _to_balance(jfeat):\n",
        "            left  = ee.Feature(jfeat.get('primary'))\n",
        "            right = ee.Feature(jfeat.get('secondary'))\n",
        "            ldict, rdict = ee.Dictionary(left.toDictionary()), ee.Dictionary(right.toDictionary())\n",
        "            diffs = common_bands.map(lambda b: ee.Algorithms.If(\n",
        "                                      ldict.contains(b),\n",
        "                                      ee.Algorithms.If(rdict.contains(b),\n",
        "                                                       ee.Number(ldict.get(b)).subtract(ee.Number(rdict.get(b))), None),\n",
        "                                      None))\n",
        "            diff_dict = ee.Dictionary.fromLists(common_bands, diffs)\n",
        "            return ee.Feature(None, {'cell_id': left.get('cell_id')}).set(diff_dict)\n",
        "\n",
        "        fc_bal = ee.FeatureCollection(joined.map(_to_balance))\n",
        "\n",
        "        ee.batch.Export.table.toDrive(\n",
        "            collection     = fc_bal,\n",
        "            description    = \"Export_WBAL_CHIRPS_MODIS_mean_minus_mean\",\n",
        "            fileNamePrefix = CSV_SPEI_BAL[:-4],\n",
        "            fileFormat     = \"CSV\"\n",
        "        ).start()\n",
        "\n",
        "        return (\"⏳ Exportación iniciada (PR_mean − PET_mean mensual).\\n\"\n",
        "                f\"Al finalizar tendrás **{CSV_SPEI_BAL}** en tu Drive.\")\n",
        "    except Exception:\n",
        "        return f\"❌ Error exportando (PR−PET):\\n{traceback.format_exc()}\"\n",
        "\n",
        "# =================== 6) SPI y SPEI con R =====================\n",
        "def compute_spi_from_csv(k_list_str=\"1,3,6,12\"):\n",
        "    \"\"\"Paso SPI-2: Lee PR mensual (CSV_SPI_PR) y calcula SPI_k → CSV largos.\"\"\"\n",
        "    try:\n",
        "        csv_path = f\"/content/drive/My Drive/{CSV_SPI_PR}\"\n",
        "        if not Path(csv_path).exists():\n",
        "            return \"❌ CSV no encontrado; completa antes el Paso 1 (PR).\"\n",
        "        df_raw = pd.read_csv(csv_path)\n",
        "        df_raw.columns = [str(c).strip() for c in df_raw.columns]\n",
        "        rename_map, month_keys = {}, []\n",
        "        for c in df_raw.columns:\n",
        "            m = re.search(r'(?<!\\d)(\\d{4})(\\d{2})(?!\\d)', str(c))\n",
        "            if m:\n",
        "                y, mo = int(m.group(1)), int(m.group(2))\n",
        "                if 1900 <= y <= 2100 and 1 <= mo <= 12:\n",
        "                    key = f\"{y}{mo:02d}\"\n",
        "                    rename_map[c] = key; month_keys.append(key)\n",
        "        if not month_keys: return \"❌ El CSV no trae columnas con patrón YYYYMM.\"\n",
        "        df_raw = df_raw.rename(columns=rename_map)\n",
        "        band_cols = sorted(set(month_keys))\n",
        "        cell_ids  = df_raw[\"cell_id\"].astype(str).tolist()\n",
        "        n_cells   = len(cell_ids)\n",
        "        idx_map   = pd.DataFrame({\"idx\": range(n_cells), \"cell_id\": cell_ids})\n",
        "        idx_map.to_csv(\"idx2cellid.csv\", index=False)\n",
        "        matrix = df_raw[band_cols].to_numpy().T\n",
        "        pd.DataFrame(matrix, columns=range(n_cells)).to_csv(\"SPI_Input.csv\", index=False)\n",
        "        first_band  = band_cols[0]; start_year, start_month = int(first_band[:4]), int(first_band[4:6])\n",
        "        k_vals = [int(k) for k in k_list_str.split(',') if k.strip().isdigit()]\n",
        "        r_code = f'''\n",
        "        if (!requireNamespace(\"SPEI\", quietly=TRUE))\n",
        "          install.packages(\"SPEI\", repos=\"https://cran.rstudio.com\");\n",
        "        if (!requireNamespace(\"zoo\", quietly=TRUE))\n",
        "          install.packages(\"zoo\", repos=\"https://cran.rstudio.com\");\n",
        "        if (!requireNamespace(\"data.table\", quietly=TRUE))\n",
        "          install.packages(\"data.table\", repos=\"https://cran.rstudio.com\");\n",
        "        library(SPEI); library(zoo); library(data.table);\n",
        "        mat <- as.matrix(read.csv(\"SPI_Input.csv\", header=TRUE))\n",
        "        tsm <- ts(mat, start=c({start_year},{start_month}), frequency=12)\n",
        "        for (k in c({','.join(map(str,k_vals))})) {{\n",
        "          spi_k <- spi(tsm, k, distribution=\"Gamma\", na.rm=TRUE)\n",
        "          m_out <- t(as.matrix(spi_k$fitted)); m_out[!is.finite(m_out)] <- NA\n",
        "          fwrite(as.data.table(m_out), file=sprintf(\"SPI_%d_month_raw.csv\", k),\n",
        "                 sep=\",\", quote=FALSE, col.names=FALSE)\n",
        "        }}\n",
        "        '''\n",
        "        ro.r(r_code)\n",
        "        date_map = [f\"{b[:4]}-{b[4:]}\" for b in band_cols]\n",
        "        out_files = []\n",
        "        for k in k_vals:\n",
        "            mat = pd.read_csv(f\"SPI_{k}_month_raw.csv\", header=None).values\n",
        "            df  = pd.DataFrame(mat, columns=date_map[:mat.shape[1]])\n",
        "            df.insert(0, \"idx\", range(n_cells))\n",
        "            df = (df.merge(idx_map, on=\"idx\").drop(columns=\"idx\")\n",
        "                    .melt(id_vars=\"cell_id\", var_name=\"date\", value_name=\"spi\"))\n",
        "            out_csv = f\"SPI_{k}_month.csv\"\n",
        "            df.to_csv(out_csv, index=False); shutil.copy(out_csv, DRIVE_DIR_SPI); out_files.append(out_csv)\n",
        "        return (\"✔️ SPI calculado y exportado:\\n‣ \" + \"\\n‣ \".join(out_files) +\n",
        "                f\"\\nArchivos en {DRIVE_DIR_SPI}\")\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error SPI: {e}\\n{traceback.format_exc()}\"\n",
        "\n",
        "def compute_spei_from_csv(k_list_str=\"1,3,6,12\"):\n",
        "    \"\"\"\n",
        "    Paso SPEI-2: Lee (PR − PET) mensual (CSV_SPEI_BAL) y calcula SPEI_k → CSV largos.\n",
        "    Además, exporta parámetros de la log-logística (xi, alpha, kappa) por mes (m01..m12)\n",
        "    y llama a export_spei_params_geotiffs para guardar TIFFs multibanda (36 bandas) en Drive.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1) Localizar CSV de balance hídrico en Drive\n",
        "        csv_path = f\"/content/drive/My Drive/{CSV_SPEI_BAL}\"\n",
        "        if not Path(csv_path).exists():\n",
        "            return \"❌ CSV no encontrado; completa antes el Paso 1 (PR−PET).\"\n",
        "\n",
        "        # 2) Leer y estandarizar nombres de columnas YYYYMM\n",
        "        df_raw = pd.read_csv(csv_path)\n",
        "        df_raw.columns = [str(c).strip() for c in df_raw.columns]\n",
        "\n",
        "        band_cols_raw = [c for c in df_raw.columns if re.fullmatch(r\"\\d{6}_\\d{6}\", str(c))]\n",
        "        rename_map = {c: c.split(\"_\")[0] for c in band_cols_raw} if band_cols_raw else {}\n",
        "        if not rename_map:\n",
        "            for c in df_raw.columns:\n",
        "                m = re.search(r'(?<!\\d)(\\d{4})(\\d{2})(?!\\d)', str(c))\n",
        "                if m:\n",
        "                    y, mo = int(m.group(1)), int(m.group(2))\n",
        "                    if 1900 <= y <= 2100 and 1 <= mo <= 12:\n",
        "                        rename_map[c] = f\"{y}{mo:02d}\"\n",
        "        df_raw = df_raw.rename(columns=rename_map)\n",
        "        if \"cell_id\" not in df_raw.columns:\n",
        "            return \"❌ Falta la columna 'cell_id' en el CSV de balance.\"\n",
        "        band_cols = sorted({c for c in df_raw.columns if re.fullmatch(r\"\\d{6}\", str(c))})\n",
        "        if not band_cols:\n",
        "            return \"❌ No se detectaron columnas YYYYMM en el CSV de balance.\"\n",
        "\n",
        "        # 3) Índices de celdas y matriz sitio×tiempo para R\n",
        "        cell_ids = df_raw[\"cell_id\"].astype(str).tolist()\n",
        "        n_cells  = len(cell_ids)\n",
        "        idx_map  = pd.DataFrame({\"idx\": range(n_cells), \"cell_id\": cell_ids})\n",
        "        idx_map.to_csv(\"SPEI_idx2cellid.csv\", index=False)\n",
        "\n",
        "        matrix = df_raw[band_cols].to_numpy().T  # filas→tiempo, columnas→sitios\n",
        "        pd.DataFrame(matrix, columns=range(n_cells)).to_csv(\"SPEI_Input.csv\", index=False)\n",
        "\n",
        "        first_band  = band_cols[0]\n",
        "        start_year, start_month = int(first_band[:4]), int(first_band[4:6])\n",
        "\n",
        "        # 4) Parsear k\n",
        "        k_vals = [int(k) for k in str(k_list_str).split(',') if str(k).strip().isdigit()]\n",
        "        if not k_vals:\n",
        "            return \"❌ k_list_str vacío o inválido.\"\n",
        "\n",
        "        # 5) Ejecutar R: SPEI + export robusto de coeficientes log-logísticos por mes\n",
        "        r_code = f'''\n",
        "        if (!requireNamespace(\"SPEI\", quietly=TRUE))\n",
        "          install.packages(\"SPEI\", repos=\"https://cran.rstudio.com\");\n",
        "        if (!requireNamespace(\"zoo\", quietly=TRUE))\n",
        "          install.packages(\"zoo\", repos=\"https://cran.rstudio.com\");\n",
        "        if (!requireNamespace(\"data.table\", quietly=TRUE))\n",
        "          install.packages(\"data.table\", repos=\"https://cran.rstudio.com\");\n",
        "        library(SPEI); library(zoo); library(data.table);\n",
        "\n",
        "        mat <- as.matrix(read.csv(\"SPEI_Input.csv\", header=TRUE))\n",
        "        tsm <- ts(mat, start=c({start_year},{start_month}), frequency=12)\n",
        "        ts_freq <- frequency(tsm)\n",
        "\n",
        "        dir.create(\"SPEI_params\", showWarnings=FALSE)\n",
        "\n",
        "        get_param_mat <- function(coe, p, ts_freq) {{\n",
        "          # Devuelve matriz sitios × ts_freq (m01..m12) para 'p', robusto a dims.\n",
        "          if (is.null(coe)) return(NULL)\n",
        "\n",
        "          # Array 3D\n",
        "          if (is.array(coe) && length(dim(coe)) == 3) {{\n",
        "            dims <- dim(coe)\n",
        "            par_dim <- 1L\n",
        "            month_dim <- if (dims[2] == ts_freq) 2L else if (dims[3] == ts_freq) 3L else {{\n",
        "              w <- which(dims == 12L); if (length(w)) w[[1]] else 3L\n",
        "            }}\n",
        "            site_dim <- setdiff(1:3, c(par_dim, month_dim))[1]\n",
        "\n",
        "            p_idx <- if (!is.null(dimnames(coe)) && !is.null(dimnames(coe)[[par_dim]])) {{\n",
        "              which(dimnames(coe)[[par_dim]] == p)\n",
        "            }} else if (p == \"xi\") 1L else if (p == \"alpha\") 2L else 3L\n",
        "            if (length(p_idx) == 0) p_idx <- 1L\n",
        "\n",
        "            a <- switch(par_dim,\n",
        "              `1` = coe[p_idx,,, drop=FALSE],\n",
        "              `2` = coe[,p_idx,, drop=FALSE],\n",
        "              `3` = coe[,,p_idx, drop=FALSE]\n",
        "            )\n",
        "\n",
        "            adims <- dim(a)\n",
        "            amonth <- which(adims == ts_freq)[1]; if (is.na(amonth)) amonth <- 3L\n",
        "            asite  <- setdiff(1:3, c(1L, amonth))[1]\n",
        "            a <- aperm(a, c(asite, amonth, setdiff(1:3, c(asite, amonth))))\n",
        "            m <- drop(a)\n",
        "            if (is.null(dim(m))) m <- matrix(m, nrow=1, byrow=TRUE)\n",
        "            colnames(m) <- sprintf(\"m%02d\", seq_len(ncol(m)))\n",
        "            return(m)\n",
        "          }}\n",
        "\n",
        "          # Matriz 2D\n",
        "          if (is.matrix(coe)) {{\n",
        "            rnames <- rownames(coe); cnames <- colnames(coe)\n",
        "            if (nrow(coe) == 3L) {{\n",
        "              pid <- if (!is.null(rnames)) which(rnames == p) else {{\n",
        "                if (p == \"xi\") 1L else if (p == \"alpha\") 2L else 3L }}\n",
        "              v <- coe[pid, , drop=TRUE]\n",
        "              m <- matrix(as.numeric(v), nrow=1)\n",
        "            }} else if (ncol(coe) == 3L) {{\n",
        "              pid <- if (!is.null(cnames)) which(cnames == p) else {{\n",
        "                if (p == \"xi\") 1L else if (p == \"alpha\") 2L else 3L }}\n",
        "              v <- coe[, pid, drop=TRUE]\n",
        "              m <- matrix(as.numeric(v), nrow=1, byrow=TRUE)\n",
        "            }} else {{\n",
        "              v <- as.numeric(coe); if (length(v) %% ts_freq != 0L) v <- v[seq_len(ts_freq)]\n",
        "              m <- matrix(v, nrow=1, byrow=TRUE)\n",
        "            }}\n",
        "            colnames(m) <- sprintf(\"m%02d\", seq_len(ncol(m)))\n",
        "            return(m)\n",
        "          }}\n",
        "\n",
        "          # Vector\n",
        "          v <- as.numeric(coe); if (length(v) %% ts_freq != 0L) v <- v[seq_len(ts_freq)]\n",
        "          m <- matrix(v, nrow=1, byrow=TRUE)\n",
        "          colnames(m) <- sprintf(\"m%02d\", seq_len(ncol(m)))\n",
        "          return(m)\n",
        "        }}\n",
        "\n",
        "        for (k in c({','.join(map(str, k_vals))})) {{\n",
        "          spei_k <- spei(tsm, k, na.rm=TRUE)\n",
        "          m_out <- t(as.matrix(spei_k$fitted)); m_out[!is.finite(m_out)] <- NA\n",
        "          fwrite(as.data.table(m_out), file=sprintf(\"SPEI_%d_month_raw.csv\", k),\n",
        "                 sep=\",\", quote=FALSE, col.names=FALSE)\n",
        "\n",
        "          coe <- spei_k$coefficients\n",
        "          if (!is.null(coe)) {{\n",
        "             xi_mat    <- get_param_mat(coe, \"xi\",    ts_freq)\n",
        "             alpha_mat <- get_param_mat(coe, \"alpha\", ts_freq)\n",
        "             kappa_mat <- get_param_mat(coe, \"kappa\", ts_freq)\n",
        "\n",
        "             if (!is.null(xi_mat))    fwrite(as.data.table(xi_mat),    file=sprintf(\"SPEI_params/SPEI_params_k%d_xi.csv\",    k), sep=\",\", quote=FALSE, col.names=TRUE)\n",
        "             if (!is.null(alpha_mat)) fwrite(as.data.table(alpha_mat), file=sprintf(\"SPEI_params/SPEI_params_k%d_alpha.csv\", k), sep=\",\", quote=FALSE, col.names=TRUE)\n",
        "             if (!is.null(kappa_mat)) fwrite(as.data.table(kappa_mat), file=sprintf(\"SPEI_params/SPEI_params_k%d_kappa.csv\", k), sep=\",\", quote=FALSE, col.names=TRUE)\n",
        "          }}\n",
        "        }}\n",
        "        '''\n",
        "        ro.r(r_code)\n",
        "\n",
        "        # 6) Convertir matrices SPEI sitio×tiempo a CSV largo y copiar a Drive\n",
        "        date_map = [f\"{b[:4]}-{b[4:]}\" for b in band_cols]\n",
        "        out_files = []\n",
        "        for k in k_vals:\n",
        "            raw_path = f\"SPEI_{k}_month_raw.csv\"\n",
        "            if not Path(raw_path).exists():\n",
        "                continue\n",
        "            mat = pd.read_csv(raw_path, header=None).values  # sitios × tiempo\n",
        "            df  = pd.DataFrame(mat, columns=date_map[:mat.shape[1]])\n",
        "            df.insert(0, \"idx\", range(n_cells))\n",
        "            df = (df.merge(idx_map, on=\"idx\").drop(columns=\"idx\")\n",
        "                    .melt(id_vars=\"cell_id\", var_name=\"date\", value_name=\"spei\"))\n",
        "            out_csv = f\"SPEI_{k}_month.csv\"\n",
        "            df.to_csv(out_csv, index=False)\n",
        "            shutil.copy(out_csv, os.path.join(DRIVE_DIR_SPEI, os.path.basename(out_csv)))\n",
        "            out_files.append(out_csv)\n",
        "\n",
        "        # 7) Exportar TIFF multibanda (36 bandas) directo a Drive reutilizando la función existente\n",
        "        tiff_msg = \"\"\n",
        "        try:\n",
        "            tiff_msg = export_spei_params_geotiffs(k_list_str)\n",
        "        except NameError:\n",
        "            tiff_msg = \"⚠️ Define export_spei_params_geotiffs() antes de llamar a compute_spei_from_csv (ejecuta la celda donde está definida).\"\n",
        "\n",
        "        # 8) Mensaje final\n",
        "        msg = (\"✔️ SPEI calculado y exportado:\\n‣ \" + \"\\n‣ \".join(out_files)\n",
        "               + f\"\\nCarpeta CSV: {DRIVE_DIR_SPEI}\")\n",
        "        if Path(\"SPEI_params\").exists():\n",
        "            msg += \"\\n✔️ Parámetros (xi, alpha, kappa) por mes guardados en ./SPEI_params/\"\n",
        "        if tiff_msg:\n",
        "            msg += f\"\\n{tiff_msg}\"\n",
        "        return msg\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error SPEI: {e}\\n{traceback.format_exc()}\"\n",
        "\n",
        "# =================== 7) RASTERIZAR / PLOT ====================\n",
        "def _aligned_transform(bounds):\n",
        "    xmin, ymin, xmax, ymax = bounds\n",
        "    res_x, res_y = _chirps_deg_res()\n",
        "    width  = int(np.round((xmax - xmin) / res_x))\n",
        "    height = int(np.round((ymax - ymin) / res_y))\n",
        "    transform = from_origin(xmin, ymax, res_x, res_y)\n",
        "    return transform, width, height\n",
        "\n",
        "def rasterize_grid_csv(csv_long, target_date, out_tif, value_col, nodata_val=-9999.0):\n",
        "    df = pd.read_csv(csv_long).query(\"date == @target_date\")\n",
        "    if df.empty:\n",
        "        raise ValueError(f\"Fecha {target_date} no encontrada en {Path(csv_long).name}.\")\n",
        "    _, ROI  = _get_regions()\n",
        "    grid_fc = _make_grid(ROI)\n",
        "    gdf     = geemap.ee_to_gdf(grid_fc).merge(df, on='cell_id', how='left')\n",
        "    gdf[value_col] = gdf[value_col].astype('float32').fillna(nodata_val)\n",
        "    bounds = gdf.total_bounds\n",
        "    transform, width, height = _aligned_transform(bounds)\n",
        "    arr = rasterize(((geom, val) for geom, val in zip(gdf.geometry, gdf[value_col])),\n",
        "                    out_shape=(height, width), transform=transform,\n",
        "                    fill=nodata_val, dtype=\"float32\")\n",
        "    with rasterio.open(out_tif, \"w\", driver=\"GTiff\", height=height, width=width,\n",
        "                       count=1, dtype=\"float32\", crs=\"EPSG:4326\",\n",
        "                       transform=transform, nodata=nodata_val) as dst:\n",
        "        dst.write(arr, 1)\n",
        "\n",
        "def plot_index_map(csv_dir, stem, k, date_str, value_col, title, drive_out_dir):\n",
        "    try:\n",
        "        region_fc, _ = _get_regions()\n",
        "        gdf_regions  = geemap.ee_to_gdf(region_fc)\n",
        "        csv_long = f\"{csv_dir}/{stem}_{k}_month.csv\"\n",
        "        if not Path(csv_long).exists():\n",
        "            return None, f\"Archivo no encontrado: {Path(csv_long).name}\"\n",
        "        tif_path = f\"/content/{stem}{k}_{date_str.replace('-', '')}.tif\"\n",
        "        rasterize_grid_csv(csv_long, date_str, tif_path, value_col)\n",
        "        shutil.copy(tif_path, os.path.join(drive_out_dir, os.path.basename(tif_path)))\n",
        "        from matplotlib.colors import TwoSlopeNorm\n",
        "        with rasterio.open(tif_path) as src:\n",
        "            data = np.ma.masked_equal(src.read(1), src.nodata)\n",
        "            norm = TwoSlopeNorm(vmin=-3, vcenter=0, vmax=3) if value_col.lower() in (\"spi\",\"spei\") else None\n",
        "            fig, ax = plt.subplots(figsize=(8, 6))\n",
        "            im = ax.imshow(\n",
        "                data, cmap=\"RdBu\" if norm else \"jet\", norm=norm,\n",
        "                extent=[src.bounds.left, src.bounds.right, src.bounds.bottom, src.bounds.top]\n",
        "            )\n",
        "            gpd.GeoSeries(unary_union(gdf_regions.boundary.geometry)).plot(\n",
        "                ax=ax, edgecolor=\"black\", linewidth=0.7, facecolor=\"none\")\n",
        "            ax.set_xlabel(\"Longitud\"); ax.set_ylabel(\"Latitud\"); ax.set_title(title)\n",
        "            plt.colorbar(im, ax=ax, label=value_col.upper()); fig.tight_layout()\n",
        "        return fig, \"✔️ GeoTIFF generado\"\n",
        "    except Exception as e:\n",
        "        fig, ax = plt.subplots(); ax.axis('off'); ax.text(0.5,0.5,str(e), ha='center', va='center', color='red')\n",
        "        return fig, f\"❌ {e}\"\n",
        "\n",
        "def export_spei_params_geotiffs(k_list_str=\"1,3,6,12\",\n",
        "                                params_dir=\"SPEI_params\",\n",
        "                                out_dir=DRIVE_DIR_SPEI,\n",
        "                                fname_fmt=\"SPEI_Params_TS{k}.tif\",\n",
        "                                nodata_val=-9999.0):\n",
        "    \"\"\"\n",
        "    Genera, por cada k, un GeoTIFF multibanda (36 bandas = 12 meses × 3 parámetros),\n",
        "    con bandas en el orden: xi_01..xi_12, alpha_01..alpha_12, kappa_01..kappa_12.\n",
        "    Resolución/CRS: grilla CHIRPS (EPSG:4326), sobre ROI Cundinamarca+Boyacá.\n",
        "    \"\"\"\n",
        "    # Grilla base y geometría alineada a CHIRPS\n",
        "    _, ROI = _get_regions()\n",
        "    grid_fc  = _make_grid(ROI)\n",
        "    gdf_base = geemap.ee_to_gdf(grid_fc)\n",
        "\n",
        "    bounds = gdf_base.total_bounds  # xmin, ymin, xmax, ymax\n",
        "    res_x, res_y = _chirps_deg_res()\n",
        "    width  = int(np.round((bounds[2]-bounds[0]) / res_x))\n",
        "    height = int(np.round((bounds[3]-bounds[1]) / res_y))\n",
        "    transform = from_origin(bounds[0], bounds[3], res_x, res_y)\n",
        "\n",
        "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Mapeo idx -> cell_id\n",
        "    idx_map = pd.read_csv(\"SPEI_idx2cellid.csv\", dtype={\"cell_id\": str})\n",
        "\n",
        "    k_vals = [int(k) for k in str(k_list_str).split(\",\") if str(k).strip().isdigit()]\n",
        "    if not k_vals:\n",
        "        return \"❌ k_list_str vacío o inválido.\"\n",
        "\n",
        "    results = []\n",
        "    for k in k_vals:\n",
        "        # Archivos de parámetros generados en R\n",
        "        f_xi    = os.path.join(params_dir, f\"SPEI_params_k{k}_xi.csv\")\n",
        "        f_alpha = os.path.join(params_dir, f\"SPEI_params_k{k}_alpha.csv\")\n",
        "        f_kappa = os.path.join(params_dir, f\"SPEI_params_k{k}_kappa.csv\")\n",
        "        if not (Path(f_xi).exists() and Path(f_alpha).exists() and Path(f_kappa).exists()):\n",
        "            results.append(f\"⚠️ Faltan CSV de parámetros (k={k})\"); continue\n",
        "\n",
        "        xi    = pd.read_csv(f_xi, dtype=float)\n",
        "        alpha = pd.read_csv(f_alpha, dtype=float)\n",
        "        kappa = pd.read_csv(f_kappa, dtype=float)\n",
        "\n",
        "        # Asegurar nombres m01..m12 si fuese necesario\n",
        "        if xi.shape[1] == 12 and not all(c.startswith(\"m\") for c in xi.columns):\n",
        "            xi.columns    = [f\"m{m:02d}\" for m in range(1,13)]\n",
        "            alpha.columns = [f\"m{m:02d}\" for m in range(1,13)]\n",
        "            kappa.columns = [f\"m{m:02d}\" for m in range(1,13)]\n",
        "\n",
        "        # Añadir idx para merge con cell_id\n",
        "        xi[\"idx\"]    = range(len(xi))\n",
        "        alpha[\"idx\"] = range(len(alpha))\n",
        "        kappa[\"idx\"] = range(len(kappa))\n",
        "\n",
        "        # Ensamblar tabla por celda con 36 columnas (xi_01.., alpha_01.., kappa_01..)\n",
        "        df = idx_map.copy()\n",
        "        for p_name, df_p in [(\"xi\", xi), (\"alpha\", alpha), (\"kappa\", kappa)]:\n",
        "            df_p = df_p.merge(idx_map[[\"idx\"]], on=\"idx\", how=\"right\").sort_values(\"idx\")\n",
        "            for m in range(1, 13):\n",
        "                df[f\"{p_name}_{m:02d}\"] = pd.to_numeric(df_p.get(f\"m{m:02d}\", np.nan), errors=\"coerce\")\n",
        "\n",
        "        # Merge con geometría de grilla\n",
        "        gdf = gdf_base.merge(df, on=\"cell_id\", how=\"left\")\n",
        "\n",
        "        # Bandas y escritura GeoTIFF\n",
        "        band_names = [f\"xi_{m:02d}\" for m in range(1,13)] + \\\n",
        "                     [f\"alpha_{m:02d}\" for m in range(1,13)] + \\\n",
        "                     [f\"kappa_{m:02d}\" for m in range(1,13)]\n",
        "\n",
        "        tif_local = f\"/content/{fname_fmt.format(k=k)}\"\n",
        "        profile = {\n",
        "            \"driver\": \"GTiff\",\n",
        "            \"height\": height,\n",
        "            \"width\": width,\n",
        "            \"count\": len(band_names),\n",
        "            \"dtype\": \"float32\",\n",
        "            \"crs\": \"EPSG:4326\",\n",
        "            \"transform\": transform,\n",
        "            \"nodata\": nodata_val,\n",
        "            \"tiled\": True,\n",
        "            \"compress\": \"LZW\",\n",
        "        }\n",
        "        with rasterio.open(tif_local, \"w\", **profile) as dst:\n",
        "            for bi, bname in enumerate(band_names, start=1):\n",
        "                vals = pd.to_numeric(gdf[bname], errors=\"coerce\").astype(\"float32\").fillna(nodata_val)\n",
        "                arr = rasterize(\n",
        "                    ((geom, val) for geom, val in zip(gdf.geometry, vals)),\n",
        "                    out_shape=(height, width), transform=transform,\n",
        "                    fill=nodata_val, dtype=\"float32\",\n",
        "                )\n",
        "                dst.write(arr, bi)\n",
        "            # Metadatos\n",
        "            dst.update_tags(\n",
        "                creator=\"Hidro-Suite (SPEI params)\",\n",
        "                description=f\"Coeficientes log-logística SPEI TS-{k} (xi, alpha, kappa) por mes\",\n",
        "                timeseries_k=k,\n",
        "                bands=\",\".join(band_names)\n",
        "            )\n",
        "\n",
        "        out_path = os.path.join(out_dir, os.path.basename(tif_local))\n",
        "        shutil.copy(tif_local, out_path)\n",
        "        results.append(os.path.basename(out_path))\n",
        "\n",
        "    ok = [r for r in results if r.lower().endswith(\".tif\")]\n",
        "    extra = [r for r in results if not r.lower().endswith(\".tif\")]\n",
        "    msg = (\"✔️ GeoTIFFs de parámetros guardados:\\n‣ \" + \"\\n‣ \".join(ok)) if ok else \"⚠️ No se generaron TIFF.\"\n",
        "    if extra:\n",
        "        msg += (\"\\n\\nObservaciones:\\n- \" + \"\\n- \".join(extra))\n",
        "    msg += f\"\\nCarpeta: {out_dir}\"\n",
        "    return msg\n",
        "\n",
        "# =================== 8) NIFT (sobre SPI) =====================\n",
        "_DRY_CLASSES = {\"mild\": (0.0, -1.0), \"moderate\": (-1.0, -1.5), \"severe\": (-1.5, -2.0), \"extreme\": (-2.0, -np.inf)}\n",
        "DEFAULT_WEIGHTS_BRASIL_NETO = {\n",
        "    \"P1\": 0.125, \"P2\": 0.125, \"P3\": 0.009, \"P4\": 0.034, \"P5\": 0.071,\n",
        "    \"P6\": 0.136, \"P7\": 0.100, \"P8\": 0.075, \"P9\": 0.075, \"P10\": 0.250\n",
        "}\n",
        "DEFAULT_WEIGHTS_UNIFORME = {f\"P{i}\": 0.10 for i in range(1, 11)}\n",
        "_NEGATIVE_PARAMS = [\"P7\", \"P10\"]\n",
        "def _sanitize_weights(w: dict, renormalize: bool = True) -> dict:\n",
        "    \"\"\"Valida claves P1..P10, fuerza no-negatividad y (opcional) renormaliza a suma=1.\"\"\"\n",
        "    w = {f\"P{i}\": float(w.get(f\"P{i}\", 0.0)) for i in range(1, 11)}\n",
        "    for k in w:\n",
        "        if not np.isfinite(w[k]) or w[k] < 0: w[k] = 0.0\n",
        "    s = sum(w.values())\n",
        "    if renormalize and s > 0:\n",
        "        w = {k: v / s for k, v in w.items()}\n",
        "    return w\n",
        "\n",
        "def _detect_events(series, run_min=3):\n",
        "    events, run, start_idx = [], [], None\n",
        "    for i, val in enumerate(series):\n",
        "        if np.isfinite(val) and val <= 0:\n",
        "            if not run: start_idx = i\n",
        "            run.append(val)\n",
        "        else:\n",
        "            if len(run) >= run_min:\n",
        "                events.append((start_idx, len(run), np.sum(run)))\n",
        "            run, start_idx = [], None\n",
        "    if len(run) >= run_min:\n",
        "        events.append((start_idx, len(run), np.sum(run)))\n",
        "    return events\n",
        "\n",
        "def _sen_slope(y, x):\n",
        "    if len(y) < 2: return np.nan\n",
        "    slopes = [(y[j]-y[i])/(x[j]-x[i]) for i, j in itertools.combinations(range(len(y)), 2)]\n",
        "    return np.median(slopes)\n",
        "\n",
        "def _compute_nift_single(cell_df, run_min=3):\n",
        "    s_all = cell_df[\"spi\"].astype(float).values\n",
        "    mask  = np.isfinite(s_all)\n",
        "    if mask.sum() == 0:\n",
        "        return dict(P1=0, P2=0, P3=0, P4=0, P5=0, P6=0, P7=np.nan, P8=np.nan, P9=np.nan)\n",
        "    s = s_all[mask]\n",
        "    total_valid = mask.sum()\n",
        "    pct = {cls: 100.0 * np.logical_and(s <= hi, s > lo).sum() / total_valid if total_valid else 0\n",
        "           for cls, (hi, lo) in _DRY_CLASSES.items()}\n",
        "    events = _detect_events(s_all, run_min)\n",
        "    if events:\n",
        "        durations  = np.array([e[1] for e in events], float)\n",
        "        severities = np.abs(np.array([e[2] for e in events], float))\n",
        "        P1 = float(len(events)); P2 = float(np.mean(severities/durations))\n",
        "    else:\n",
        "        P1 = P2 = 0.0; durations = severities = np.array([], float)\n",
        "    months_idx = np.flatnonzero(mask).astype(float); P7 = _sen_slope(s, months_idx)\n",
        "    if durations.size:\n",
        "        starts = np.array([e[0] for e in events], float)\n",
        "        P8 = _sen_slope(durations,  starts); P9 = _sen_slope(severities, starts)\n",
        "    else:\n",
        "        P8 = P9 = np.nan\n",
        "    return dict(P1=P1, P2=P2, P3=pct['mild'], P4=pct['moderate'], P5=pct['severe'], P6=pct['extreme'], P7=P7, P8=P8, P9=P9)\n",
        "\n",
        "def _prepare_mean_precip(start=\"1985-01-01\", end=\"2024-12-31\"):\n",
        "    cm = _chirps_monthly_ic(start, end).select('pr')\n",
        "    n_years = (pd.to_datetime(end) - pd.to_datetime(start)).days / 365.25\n",
        "    ann_pr  = cm.sum().divide(n_years)\n",
        "    chirps_proj = _chirps_proj(); cell_res = chirps_proj.nominalScale()\n",
        "    _, ROI   = _get_regions(); grid_fc  = _make_grid(ROI)\n",
        "    pr_fc    = ann_pr.reduceRegions(grid_fc, ee.Reducer.mean(), scale=cell_res, crs=chirps_proj)\n",
        "    gdf_pr   = geemap.ee_to_gdf(pr_fc).rename(columns={'mean': 'prec'})\n",
        "    return gdf_pr[['cell_id', 'prec']]\n",
        "\n",
        "def compute_nift(k_list_str=\"1,3,6,12\", run_min=3, weights: dict | None = None, renormalize=True):\n",
        "    \"\"\"\n",
        "    Calcula NIFT permitiendo pesos editables desde UI.\n",
        "    - weights: dict con claves 'P1'..'P10'. Por defecto usa Brasil Neto.\n",
        "    - renormalize=True: reescala para que sumen 1 si fuese necesario.\n",
        "    - Exporta cada parámetro (P1-P10, Pn, NIFT) como un GeoTIFF individual a la carpeta de Drive (DRIVE_DIR_SPI) al momento del cálculo.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- CONFIGURACIÓN DE GEOMETRÍA PARA RASTERIZAR (alineado a CHIRPS) ---\n",
        "        _, ROI = _get_regions()\n",
        "        grid_fc  = _make_grid(ROI)\n",
        "        gdf_base = geemap.ee_to_gdf(grid_fc)\n",
        "        bounds = gdf_base.total_bounds\n",
        "        res_x, res_y = _chirps_deg_res()\n",
        "        width  = int(np.round((bounds[2]-bounds[0]) / res_x))\n",
        "        height = int(np.round((bounds[3]-bounds[1]) / res_y))\n",
        "        transform = from_origin(bounds[0], bounds[3], res_x, res_y)\n",
        "        nodata_val = -9999.0\n",
        "        # --- FIN CONFIGURACIÓN GEOMETRÍA ---\n",
        "\n",
        "        # 1) elegir pesos\n",
        "        if weights is None:\n",
        "            weights = DEFAULT_WEIGHTS_BRASIL_NETO.copy()\n",
        "        weights = _sanitize_weights(weights, renormalize=renormalize)\n",
        "\n",
        "        # 2) parseo de k\n",
        "        k_vals = [int(k) for k in str(k_list_str).split(',') if str(k).strip().isdigit()]\n",
        "        if not k_vals:\n",
        "            return \"❌ No se proporcionó ningún k válido.\"\n",
        "\n",
        "        resultados = []\n",
        "        for k in k_vals:\n",
        "            csv_long = f\"{DRIVE_DIR_SPI}/SPI_{k}_month.csv\"\n",
        "            if not Path(csv_long).exists():\n",
        "                resultados.append(f\"⚠️ SPI-{k} no encontrado\"); continue\n",
        "\n",
        "            # 3) computar parámetros P1..P9 por celda\n",
        "            df = pd.read_csv(csv_long, dtype={'cell_id': str})\n",
        "            out = []\n",
        "            for cid, grp in df.groupby('cell_id'):\n",
        "                param = _compute_nift_single(grp.sort_values('date'), run_min)\n",
        "                param['cell_id'] = cid\n",
        "                out.append(param)\n",
        "            nift_df = pd.DataFrame(out)\n",
        "\n",
        "            # 4) P10 = precipitación media anual (ya lo haces con EE)\n",
        "            pr_df   = _prepare_mean_precip()\n",
        "            nift_df = nift_df.merge(pr_df, on='cell_id', how='left').rename(columns={'prec':'P10'})\n",
        "\n",
        "            # 5) normalización 0..1 con signo correcto (P7,P10 negativas)\n",
        "            for col in [f\"P{i}\" for i in range(1, 11)]:\n",
        "                vals = pd.to_numeric(nift_df[col], errors=\"coerce\")\n",
        "                if vals.notna().sum() == 0:\n",
        "                    nift_df[f\"{col}n\"] = 0.0\n",
        "                    continue\n",
        "                mx, mn = np.nanmax(vals), np.nanmin(vals)\n",
        "                if np.isclose(mx, mn, atol=1e-12):\n",
        "                    nift_df[f\"{col}n\"] = 0.0\n",
        "                elif col in _NEGATIVE_PARAMS:\n",
        "                    nift_df[f\"{col}n\"] = (mx - vals) / (mx - mn)\n",
        "                else:\n",
        "                    nift_df[f\"{col}n\"] = (vals - mn) / (mx - mn)\n",
        "\n",
        "            # 6) NIFT = sum_j w_j * Pjn * 100\n",
        "            cols_n = [f\"P{i}n\" for i in range(1, 11)]\n",
        "            w_vec  = np.array([weights[f\"P{i}\"] for i in range(1, 11)], float)\n",
        "            nift_df['NIFT'] = nift_df[cols_n].to_numpy().dot(w_vec) * 100.0\n",
        "\n",
        "            # 7) meta\n",
        "            nift_df['date'] = df['date'].max()\n",
        "\n",
        "            # 8) guardar CSV y preset usado\n",
        "            out_csv = f\"NIFT_k{k}.csv\"\n",
        "            nift_df.to_csv(out_csv, index=False)\n",
        "            shutil.copy(out_csv, os.path.join(DRIVE_DIR_SPI, os.path.basename(out_csv)))\n",
        "            with open(os.path.join(DRIVE_DIR_SPI, f\"NIFT_weights_k{k}.json\"), \"w\") as f:\n",
        "                import json; json.dump({\"weights\": weights, \"k\": k, \"run_min\": run_min}, f, indent=2)\n",
        "\n",
        "            # --- NUEVO: Exportar cada parámetro como un GeoTIFF individual ---\n",
        "            tiff_msg = \"\"\n",
        "            try:\n",
        "                params_to_export = [f\"P{i}\" for i in range(1, 11)] + \\\n",
        "                                   [f\"P{i}n\" for i in range(1, 11)] + \\\n",
        "                                   ['NIFT']\n",
        "                n_tiffs_saved = 0\n",
        "                for param in params_to_export:\n",
        "                    if param not in nift_df.columns:\n",
        "                        continue\n",
        "\n",
        "                    # Merge con la grilla base\n",
        "                    gdf = gdf_base.merge(nift_df[[\"cell_id\", param]], on=\"cell_id\", how=\"left\")\n",
        "                    vals = pd.to_numeric(gdf[param], errors=\"coerce\").astype(\"float32\").fillna(nodata_val)\n",
        "\n",
        "                    # Rasterizar\n",
        "                    arr = rasterize(\n",
        "                        ((geom, val) for geom, val in zip(gdf.geometry, vals)),\n",
        "                        out_shape=(height, width), transform=transform,\n",
        "                        fill=nodata_val, dtype=\"float32\",\n",
        "                    )\n",
        "\n",
        "                    # Escribir GeoTIFF local y copiar a Drive\n",
        "                    tif_local = f\"/content/NIFT_Param_{param}_TS{k}.tif\"\n",
        "                    profile = {\n",
        "                        \"driver\": \"GTiff\", \"height\": height, \"width\": width, \"count\": 1,\n",
        "                        \"dtype\": \"float32\", \"crs\": \"EPSG:4326\", \"transform\": transform,\n",
        "                        \"nodata\": nodata_val, \"tiled\": True, \"compress\": \"LZW\",\n",
        "                    }\n",
        "                    with rasterio.open(tif_local, \"w\", **profile) as dst:\n",
        "                        dst.write(arr, 1)\n",
        "                        dst.update_tags(description=f\"NIFT Parameter {param} for Timescale {k}\")\n",
        "\n",
        "                    out_path = os.path.join(DRIVE_DIR_SPI, os.path.basename(tif_local))\n",
        "                    shutil.copy(tif_local, out_path)\n",
        "                    n_tiffs_saved += 1\n",
        "\n",
        "                if n_tiffs_saved > 0:\n",
        "                    tiff_msg = f\"\\n✔️ {n_tiffs_saved} GeoTIFFs de parámetros guardados en {DRIVE_DIR_SPI}.\"\n",
        "\n",
        "            except Exception as e_tiff:\n",
        "                tiff_msg = f\"\\n⚠️ Error al guardar GeoTIFFs de parámetros para k={k}: {e_tiff}\"\n",
        "            # --- FIN NUEVO ---\n",
        "\n",
        "            msg = f\"✔️ NIFT-{k} calculado (preset w sum={sum(weights.values()):.3f}) → {out_csv}\"\n",
        "            msg += tiff_msg\n",
        "            resultados.append(msg)\n",
        "\n",
        "        return \"\\n\".join(resultados)\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error NIFT: {e}\"\n",
        "\n",
        "def export_nift_geotiffs(k_list_str=\"1,3,6,12\", out_dir=DRIVE_DIR_SPI,\n",
        "                         fname_fmt=\"NIFT_TS{k}.tif\", nodata_val=-9999.0):\n",
        "    \"\"\"\n",
        "    Exporta un GeoTIFF por cada k con el valor final de NIFT por celda (0–100).\n",
        "    • Resolución/CRS: alineado a CHIRPS (~0.05°, EPSG:4326).\n",
        "    • Salida: copia los .tif a out_dir (por defecto, DRIVE_DIR_SPI).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- grilla base y geometría alineada a CHIRPS ---\n",
        "        _, ROI = _get_regions()\n",
        "        grid_fc  = _make_grid(ROI)\n",
        "        gdf_base = geemap.ee_to_gdf(grid_fc)\n",
        "\n",
        "        bounds = gdf_base.total_bounds  # xmin, ymin, xmax, ymax\n",
        "        res_x, res_y = _chirps_deg_res()\n",
        "        width  = int(np.round((bounds[2]-bounds[0]) / res_x))\n",
        "        height = int(np.round((bounds[3]-bounds[1]) / res_y))\n",
        "        transform = from_origin(bounds[0], bounds[3], res_x, res_y)\n",
        "\n",
        "        Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # --- lista de k ---\n",
        "        k_vals = [int(k) for k in str(k_list_str).split(\",\") if str(k).strip().isdigit()]\n",
        "        if not k_vals:\n",
        "            return \"❌ k_list_str vacío o inválido.\"\n",
        "\n",
        "        saved = []\n",
        "        for k in k_vals:\n",
        "            csv_path = f\"{DRIVE_DIR_SPI}/NIFT_k{k}.csv\"\n",
        "            if not Path(csv_path).exists():\n",
        "                saved.append(f\"⚠️ Falta {Path(csv_path).name}\");\n",
        "                continue\n",
        "\n",
        "            df_k = pd.read_csv(csv_path, dtype={\"cell_id\": str})\n",
        "            if \"NIFT\" not in df_k.columns:\n",
        "                saved.append(f\"❌ {Path(csv_path).name} sin columna 'NIFT'\")\n",
        "                continue\n",
        "\n",
        "            # Merge con grilla y rasterización\n",
        "            gdf = gdf_base.merge(df_k[[\"cell_id\", \"NIFT\"]], on=\"cell_id\", how=\"left\")\n",
        "            gdf[\"NIFT\"] = pd.to_numeric(gdf[\"NIFT\"], errors=\"coerce\").astype(\"float32\").fillna(nodata_val)\n",
        "\n",
        "            arr = rasterize(\n",
        "                ((geom, val) for geom, val in zip(gdf.geometry, gdf[\"NIFT\"])),\n",
        "                out_shape=(height, width),\n",
        "                transform=transform,\n",
        "                fill=nodata_val,\n",
        "                dtype=\"float32\",\n",
        "            )\n",
        "\n",
        "            # Escribir GeoTIFF (temporal en /content → copiar a Drive)\n",
        "            tif_local = f\"/content/{fname_fmt.format(k=k)}\"\n",
        "            profile = {\n",
        "                \"driver\": \"GTiff\",\n",
        "                \"height\": height,\n",
        "                \"width\": width,\n",
        "                \"count\": 1,\n",
        "                \"dtype\": \"float32\",\n",
        "                \"crs\": \"EPSG:4326\",\n",
        "                \"transform\": transform,\n",
        "                \"nodata\": nodata_val,\n",
        "                \"tiled\": True,\n",
        "                \"compress\": \"LZW\",\n",
        "            }\n",
        "            with rasterio.open(tif_local, \"w\", **profile) as dst:\n",
        "                dst.write(arr, 1)\n",
        "                # opcional: metadatos\n",
        "                dst.update_tags(creator=\"Hidro-Suite (NIFT)\",\n",
        "                                description=f\"NIFT TS-{k} (0–100), alineado a CHIRPS\",\n",
        "                                timeseries_k=k)\n",
        "\n",
        "            out_path = os.path.join(out_dir, os.path.basename(tif_local))\n",
        "            shutil.copy(tif_local, out_path)\n",
        "            saved.append(os.path.basename(out_path))\n",
        "\n",
        "        ok = [s for s in saved if s.lower().endswith(\".tif\")]\n",
        "        extra = [s for s in saved if not s.lower().endswith(\".tif\")]\n",
        "        msg = (\"✔️ GeoTIFFs guardados:\\n‣ \" + \"\\n‣ \".join(ok)) if ok else \"⚠️ No se generaron TIFF.\"\n",
        "        if extra:\n",
        "            msg += (\"\\n\\nObservaciones:\\n- \" + \"\\n- \".join(extra))\n",
        "        msg += f\"\\nCarpeta: {out_dir}\"\n",
        "        return msg\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error exportando NIFT a GeoTIFF: {e}\\n{traceback.format_exc()}\"\n",
        "\n",
        "from PIL import Image\n",
        "def save_png_trim(fig, out_path, dpi=600, bg='white', thr=252, pad_px=0):\n",
        "    \"\"\"\n",
        "    Guarda 'fig' como PNG de alta resolución.\n",
        "    \"\"\"\n",
        "    import io, numpy as _np\n",
        "    buf = io.BytesIO()\n",
        "    # guardado inicial (aprovecha bbox tight para minimizar)\n",
        "    fig.savefig(buf, format=\"png\", dpi=int(dpi),\n",
        "                facecolor=bg, edgecolor=bg,\n",
        "                bbox_inches=\"tight\", pad_inches=0.01)\n",
        "    buf.seek(0)\n",
        "\n",
        "    im = Image.open(buf).convert(\"RGBA\")\n",
        "    arr = _np.array(im)            # HxWx4\n",
        "    rgb, alpha = arr[..., :3], arr[..., 3]\n",
        "    # Trata transparencia como blanco puro\n",
        "    rgb = _np.where(alpha[..., None] == 0, 255, rgb)\n",
        "\n",
        "    # máscara de \"no-blanco\" (si cualquier canal < thr)\n",
        "    nonwhite = (rgb < thr).any(axis=2)\n",
        "    if nonwhite.any():\n",
        "        ys, xs = _np.where(nonwhite)\n",
        "        y0, y1 = ys.min(), ys.max() + 1\n",
        "        x0, x1 = xs.min(), xs.max() + 1\n",
        "        # acolchado\n",
        "        y0 = max(0, y0 - pad_px); x0 = max(0, x0 - pad_px)\n",
        "        y1 = min(im.height, y1 + pad_px); x1 = min(im.width, x1 + pad_px)\n",
        "        im = im.crop((x0, y0, x1, y1))\n",
        "    im.save(out_path, format=\"PNG\", optimize=True)\n",
        "\n",
        "\n",
        "# --- Conjuntos de parámetros para figuras compactas ---\n",
        "_SPI_GROUPS = {\n",
        "    \"G1\": [\"P1\",\"P2\"],              # dinámica de eventos\n",
        "    \"G2\": [\"P3\",\"P4\",\"P5\",\"P6\"],    # % mild/moderate/severe/extreme\n",
        "    \"G3\": [\"P7\",\"P8\",\"P9\"],         # tendencias (Sen)\n",
        "    \"G4\": [\"P10\"],                  # precipitación media anual\n",
        "}\n",
        "\n",
        "def _spi_plot_maps(k_list_str=\"1,3,6,12\", use_normalized=False):\n",
        "    try:\n",
        "        k_vals = [int(k) for k in k_list_str.split(\",\") if k.strip().isdigit()]\n",
        "        if not k_vals: raise ValueError(\"No se especificó ningún k válido.\")\n",
        "\n",
        "        # Base CHIRPS / grilla\n",
        "        _, ROI       = _get_regions()\n",
        "        grid_fc      = _make_grid(ROI)\n",
        "        gdf_base     = geemap.ee_to_gdf(grid_fc)\n",
        "\n",
        "        region_fc, _ = _get_regions()\n",
        "        gdf_regions  = geemap.ee_to_gdf(region_fc)\n",
        "        single_border = unary_union(gdf_regions.boundary.geometry)\n",
        "\n",
        "        bounds = gdf_base.total_bounds\n",
        "        res_x, res_y = _chirps_deg_res()\n",
        "        width  = int(np.round((bounds[2]-bounds[0])/res_x))\n",
        "        height = int(np.round((bounds[3]-bounds[1])/res_y))\n",
        "        transform = from_origin(bounds[0], bounds[3], res_x, res_y)\n",
        "\n",
        "        # --- figura de parámetros P1–P10 ---\n",
        "        var_params = [f\"P{i}\" for i in range(1,11)]\n",
        "        n_rows, n_cols = len(var_params), len(k_vals)\n",
        "        fig_params, ax_p = plt.subplots(n_rows, n_cols,\n",
        "                                        figsize=(3.1*n_cols, 2.4*n_rows),\n",
        "                                        sharex=True, sharey=True, dpi=300)\n",
        "        if n_cols == 1:\n",
        "            ax_p = np.expand_dims(ax_p, 1)\n",
        "\n",
        "        # 1) NORMALIZACIÓN POR FILA (igual que en _spi_plot_maps_grouped)\n",
        "        row_norms = []\n",
        "        for r, var in enumerate(var_params):\n",
        "            colname = var + (\"n\" if use_normalized else \"\")\n",
        "            vals_concat = []\n",
        "            for k in k_vals:\n",
        "                df_k = pd.read_csv(f\"{DRIVE_DIR_SPI}/NIFT_k{k}.csv\", dtype={\"cell_id\": str})\n",
        "                if colname in df_k.columns:\n",
        "                    vals_concat.append(pd.to_numeric(df_k[colname], errors=\"coerce\").values)\n",
        "            all_vals = np.concatenate(vals_concat) if vals_concat else np.array([0.0])\n",
        "            if use_normalized:\n",
        "                row_norms.append(colors.Normalize(vmin=0.0, vmax=1.0))\n",
        "            else:\n",
        "                vmin = float(np.nanmin(all_vals)) if np.isfinite(all_vals).any() else 0.0\n",
        "                vmax = float(np.nanmax(all_vals)) if np.isfinite(all_vals).any() else 1.0\n",
        "                if np.isclose(vmin, vmax, atol=1e-12):\n",
        "                    vmin, vmax = 0.0, 1.0\n",
        "                row_norms.append(colors.Normalize(vmin=vmin, vmax=vmax))\n",
        "\n",
        "        # 2) DIBUJO USANDO ESA norm POR FILA\n",
        "        for c, k in enumerate(k_vals):\n",
        "            csv_path = f\"{DRIVE_DIR_SPI}/NIFT_k{k}.csv\"\n",
        "            if not Path(csv_path).exists():\n",
        "                raise FileNotFoundError(f\"{csv_path} no encontrado.\")\n",
        "            df_k = pd.read_csv(csv_path, dtype={\"cell_id\": str})\n",
        "\n",
        "            for r, var in enumerate(var_params):\n",
        "                col = var + (\"n\" if use_normalized else \"\")\n",
        "                gdf = gdf_base.merge(df_k[[\"cell_id\", col]], on=\"cell_id\", how=\"left\")\n",
        "                gdf[col] = pd.to_numeric(gdf[col], errors=\"coerce\").astype(\"float32\")\n",
        "                arr = rasterize(((geom, val) for geom, val in zip(gdf.geometry, gdf[col].fillna(-9999.0))),\n",
        "                                out_shape=(height, width), transform=transform,\n",
        "                                fill=-9999.0, dtype=\"float32\")\n",
        "                data = np.ma.masked_equal(arr, -9999.0)\n",
        "\n",
        "                ax = ax_p[r, c]\n",
        "                im = ax.imshow(\n",
        "                    data, cmap=\"jet\", norm=row_norms[r],\n",
        "                    extent=[bounds[0], bounds[2], bounds[1], bounds[3]]\n",
        "                )\n",
        "                gpd.GeoSeries(single_border).plot(ax=ax, edgecolor=\"black\", linewidth=0.7, facecolor=\"none\")\n",
        "                if r == 0:\n",
        "                    ax.set_title(f\"TS-{k}\", fontsize=9, pad=2)\n",
        "                if c == 0:\n",
        "                    ax.text(-0.05, 0.5, var, transform=ax.transAxes, ha=\"right\", va=\"center\", fontsize=9)\n",
        "                ax.axis(\"off\")\n",
        "\n",
        "        # 3) BARRAS DE COLOR (una por fila, consistente con TODOS los k)\n",
        "        for r in range(n_rows):\n",
        "            sm = mpl.cm.ScalarMappable(norm=row_norms[r], cmap=\"jet\")\n",
        "            sm.set_array([])\n",
        "            fig_params.colorbar(sm, ax=ax_p[r, -1], fraction=0.04, pad=0.02)\n",
        "\n",
        "        fig_params.tight_layout()\n",
        "\n",
        "        # --- figura NIFT por k ---\n",
        "        n_cols_nift = 2 if len(k_vals) <= 4 else 3\n",
        "        n_rows_nift = int(np.ceil(len(k_vals)/n_cols_nift))\n",
        "        fig_nift, ax_n = plt.subplots(n_rows_nift, n_cols_nift,\n",
        "                                      figsize=(4*n_cols_nift+0.5, 3*n_rows_nift),\n",
        "                                      sharex=True, sharey=True, dpi=300)\n",
        "        ax_n = ax_n.ravel()\n",
        "        im_nift = None\n",
        "\n",
        "        tag = \"_\".join(map(str, k_vals)) + (\"_norm\" if use_normalized else \"\")\n",
        "        out_maps = os.path.join(DRIVE_DIR_SPI, f\"NIFT_all_maps_TS_{tag}.png\")\n",
        "        save_png_trim(fig_nift, out_maps, dpi=600)\n",
        "\n",
        "        for i, k in enumerate(k_vals):\n",
        "            csv_path = f\"{DRIVE_DIR_SPI}/NIFT_k{k}.csv\"\n",
        "            df_k     = pd.read_csv(csv_path, dtype={\"cell_id\": str})\n",
        "            gdf      = gdf_base.merge(df_k[[\"cell_id\",\"NIFT\"]], on=\"cell_id\", how=\"left\")\n",
        "            gdf[\"NIFT\"] = pd.to_numeric(gdf[\"NIFT\"], errors=\"coerce\").astype(\"float32\").fillna(-9999.0)\n",
        "            arr = rasterize(((geom, val) for geom, val in zip(gdf.geometry, gdf[\"NIFT\"])),\n",
        "                            out_shape=(height, width), transform=transform,\n",
        "                            fill=-9999.0, dtype=\"float32\")\n",
        "            data = np.ma.masked_equal(arr, -9999.0)\n",
        "            ax   = ax_n[i]\n",
        "            im_nift = ax.imshow(data, cmap=\"jet\", norm=colors.Normalize(vmin=0, vmax=100),\n",
        "                                extent=[bounds[0], bounds[2], bounds[1], bounds[3]])\n",
        "            gdf_regions.boundary.plot(ax=ax, edgecolor=\"black\", linewidth=0.7, facecolor=\"none\")\n",
        "            ax.set_title(f\"TS-{k}\", fontsize=9, pad=2); ax.axis(\"off\")\n",
        "\n",
        "        for ax in ax_n[len(k_vals):]:\n",
        "            ax.axis(\"off\")\n",
        "        fig_nift.tight_layout(rect=[0, 0.02, 0.86, 1])\n",
        "        cax = fig_nift.add_axes([0.88, 0.20, 0.025, 0.65])\n",
        "        fig_nift.colorbar(im_nift, cax=cax, label=\"NIFT (0–100)\")\n",
        "\n",
        "        tag = \"_\".join(map(str, k_vals)) + (\"_norm\" if use_normalized else \"\")\n",
        "        out_params = os.path.join(DRIVE_DIR_SPI, f\"NIFT_P1P10_full_TS_{tag}.png\")\n",
        "        out_maps   = os.path.join(DRIVE_DIR_SPI, f\"NIFT_maps_TS_{tag}.png\")\n",
        "        save_png_trim(fig_params, out_params, dpi=600)\n",
        "        save_png_trim(fig_nift,   out_maps,   dpi=600)\n",
        "\n",
        "        return fig_params, fig_nift, \"✔️ Mapas NIFT y P1–P10 generados (escalas por fila unificadas).\"\n",
        "    except Exception as e:\n",
        "        fig_err, ax_err = plt.subplots(); ax_err.axis(\"off\")\n",
        "        ax_err.text(0.5,0.5,str(e), ha=\"center\", va=\"center\", color=\"red\")\n",
        "        return fig_err, fig_err, f\"❌ {e}\"\n",
        "\n",
        "\n",
        "def _spi_plot_maps_grouped(k_list_str=\"1,3,6,12\", use_normalized=False):\n",
        "    \"\"\"\n",
        "    Devuelve 4 figuras compactas (G1..G4) con barras de color por fila y títulos TS-k.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        k_vals = [int(k) for k in k_list_str.split(\",\") if k.strip().isdigit()]\n",
        "        if not k_vals: raise ValueError(\"No se especificó ningún k válido.\")\n",
        "\n",
        "        _, ROI       = _get_regions()\n",
        "        grid_fc      = _make_grid(ROI)\n",
        "        gdf_base     = geemap.ee_to_gdf(grid_fc)\n",
        "\n",
        "        region_fc, _ = _get_regions()\n",
        "        gdf_regions  = geemap.ee_to_gdf(region_fc)\n",
        "        single_border = unary_union(gdf_regions.boundary.geometry)\n",
        "\n",
        "        bounds = gdf_base.total_bounds\n",
        "        res_x, res_y = _chirps_deg_res()\n",
        "        width  = int(np.round((bounds[2]-bounds[0])/res_x))\n",
        "        height = int(np.round((bounds[3]-bounds[1])/res_y))\n",
        "        transform = from_origin(bounds[0], bounds[3], res_x, res_y)\n",
        "\n",
        "        TITLE_FZ, ROW_LABEL_FZ, CB_TICK_FZ = 16, 16, 12\n",
        "        MAP_W_IN, MAP_H_IN = 2.6, 2.2\n",
        "        LABEL_W_IN, CB_W_IN = 0.85, 0.35\n",
        "        TITLE_PAD, TITLE_Y = 3.0, 0.985\n",
        "        WSPACE, HSPACE = 0.004, 0.004\n",
        "        LEFT_M, RIGHT_M, TOP_M, BOT_M = 0.015, 0.90, 0.94, 0.06\n",
        "        TOP_M_SINGLE = 0.88\n",
        "        CB_HEIGHT_FRAC, CB_INSET_W_FRAC = 0.82, 0.75\n",
        "        from matplotlib import gridspec\n",
        "\n",
        "        def _build_group(var_list):\n",
        "            n_rows, n_cols = len(var_list), len(k_vals)\n",
        "            fig_w = LABEL_W_IN + n_cols*MAP_W_IN + CB_W_IN\n",
        "            fig_h = n_rows*MAP_H_IN\n",
        "            fig = plt.figure(figsize=(fig_w, fig_h), dpi=300)\n",
        "            fig.subplots_adjust(left=LEFT_M, right=RIGHT_M,\n",
        "                                top=(TOP_M if n_rows>1 else TOP_M_SINGLE), bottom=BOT_M)\n",
        "            width_ratios = [LABEL_W_IN/MAP_W_IN] + [1.0]*n_cols + [CB_W_IN/MAP_W_IN]\n",
        "            gs = gridspec.GridSpec(nrows=n_rows, ncols=n_cols+2, figure=fig,\n",
        "                                   width_ratios=width_ratios, height_ratios=[1.0]*n_rows,\n",
        "                                   wspace=WSPACE, hspace=HSPACE)\n",
        "\n",
        "            # normalización por fila\n",
        "            row_norms = []\n",
        "            for var in var_list:\n",
        "                colname = var+\"n\" if use_normalized else var\n",
        "                vals_concat = []\n",
        "                for k in k_vals:\n",
        "                    df_k = pd.read_csv(f\"{DRIVE_DIR_SPI}/NIFT_k{k}.csv\", dtype={\"cell_id\": str})\n",
        "                    vals_concat.append(df_k[colname].astype(float).values)\n",
        "                all_vals = np.concatenate(vals_concat) if vals_concat else np.array([0.0])\n",
        "                if use_normalized:\n",
        "                    row_norms.append(colors.Normalize(vmin=0, vmax=1))\n",
        "                else:\n",
        "                    vmin = float(np.nanmin(all_vals)) if np.isfinite(all_vals).any() else 0.0\n",
        "                    vmax = float(np.nanmax(all_vals)) if np.isfinite(all_vals).any() else 1.0\n",
        "                    if np.isclose(vmin, vmax, atol=1e-12): vmin, vmax = 0.0, 1.0\n",
        "                    row_norms.append(colors.Normalize(vmin=vmin, vmax=vmax))\n",
        "\n",
        "            for r, var in enumerate(var_list):\n",
        "                colname = var+\"n\" if use_normalized else var\n",
        "                ax_label = fig.add_subplot(gs[r, 0]); ax_label.axis(\"off\")\n",
        "                ax_label.text(0.98, 0.5, var, transform=ax_label.transAxes,\n",
        "                              ha=\"right\", va=\"center\", fontsize=ROW_LABEL_FZ)\n",
        "\n",
        "                last_im = None\n",
        "                for c, k in enumerate(k_vals):\n",
        "                    df_k = pd.read_csv(f\"{DRIVE_DIR_SPI}/NIFT_k{k}.csv\", dtype={\"cell_id\": str})\n",
        "                    gdf  = gdf_base.merge(df_k[[\"cell_id\", colname]], on=\"cell_id\", how=\"left\")\n",
        "                    gdf[colname] = gdf[colname].astype(\"float32\").fillna(-9999.0)\n",
        "                    arr = rasterize(((geom, val) for geom, val in zip(gdf.geometry, gdf[colname])),\n",
        "                                    out_shape=(height, width), transform=transform,\n",
        "                                    fill=-9999.0, dtype=\"float32\")\n",
        "                    data = np.ma.masked_equal(arr, -9999.0)\n",
        "                    ax   = fig.add_subplot(gs[r, c+1])\n",
        "                    last_im = ax.imshow(data, cmap=\"jet\", norm=row_norms[r],\n",
        "                                        extent=[bounds[0], bounds[2], bounds[1], bounds[3]])\n",
        "                    gpd.GeoSeries(single_border).plot(ax=ax, edgecolor=\"black\", linewidth=0.7, facecolor=\"none\")\n",
        "                    if r == 0: ax.set_title(f\"TS-{k}\", fontsize=TITLE_FZ, pad=TITLE_PAD, y=TITLE_Y)\n",
        "                    ax.axis(\"off\")\n",
        "\n",
        "                cax_cell = fig.add_subplot(gs[r, n_cols+1]); cax_cell.axis(\"off\")\n",
        "                cax = cax_cell.inset_axes([(1-CB_INSET_W_FRAC)/2, (1-CB_HEIGHT_FRAC)/2,\n",
        "                                           CB_INSET_W_FRAC, CB_HEIGHT_FRAC])\n",
        "                sm = mpl.cm.ScalarMappable(norm=row_norms[r], cmap=\"jet\"); sm.set_array([])\n",
        "                cb = fig.colorbar(sm, cax=cax, orientation=\"vertical\")\n",
        "                cb.ax.tick_params(labelsize=CB_TICK_FZ, length=2, pad=2)\n",
        "            return fig\n",
        "\n",
        "        figs = [ _build_group(_SPI_GROUPS[key]) for key in [\"G1\",\"G2\",\"G3\",\"G4\"] ]\n",
        "\n",
        "        tag = \"_\".join([x.strip() for x in k_list_str.split(\",\") if x.strip().isdigit()]) + (\"_norm\" if use_normalized else \"\")\n",
        "        names = [\"G1_dynEventos\",\"G2_porcentajes\",\"G3_tendencias\",\"G4_precip\"]\n",
        "        for fig, name in zip(figs, names):\n",
        "            outp = os.path.join(DRIVE_DIR_SPI, f\"NIFT_{name}_TS_{tag}.png\")\n",
        "            save_png_trim(fig, outp, dpi=600)\n",
        "\n",
        "        return figs[0], figs[1], figs[2], figs[3], \"✔️ Figuras G1–G4 generadas.\"\n",
        "    except Exception as e:\n",
        "        fig, ax = plt.subplots(); ax.axis('off'); ax.text(0.5,0.5,str(e), ha=\"center\", va=\"center\", color=\"red\")\n",
        "        return fig, fig, fig, fig, f\"❌ {e}\"\n",
        "\n",
        "# =================== 9) VALIDACIÓN SPEI (CSIC vs Local) =====\n",
        "def _gee_mean_series(k, start='1958-01-01', end='2023-01-01'):\n",
        "    band = f\"SPEI_{int(k):02d}_month\"\n",
        "    ic = (ee.ImageCollection('CSIC/SPEI/2_10').select(band).filterDate(start, end))\n",
        "    ROI = _get_regions()[1]\n",
        "    def _to_feat(img):\n",
        "        mean = img.reduceRegion(ee.Reducer.mean(), ROI, 10000, maxPixels=1e13).get(band)\n",
        "        return ee.Feature(None, {'date': img.date().format('YYYY-MM'), 'mean': mean})\n",
        "    fc = ee.FeatureCollection(ic.map(_to_feat)).filter(ee.Filter.notNull(['mean']))\n",
        "    gdf = geemap.ee_to_gdf(fc)\n",
        "    df  = gdf.drop(columns='geometry', errors='ignore')\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    ser = (df.sort_values('date').set_index('date')['mean'].astype(float))\n",
        "    return ser\n",
        "\n",
        "def _csv_mean_series(k):\n",
        "    path = f'{DRIVE_DIR_SPEI}/SPEI_{k}_month.csv'\n",
        "    if not pathlib.Path(path).exists():\n",
        "        raise FileNotFoundError(f'CSV {path} no encontrado.')\n",
        "    df = pd.read_csv(path, parse_dates=['date'])\n",
        "    ser = df.groupby('date')['spei'].mean().sort_index().astype(float)\n",
        "    return ser\n",
        "\n",
        "def _plot_validation_series(k_list_str=\"1,3,6,12\", start_date=\"\", end_date=\"\"):\n",
        "    try:\n",
        "        k_vals = [int(x) for x in k_list_str.split(',') if x.strip().isdigit()]\n",
        "        sd = pd.to_datetime(start_date) if start_date.strip() else None\n",
        "        ed = pd.to_datetime(end_date)   if end_date.strip() else None\n",
        "        n = len(k_vals)\n",
        "        fig, axs = plt.subplots(n, 1, figsize=(10, 3*n), sharex=True)\n",
        "        if n == 1: axs = [axs]\n",
        "        for ax, k in zip(axs, k_vals):\n",
        "            ser_gee = _gee_mean_series(k)\n",
        "            ser_loc = _csv_mean_series(k)\n",
        "            df = pd.concat({'GEE': ser_gee, 'Local': ser_loc}, axis=1).dropna(how='all')\n",
        "            if sd is not None: df = df.loc[sd:]\n",
        "            if ed is not None: df = df.loc[:ed]\n",
        "            if df.empty:\n",
        "                ax.text(0.5, 0.5, f'SPEI-{k}: sin datos', ha='center', va='center')\n",
        "                ax.axis('off'); continue\n",
        "            df['GEE'].plot(ax=ax, lw=1, label='GEE')\n",
        "            df['Local'].plot(ax=ax, lw=1, ls='--', label='Local')\n",
        "            ax.set_ylabel(f'SPEI-{k}'); ax.grid(ls=':')\n",
        "            ax.legend(); ax.set_title(f'k={k}')\n",
        "        fig.tight_layout()\n",
        "        return fig, \"✔️ Curvas generadas.\"\n",
        "    except Exception as e:\n",
        "        fig, ax = plt.subplots(); ax.axis('off'); ax.text(0.5,0.5,f'Error: {e}',ha='center',va='center',color='red')\n",
        "        return fig, f\"❌ {e}\"\n",
        "\n",
        "# =================== 10) ENSO (ONI) ==========================\n",
        "def _read_enso_monthly(csv_path: str):\n",
        "    if not os.path.isfile(csv_path):\n",
        "        raise FileNotFoundError(f\"No se encontró ENSO.csv en: {csv_path}\")\n",
        "    raw = pd.read_csv(csv_path, dtype=str)\n",
        "    if 'Year' not in raw.columns:\n",
        "        raw = pd.read_csv(csv_path, dtype=str, sep=';')\n",
        "    raw = raw[raw['Year'].str.fullmatch(r'\\d{4}', na=False)].copy()\n",
        "    raw['Year'] = raw['Year'].astype(int)\n",
        "    seasons = ['DJF','JFM','FMA','MAM','AMJ','MJJ','JJA','JAS','ASO','SON','OND','NDJ']\n",
        "    missing = [c for c in seasons if c not in raw.columns]\n",
        "    if missing: raise ValueError(f\"ENSO.csv sin columnas: {missing}\")\n",
        "    for c in seasons: raw[c] = pd.to_numeric(raw[c].str.strip(), errors='coerce')\n",
        "    season_to_month = {'DJF':1,'JFM':2,'FMA':3,'MAM':4,'AMJ':5,'MJJ':6,'JJA':7,'JAS':8,'ASO':9,'SON':10,'OND':11,'NDJ':12}\n",
        "    records = []\n",
        "    for _, r in raw.iterrows():\n",
        "        y = int(r['Year'])\n",
        "        for s, m in season_to_month.items():\n",
        "            val = r[s]\n",
        "            if pd.notna(val):\n",
        "                records.append((pd.Timestamp(year=y, month=m, day=1), float(val)))\n",
        "    ser = pd.Series(dict(records)).sort_index(); ser.name = 'ENSO'; return ser\n",
        "\n",
        "\n",
        "AXIS_LABEL_FONTSIZE = 15\n",
        "TICK_LABEL_FONTSIZE = 13\n",
        "LEGEND_FONTSIZE     = 13\n",
        "METRICS_FONTSIZE    = 13\n",
        "TITLE_FONTSIZE      = 16\n",
        "CB_LABEL_FONTSIZE   = 13\n",
        "CB_TICK_FONTSIZE    = 12\n",
        "\n",
        "\n",
        "def _plot_spei_vs_enso(k_list_str=\"1,3,6,12\", start_date=\"\", end_date=\"\", enso_csv_path=None,\n",
        "                       enso_lag=-1, invert_axis=True, show_colorbar=False):\n",
        "    import matplotlib.dates as mdates\n",
        "    from matplotlib.collections import LineCollection\n",
        "    try:\n",
        "        if enso_csv_path is None:\n",
        "            enso_csv_path = f\"{DRIVE_DIR_SPEI}/ENSO.csv\"\n",
        "        enso = _read_enso_monthly(enso_csv_path)\n",
        "        if int(enso_lag) != 0:\n",
        "            enso = enso.shift(-int(enso_lag), freq=\"MS\")\n",
        "        if start_date.strip(): enso = enso[enso.index >= pd.to_datetime(start_date)]\n",
        "        if end_date.strip():   enso = enso[enso.index <= pd.to_datetime(end_date)]\n",
        "\n",
        "        k_vals = [int(x) for x in k_list_str.split(',') if str(x).strip().isdigit()]\n",
        "        if not k_vals: raise ValueError(\"k_list_str vacío o inválido.\")\n",
        "        n = len(k_vals)\n",
        "\n",
        "        fig, axs = plt.subplots(n, 1, figsize=(11, 3.4*n), sharex=True)\n",
        "        if n == 1: axs = [axs]\n",
        "\n",
        "        for ax, k in zip(axs, k_vals):\n",
        "            spei = _csv_mean_series(k).astype(float)\n",
        "            if start_date.strip(): spei = spei[spei.index >= pd.to_datetime(start_date)]\n",
        "            if end_date.strip():   spei = spei[spei.index <= pd.to_datetime(end_date)]\n",
        "\n",
        "            df = pd.concat({\"SPEI\": spei, \"ENSO\": enso.astype(float)}, axis=1).dropna()\n",
        "            if df.empty:\n",
        "                ax.axis('off'); ax.text(0.5,0.5,f'k={k}: sin solapamiento', ha='center', va='center', fontsize=METRICS_FONTSIZE); continue\n",
        "\n",
        "            r_pearson  = df[\"SPEI\"].corr(df[\"ENSO\"], method=\"pearson\")\n",
        "            r_spearman = df[\"SPEI\"].corr(df[\"ENSO\"], method=\"spearman\")\n",
        "\n",
        "            # Serie SPEI (línea gris)\n",
        "            ax.plot(df.index, df[\"SPEI\"], lw=0.9, color=\"0.25\", label=\"SPEI (Local)\")\n",
        "            ax.set_ylabel(f\"SPEI-{k}\", fontsize=AXIS_LABEL_FONTSIZE)\n",
        "            ax.tick_params(axis=\"both\", labelsize=TICK_LABEL_FONTSIZE)\n",
        "            ax.grid(ls=\":\", alpha=0.7)\n",
        "\n",
        "            # ENSO como línea coloreada (eje secundario)\n",
        "            x = mdates.date2num(df.index.to_pydatetime())\n",
        "            y = df[\"ENSO\"].values.astype(float)\n",
        "            points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
        "            segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "\n",
        "            max_abs = float(np.nanmax(np.abs(y))); max_abs = max(max_abs, 2.0)\n",
        "            norm = colors.TwoSlopeNorm(vmin=-max_abs, vcenter=0.0, vmax=max_abs)\n",
        "            cmap = plt.get_cmap(\"RdBu_r\")\n",
        "\n",
        "            lc = LineCollection(segments, cmap=cmap, norm=norm, linewidth=1.5, zorder=3)\n",
        "            lc.set_array((y[:-1] + y[1:]) / 2.0)\n",
        "\n",
        "            ax2 = ax.twinx()\n",
        "            ax2.add_collection(lc)\n",
        "            ax2.set_xlim(x.min(), x.max())\n",
        "            if invert_axis:\n",
        "                ax2.set_ylim(max_abs, -max_abs)\n",
        "            else:\n",
        "                ax2.set_ylim(-max_abs, max_abs)\n",
        "            ax2.set_ylabel(\"ENSO (ONI)\", fontsize=AXIS_LABEL_FONTSIZE)\n",
        "            ax2.tick_params(axis=\"both\", labelsize=TICK_LABEL_FONTSIZE)\n",
        "\n",
        "            if show_colorbar:\n",
        "                cbar = fig.colorbar(lc, ax=ax2, fraction=0.035, pad=0.02)\n",
        "                cbar.set_label(\"ENSO (ONI)\", fontsize=CB_LABEL_FONTSIZE)\n",
        "                cbar.ax.tick_params(labelsize=CB_TICK_FONTSIZE)\n",
        "                cbar.set_ticks([-2, -1, 0, 1, 2])\n",
        "\n",
        "            # Métricas\n",
        "            ax.text(0.02, 0.98,\n",
        "                    f\"r={r_pearson:.3f} · ρ={r_spearman:.3f} · n={len(df)} · lag_ENSO={int(enso_lag)}\",\n",
        "                    transform=ax.transAxes, ha='left', va='top',\n",
        "                    bbox=dict(facecolor='white', edgecolor='0.6', alpha=0.85, pad=3),\n",
        "                    fontsize=METRICS_FONTSIZE, zorder=10)\n",
        "\n",
        "            ax.set_title(f\"SPEI vs ENSO (k={k})\", fontsize=TITLE_FONTSIZE, pad=4)\n",
        "\n",
        "        axs[-1].set_xlabel(\"Fecha\", fontsize=AXIS_LABEL_FONTSIZE)\n",
        "        fig.tight_layout()\n",
        "        return fig, \"✔️ Comparación SPEI–ENSO generada.\"\n",
        "    except Exception as e:\n",
        "        fig, ax = plt.subplots(); ax.axis('off')\n",
        "        ax.text(0.5, 0.5, f'Error: {e}', ha='center', va='center', color='red', fontsize=METRICS_FONTSIZE)\n",
        "        return fig, f\"❌ {e}\"\n",
        "\n",
        "def _plot_spei_vs_spei_clc(k_list_str=\"1,3,6,12\", start_date=\"\", end_date=\"\"):\n",
        "    \"\"\"\n",
        "    Validación SPEI (Local) vs SPEI CSIC con el estilo clásico:\n",
        "      • CSIC línea continua\n",
        "      • Local línea punteada\n",
        "      • Texto con r de Pearson, rho de Spearman y n\n",
        "    \"\"\"\n",
        "    try:\n",
        "        k_vals = [int(x) for x in k_list_str.split(',') if str(x).strip().isdigit()]\n",
        "        if not k_vals:\n",
        "            raise ValueError(\"k_list_str vacío o inválido.\")\n",
        "        n = len(k_vals)\n",
        "\n",
        "        fig, axs = plt.subplots(n, 1, figsize=(11, 3.2*n), sharex=True)\n",
        "        if n == 1:\n",
        "            axs = [axs]\n",
        "\n",
        "        for ax, k in zip(axs, k_vals):\n",
        "            ser_gee = _gee_mean_series(k)   # CSIC/GEE\n",
        "            ser_loc = _csv_mean_series(k)   # Local\n",
        "\n",
        "            df = pd.concat({\"CSIC\": ser_gee.astype(float),\n",
        "                            \"Local\": ser_loc.astype(float)}, axis=1).dropna()\n",
        "\n",
        "            if start_date.strip():\n",
        "                df = df[df.index >= pd.to_datetime(start_date)]\n",
        "            if end_date.strip():\n",
        "                df = df[df.index <= pd.to_datetime(end_date)]\n",
        "\n",
        "            if df.empty:\n",
        "                ax.axis('off')\n",
        "                ax.text(0.5, 0.5, f'k={k}: sin solapamiento', ha='center', va='center', fontsize=METRICS_FONTSIZE)\n",
        "                continue\n",
        "\n",
        "            # Métricas\n",
        "            r_pearson  = df[\"Local\"].corr(df[\"CSIC\"], method=\"pearson\")\n",
        "            r_spearman = df[\"Local\"].corr(df[\"CSIC\"], method=\"spearman\")\n",
        "\n",
        "            ax.plot(df.index, df[\"CSIC\"],   lw=1.2, color=\"C0\", label=\"SPEI (CSIC)\")\n",
        "            ax.plot(df.index, df[\"Local\"], lw=1.2, color=\"C1\", ls=\"--\", label=\"SPEI (Local)\")\n",
        "\n",
        "            ax.set_ylabel(f\"SPEI-{k}\", fontsize=AXIS_LABEL_FONTSIZE)\n",
        "            ax.tick_params(axis=\"both\", labelsize=TICK_LABEL_FONTSIZE)\n",
        "            ax.grid(ls=\":\", alpha=0.7)\n",
        "\n",
        "            # Leyenda\n",
        "            ax.legend(loc=\"upper left\", fontsize=LEGEND_FONTSIZE)\n",
        "\n",
        "            ax.set_title(f\"Validación SPEI Local vs CSIC (k={k})\", fontsize=TITLE_FONTSIZE, pad=4)\n",
        "\n",
        "            # Métricas\n",
        "            ax.text(0.98, 0.98,\n",
        "                    f\"r={r_pearson:.3f} · ρ={r_spearman:.3f} · n={len(df)}\",\n",
        "                    transform=ax.transAxes, ha=\"right\", va=\"top\",\n",
        "                    bbox=dict(facecolor='white', edgecolor='0.6', alpha=0.85, pad=3),\n",
        "                    fontsize=METRICS_FONTSIZE, zorder=10)\n",
        "\n",
        "        axs[-1].set_xlabel(\"Fecha\", fontsize=AXIS_LABEL_FONTSIZE)\n",
        "        fig.tight_layout()\n",
        "        return fig, \"✔️ Validación (línea punteada) generada.\"\n",
        "    except Exception as e:\n",
        "        fig, ax = plt.subplots(); ax.axis('off')\n",
        "        ax.text(0.5, 0.5, f'Error: {e}', ha='center', va='center', color='red', fontsize=METRICS_FONTSIZE)\n",
        "        return fig, f\"❌ {e}\"\n",
        "\n",
        "# =================== 11) FIRMS (VIIRS 375 m) =================\n",
        "def _firms_monthly_fc(start_date, end_date, conf_thr=80):\n",
        "    sd, ed = ee.Date(start_date), ee.Date(end_date)\n",
        "    if sd.millis().getInfo() >= ed.millis().getInfo():\n",
        "        raise ValueError(\"start_date debe ser < end_date\")\n",
        "    grid_fc  = _make_grid(_get_regions()[1])\n",
        "    n_months = ed.difference(sd, 'month').int()\n",
        "    def _per_month(n):\n",
        "        d0 = sd.advance(n, 'month'); d1 = d0.advance(1, 'month')\n",
        "        presence = (ee.ImageCollection(FIRMS_COLLECTION)\n",
        "                      .filterDate(d0, d1).filterBounds(_get_regions()[1])\n",
        "                      .map(lambda im: im.updateMask(im.select('confidence').gte(conf_thr)).select('T21').gt(0))\n",
        "                      .max().unmask(0).rename('fires').toByte())\n",
        "        month_fc = (presence.reduceRegions(collection=grid_fc, reducer=ee.Reducer.max(), scale=375, crs='EPSG:4326')\n",
        "                      .map(lambda f: (f.set({'fires': ee.Number(f.get('max')).int(), 'date' : d0.format('YYYY-MM')})\n",
        "                                         .select(['cell_id','date','fires'])))\n",
        "                      .filter(ee.Filter.eq('fires', 1)))\n",
        "        return month_fc\n",
        "    return ee.FeatureCollection(ee.List.sequence(0, n_months.subtract(1)).map(_per_month)).flatten()\n",
        "\n",
        "def _export_firms_monthly(start_date, end_date, conf_thr=80):\n",
        "    try:\n",
        "        fc  = _firms_monthly_fc(start_date, end_date, conf_thr)\n",
        "        tag = f\"{start_date.replace('-','')}_{end_date.replace('-','')}\"\n",
        "        ee.batch.Export.table.toDrive(\n",
        "            collection     = fc.filter(ee.Filter.gt('fires', 0)),\n",
        "            description    = f'FIRMS_MONTH_{tag}',\n",
        "            folder         = EXPORT_FOLDER,\n",
        "            fileNamePrefix = f'FIRMS_MONTH_{tag}',\n",
        "            fileFormat     = 'CSV',\n",
        "            selectors      = ['cell_id', 'date', 'fires']\n",
        "        ).start()\n",
        "        return (f\"⏳ Exportando FIRMS ({tag}) — confianza ≥{conf_thr}.\")\n",
        "    except Exception as e:\n",
        "        return f\"❌ {e}\"\n",
        "\n",
        "def _csv_firms_monthly_df(csv_path: str) -> pd.DataFrame:\n",
        "    if not os.path.isfile(csv_path):\n",
        "        raise FileNotFoundError(f\"CSV no encontrado: {csv_path}\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    req = {'cell_id', 'date', 'fires'}\n",
        "    if not req.issubset(df.columns):\n",
        "        raise ValueError(f\"El CSV debe tener columnas {req}\")\n",
        "    df['date']  = pd.to_datetime(df['date'])\n",
        "    df['fires'] = df['fires'].fillna(0).astype(int)\n",
        "    return df[['cell_id', 'date', 'fires']]\n",
        "\n",
        "def rasterize_fires(csv_path, start_month, end_month, out_tif, nodata_val=-9999.0):\n",
        "    \"\"\"GeoTIFF binario 1=presencia (alineado CHIRPS) desde CSV mensual.\"\"\"\n",
        "    df = pd.read_csv(csv_path, parse_dates=['date'])\n",
        "    df['month'] = df['date'].dt.to_period('M')\n",
        "    sm, em = pd.Period(start_month, 'M'), pd.Period(end_month, 'M')\n",
        "    df = df[(df['month'] >= sm) & (df['month'] <= em)]\n",
        "    if df.empty: raise ValueError(\"El rango seleccionado no contiene incendios.\")\n",
        "    pres_df = (df.groupby('cell_id', as_index=False).agg(presence=('fires', lambda _: 1)))\n",
        "    _, ROI  = _get_regions(); grid_fc = _make_grid(ROI)\n",
        "    gdf     = geemap.ee_to_gdf(grid_fc).merge(pres_df, on='cell_id', how='left')\n",
        "    gdf['presence'] = gdf['presence'].fillna(0).astype('float32')\n",
        "    res_x, res_y = _chirps_deg_res()\n",
        "    xmin, ymin, xmax, ymax = gdf.total_bounds\n",
        "    width  = int(np.round((xmax - xmin) / res_x)); height = int(np.round((ymax - ymin) / res_y))\n",
        "    transform = from_origin(xmin, ymax, res_x, res_y)\n",
        "    arr = rasterize(((geom, val) for geom, val in zip(gdf.geometry, gdf.presence)),\n",
        "                    out_shape=(height, width), transform=transform, fill=nodata_val, dtype='float32')\n",
        "    with rasterio.open(out_tif, 'w', driver='GTiff', height=height, width=width, count=1, dtype='float32',\n",
        "                       crs='EPSG:4326', transform=transform, nodata=nodata_val) as dst:\n",
        "        dst.write(arr, 1)\n",
        "\n",
        "def _download_fires_gee_tif(start_month, end_month, out_tif, conf_thr=80):\n",
        "    sd = ee.Date(f\"{start_month}-01\"); ed = ee.Date(f\"{end_month}-01\").advance(1, 'month')\n",
        "    viirs_proj = (ee.ImageCollection(FIRMS_COLLECTION).first().select('T21').projection())\n",
        "    ch_proj    = _chirps_proj()\n",
        "    presence_375 = (ee.ImageCollection(FIRMS_COLLECTION)\n",
        "                      .filterDate(sd, ed).filterBounds(_get_regions()[1])\n",
        "                      .map(lambda im: im.updateMask(im.select('confidence').gte(conf_thr)).select('T21').gt(0))\n",
        "                      .max().unmask(0).setDefaultProjection(viirs_proj))\n",
        "    presence_ch = (presence_375.reduceResolution(ee.Reducer.max(), bestEffort=True, maxPixels=256)\n",
        "                              .reproject(ch_proj).toByte())\n",
        "    geemap.ee_export_image(presence_ch, filename=out_tif,\n",
        "                           scale=ch_proj.nominalScale().getInfo(),\n",
        "                           region=_get_regions()[1], file_per_band=False)\n",
        "\n",
        "def _plot_fires(csv_path, start_month, end_month):\n",
        "    \"\"\"Lado a lado: (1) raster desde CSV y (2) GeoTIFF descargado de EE (ambos a grilla CHIRPS).\"\"\"\n",
        "    from matplotlib import colors as mcolors\n",
        "    try:\n",
        "        tag = f\"{start_month.replace('-','')}_{end_month.replace('-','')}\"\n",
        "        local_tif = f\"/content/FIRES_LOCAL_{tag}.tif\"\n",
        "        gee_tif   = f\"/content/FIRES_GEE_{tag}.tif\"\n",
        "        rasterize_fires(csv_path, start_month, end_month, local_tif)\n",
        "        _download_fires_gee_tif(start_month, end_month, gee_tif)\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=True)\n",
        "        for ax, tif, title in zip((ax1, ax2), (local_tif, gee_tif), (\"CSV → raster\", \"GEE directo\")):\n",
        "            with rasterio.open(tif) as src:\n",
        "                data = src.read(1)\n",
        "                mask = data == src.nodata if src.nodata is not None else np.isnan(data)\n",
        "                data = np.ma.masked_where(mask, data)\n",
        "                cmap   = mcolors.ListedColormap(['white', 'red'])\n",
        "                bounds = [-0.5, 0.5, 1.5]; norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
        "                ax.imshow(data, cmap=cmap, norm=norm,\n",
        "                          extent=[src.bounds.left, src.bounds.right, src.bounds.bottom, src.bounds.top])\n",
        "                ax.set_title(title); ax.set_xlabel(\"Longitud\"); ax.set_ylabel(\"Latitud\"); ax.grid(ls=\":\")\n",
        "        fig.suptitle(f\"Incendios {start_month} → {end_month}\"); fig.tight_layout()\n",
        "        return fig, \"✔️ Comparación generada\"\n",
        "    except Exception as e:\n",
        "        fig, ax = plt.subplots(); ax.axis('off'); ax.text(0.5, 0.5, str(e), ha='center', va='center', color='red')\n",
        "        return fig, f\"❌ {e}\"\n",
        "\n",
        "# =================== 12) MÉTRICAS GLOBALES ===================\n",
        "def correlate_spei_firms_csv(k: int, lag: int, firms_csv_path: str, fire_min: int = 1):\n",
        "    try:\n",
        "        fig, ax = plt.subplots(); ax.axis('off')\n",
        "        ax.text(0.5,0.5,\"OBSOLETO: Usar análisis GWSS (basado en distancia)\", ha='center',va='center',color='red')\n",
        "        return fig, fig, fig, \"Función obsoleta. Usar GWSS.\"\n",
        "    except Exception as e:\n",
        "        fig, ax = plt.subplots(); ax.axis('off'); ax.text(0.5,0.5,str(e),ha='center',va='center',color='red')\n",
        "        return fig, fig, fig, f\"❌ {e}\"\n",
        "\n",
        "# =================== 13) GWSS — UNIFICADO (SIMPLIFICADO) ====================\n",
        "\n",
        "def _parse_years_str(years_str: str):\n",
        "    years = set()\n",
        "    if not years_str or not str(years_str).strip(): return []\n",
        "    parts = re.split(r'[,\\s]+', years_str.strip())\n",
        "    for p in parts:\n",
        "        if not p: continue\n",
        "        if '-' in p:\n",
        "            a, b = p.split('-', 1); a, b = int(a), int(b); years.update(range(min(a, b), max(a, b) + 1))\n",
        "        else:\n",
        "            years.add(int(p))\n",
        "    return sorted(years)\n",
        "\n",
        "def _cellid_to_lonlat(cell_id: str):\n",
        "    lon_str, lat_str = str(cell_id).split('_'); return int(lon_str)/1e4, int(lat_str)/1e4\n",
        "\n",
        "#\n",
        "# LA FUNCIÓN _apply_riskset_and_sampling HA SIDO ELIMINADA\n",
        "#\n",
        "\n",
        "def _months_by_enso_phase(enso_csv_path, phase=\"nino\", thr=0.5, enso_lag=0):\n",
        "    ser = _read_enso_monthly(enso_csv_path).astype(float)\n",
        "    if int(enso_lag) != 0: ser = ser.shift(-int(enso_lag), freq=\"MS\")\n",
        "    if phase == \"nino\":   mask = ser >= thr\n",
        "    elif phase == \"nina\": mask = ser <= -thr\n",
        "    elif phase == \"neutral\": mask = (ser.abs() < thr)\n",
        "    else: raise ValueError(\"phase: 'nino'|'nina'|'neutral'\")\n",
        "    months = pd.Series(mask[mask.index.notna() & mask.notna()])\n",
        "    return set(months[months].index.to_period(\"M\"))\n",
        "\n",
        "def _prepare_gw_input_unified(k: int, lag: int, firms_csv_path: str, years_str: str,\n",
        "                              phase: str, enso_csv_path: str, enso_thr: float, enso_lag: int):\n",
        "    \"\"\"\n",
        "    PREPARA ENTRADA GW (celda-mes) usando la lógica de DISTANCIA AL INCENDIO MÁS CERCANO.\n",
        "    - X (spei): Valor SPEI mensual de la celda.\n",
        "    - Y (fire): Métrica de \"cercanía\" (closeness) al incendio más cercano ESE MES.\n",
        "                closeness = 1.0 / (distancia_km + 1.0)\n",
        "                (1.0 si la celda tiene fuego, 0.0 si el mes no tuvo incendios)\n",
        "    \"\"\"\n",
        "    from sklearn.neighbors import BallTree\n",
        "    import numpy as np\n",
        "\n",
        "    # --- 1. SPEI mensual (aplicar lag y periodizar a 'M') ---\n",
        "    spei_path = f\"{DRIVE_DIR_SPEI}/SPEI_{k}_month.csv\"\n",
        "    if not os.path.isfile(spei_path):\n",
        "        raise FileNotFoundError(f\"No existe {spei_path}\")\n",
        "    spei = pd.read_csv(spei_path, parse_dates=['date'], dtype={'cell_id': str})\n",
        "    spei['date'] = (spei['date'] + pd.DateOffset(months=int(lag))).dt.to_period('M')\n",
        "\n",
        "    # --- 2. FIRMS mensual normalizado ---\n",
        "    fires = _csv_firms_monthly_df(firms_csv_path) # valida schema\n",
        "    fires['date'] = fires['date'].dt.to_period('M')\n",
        "\n",
        "    # --- 3. Filtros por años y ENSO (construir conjunto de meses a conservar) ---\n",
        "    months_keep: set[pd.Period] | None = None\n",
        "    years = _parse_years_str(years_str)\n",
        "\n",
        "    all_spei_months = set(spei['date'].unique())\n",
        "\n",
        "    if years:\n",
        "        months_years = {m for m in all_spei_months if m.year in years}\n",
        "        months_keep = months_years if months_years else set()\n",
        "\n",
        "    if phase in (\"nino\", \"nina\", \"neutral\"):\n",
        "        enso_csv_eff = enso_csv_path or f\"{DRIVE_DIR_SPEI}/ENSO.csv\"\n",
        "        months_phase = _months_by_enso_phase(enso_csv_eff, phase=phase, thr=float(enso_thr), enso_lag=int(enso_lag))\n",
        "        months_keep = months_phase if months_keep is None else (months_keep & months_phase)\n",
        "\n",
        "    if months_keep is not None:\n",
        "        if len(months_keep) == 0:\n",
        "            raise ValueError(\"Tras filtros de años/ENSO no quedan meses.\")\n",
        "        spei  = spei[spei['date'].isin(months_keep)]\n",
        "        fires = fires[fires['date'].isin(months_keep)]\n",
        "\n",
        "    if spei.empty:\n",
        "        raise ValueError(\"SPEI vacío tras filtros.\")\n",
        "\n",
        "    # --- 4. LÓGICA DE DISTANCIA (CERCANÍA) ---\n",
        "\n",
        "    # 4a. Celdas maestras (todas las que tienen datos SPEI)\n",
        "    all_cells = pd.DataFrame(spei['cell_id'].unique(), columns=['cell_id'])\n",
        "    coords_lonlat = pd.DataFrame(\n",
        "        all_cells['cell_id'].apply(_cellid_to_lonlat).tolist(),\n",
        "        index=all_cells.index,\n",
        "        columns=['lon', 'lat']\n",
        "    )\n",
        "    all_cells = all_cells.join(coords_lonlat)\n",
        "    coords_all_rad = np.deg2rad(all_cells[['lat', 'lon']].values.astype(float))\n",
        "    all_cells_idx_map = pd.Series(index=all_cells['cell_id'], data=np.arange(len(all_cells)))\n",
        "\n",
        "    # 4b. Celdas con incendios POR MES\n",
        "    fire_events_by_month = fires[fires['fires'] > 0][['date', 'cell_id']].drop_duplicates()\n",
        "    fire_events_by_month['idx'] = fire_events_by_month['cell_id'].map(all_cells_idx_map)\n",
        "    fire_idxs_by_month = fire_events_by_month.dropna(subset=['idx']).groupby('date')['idx'].apply(list)\n",
        "\n",
        "    # 4c. Calcular distancias SOLO para meses CON incendios\n",
        "    monthly_results = []\n",
        "    R_km = 6371.0088\n",
        "\n",
        "    valid_months_with_fires = set(spei['date'].unique()) & set(fire_idxs_by_month.index)\n",
        "\n",
        "    for month in valid_months_with_fires:\n",
        "        fire_indices = fire_idxs_by_month[month]\n",
        "        fire_indices_int = [int(i) for i in fire_indices if pd.notna(i)]\n",
        "        if not fire_indices_int:\n",
        "            continue\n",
        "\n",
        "        fire_coords_rad = coords_all_rad[fire_indices_int]\n",
        "\n",
        "        tree = BallTree(fire_coords_rad, metric='haversine')\n",
        "        distances_rad, _ = tree.query(coords_all_rad, k=1)\n",
        "        distances_km = distances_rad.flatten() * R_km\n",
        "\n",
        "        closeness = 1.0 / (distances_km + 1.0)\n",
        "\n",
        "        month_df = all_cells[['cell_id']].copy()\n",
        "        month_df['date'] = month\n",
        "        month_df['fire'] = closeness # Y = Closeness (1.0 = en fuego, ~0.0 = lejos)\n",
        "\n",
        "        monthly_results.append(month_df)\n",
        "\n",
        "    if monthly_results:\n",
        "        df_dist = pd.concat(monthly_results)\n",
        "    else:\n",
        "        df_dist = pd.DataFrame(columns=['cell_id', 'date', 'fire'])\n",
        "\n",
        "    # 5. Unir SPEI, Coordenadas y Distancias\n",
        "\n",
        "    # 5a. Unir SPEI con la lista maestra de celdas (para obtener lon/lat)\n",
        "    df = spei.merge(all_cells, on='cell_id', how='left')\n",
        "\n",
        "    # 5b. Unir (how='left') con los resultados de distancias.\n",
        "    #      Meses SIN incendios tendrán NaN en 'fire'.\n",
        "    df = df.merge(df_dist, on=['cell_id', 'date'], how='left')\n",
        "\n",
        "    # 5c. Rellenar los NaN (meses sin incendios):\n",
        "    #     - 'fire' (closeness) = 0.0 (distancia infinita)\n",
        "    df['fire'] = df['fire'].fillna(0.0)\n",
        "\n",
        "    # 5d. Limpiar datos inválidos de SPEI o coordenadas\n",
        "    df = df[np.isfinite(df['spei']) & np.isfinite(df['fire']) & np.isfinite(df['lon'])]\n",
        "\n",
        "    if df.empty:\n",
        "         raise ValueError(\"El dataframe final está vacío tras unir SPEI y distancias.\")\n",
        "\n",
        "    # --- 6. Balanceo y Riskset ELIMINADOS ---\n",
        "\n",
        "    if df['fire'].nunique() < 2:\n",
        "        raise ValueError(\"La variable 'fire' (closeness) no tiene varianza. (Probablemente 0 incendios en el período)\")\n",
        "\n",
        "    # --- 7. Guardar entrada (todas filas) + sitios (una por celda) ---\n",
        "    df[['cell_id', 'lon', 'lat', 'spei', 'fire']].to_csv(\"gw_input.csv\", index=False)\n",
        "    # Guardar TODOS los sitios (celdas)\n",
        "    all_cells[['cell_id', 'lon', 'lat']].to_csv(\"gw_sites.csv\", index=False)\n",
        "\n",
        "    return len(df)\n",
        "\n",
        "\n",
        "def _run_gwss_gwmodel(bw_neighbors: int = 120, kernel: str = \"bisquare\", use_spearman: bool = False):\n",
        "    # ===================== CAMBIO 1: Lógica para priorizar columnas =====================\n",
        "    if use_spearman:\n",
        "        search_order = (\n",
        "            \"'SCorr_spei.fire','SCorr_fire.spei','SCorr_spei_fire','SCorr_fire_spei',\"\n",
        "            \"'Corr_spei.fire','Corr_fire.spei','Corr_spei_fire','Corr_fire_spei'\"\n",
        "        )\n",
        "    else:\n",
        "        search_order = (\n",
        "            \"'Corr_spei.fire','Corr_fire.spei','Corr_spei_fire','Corr_fire_spei',\"\n",
        "            \"'SCorr_spei.fire','SCorr_fire.spei','SCorr_spei_fire','SCorr_fire_spei'\"\n",
        "        )\n",
        "    # =============================== FIN DEL CAMBIO 1 ===================================\n",
        "\n",
        "    r_code = f\"\"\"\n",
        "    if (!requireNamespace('GWmodel', quietly=TRUE))\n",
        "        install.packages('GWmodel', repos='https://cran.rstudio.com');\n",
        "    if (!requireNamespace('sp', quietly=TRUE))\n",
        "        install.packages('sp', repos='https://cran.rstudio.com');\n",
        "    suppressPackageStartupMessages({{ library(GWmodel); library(sp) }})\n",
        "\n",
        "    df <- read.csv('gw_input.csv', stringsAsFactors=FALSE)\n",
        "    sites <- if (file.exists('gw_sites.csv')) read.csv('gw_sites.csv', stringsAsFactors=FALSE) else df[,c('cell_id','lon','lat')]\n",
        "\n",
        "    req <- c('spei','fire','lon','lat')\n",
        "    if (!all(req %in% names(df))) stop('gw_input.csv sin columnas requeridas.')\n",
        "    df$spei <- as.numeric(df$spei); df$fire <- as.numeric(df$fire)\n",
        "    df$lon  <- as.numeric(df$lon);  df$lat  <- as.numeric(df$lat)\n",
        "    df <- df[is.finite(df$spei) & is.finite(df$fire) & is.finite(df$lon) & is.finite(df$lat), , drop=FALSE]\n",
        "    if (nrow(df) < 5) stop('Muy pocas filas en gw_input.csv (n < 5).')\n",
        "\n",
        "    sites$lon <- as.numeric(sites$lon); sites$lat <- as.numeric(sites$lat)\n",
        "    sites <- sites[is.finite(sites$lon) & is.finite(sites$lat), , drop=FALSE]\n",
        "    if (nrow(sites) < 1) stop('gw_sites.csv sin sitios válidos.')\n",
        "\n",
        "    coordinates(df)    <- ~ lon + lat\n",
        "    proj4string(df)    <- CRS('+proj=longlat +datum=WGS84 +no_defs')\n",
        "    coordinates(sites) <- ~ lon + lat\n",
        "    proj4string(sites) <- CRS('+proj=longlat +datum=WGS84 +no_defs')\n",
        "\n",
        "    bw_req <- {int(bw_neighbors)}\n",
        "    bw_adapt <- min(bw_req, nrow(df) - 1L)\n",
        "    if (bw_adapt < 2L) stop('BW adaptativo demasiado pequeño (bw < 2).')\n",
        "\n",
        "    qflag <- FALSE\n",
        "\n",
        "    key_df <- paste0(round(coordinates(df)[,1], 10L), '|', round(coordinates(df)[,2], 10L))\n",
        "    max_dups <- max(table(key_df))\n",
        "    use_adaptive <- TRUE\n",
        "    bw_fixed <- NA_real_\n",
        "\n",
        "    if (is.finite(max_dups) && max_dups >= bw_adapt) {{\n",
        "      use_adaptive <- FALSE\n",
        "      uid <- !duplicated(coordinates(df))\n",
        "      df_unique <- df[uid,]\n",
        "      if (nrow(df_unique) < 3) stop('Muy pocas ubicaciones únicas para BW fijo.')\n",
        "\n",
        "      k_unique <- max(5L, min(round(bw_adapt * 0.6), nrow(df_unique) - 1L))\n",
        "      dmat_sites_unique <- spDists(sites, df_unique, longlat=TRUE)\n",
        "      kth <- apply(dmat_sites_unique, 1, function(x) {{\n",
        "        xs <- sort(x, partial=k_unique)\n",
        "        xs[k_unique]\n",
        "      }})\n",
        "      bw_fixed <- suppressWarnings(median(kth[is.finite(kth) & kth > 0], na.rm=TRUE))\n",
        "      if (!is.finite(bw_fixed) || bw_fixed <= 0) {{\n",
        "        bw_fixed <- suppressWarnings(max(kth[is.finite(kth)], na.rm=TRUE))\n",
        "      }}\n",
        "      if (!is.finite(bw_fixed) || bw_fixed <= 0) stop('No se pudo estimar BW fijo > 0.')\n",
        "    }}\n",
        "\n",
        "    res <- if (use_adaptive) {{\n",
        "      gwss(\n",
        "        data          = df,\n",
        "        summary.locat = sites,\n",
        "        vars          = c('spei','fire'),\n",
        "        kernel        = '{kernel}',\n",
        "        adaptive      = TRUE,\n",
        "        bw            = bw_adapt,\n",
        "        longlat       = TRUE,\n",
        "        quantile      = qflag\n",
        "      )\n",
        "    }} else {{\n",
        "      gwss(\n",
        "        data          = df,\n",
        "        summary.locat = sites,\n",
        "        vars          = c('spei','fire'),\n",
        "        kernel        = '{kernel}',\n",
        "        adaptive      = FALSE,\n",
        "        bw            = bw_fixed,\n",
        "        longlat       = TRUE,\n",
        "        quantile      = qflag\n",
        "      )\n",
        "    }}\n",
        "\n",
        "    S  <- res$SDF@data\n",
        "    cn <- names(S)\n",
        "\n",
        "    pick_corr <- function(cn) {{\n",
        "      # ================== CAMBIO 2: Se inyecta el orden de búsqueda ==================\n",
        "      cand <- c({search_order})\n",
        "      # ============================ FIN DEL CAMBIO 2 ===============================\n",
        "      hit <- cand[cand %in% cn]\n",
        "      if (length(hit) > 0) return(hit[1])\n",
        "      g <- grep('S?Corr.*(spei).*(fire)', cn, ignore.case=TRUE, value=TRUE)\n",
        "      if (length(g) > 0) return(g[1])\n",
        "      stop('No se encontró columna de correlación local (Corr/SCorr).')\n",
        "    }}\n",
        "    corr_col <- pick_corr(cn)\n",
        "\n",
        "    get_sd <- function(varname) {{\n",
        "      sd_col <- cn[grepl('(LSD|\\\\\\\\bSD\\\\\\\\b|Std)', cn, ignore.case=TRUE) & grepl(varname, cn, ignore.case=TRUE)]\n",
        "      if (length(sd_col) > 0) return(S[[sd_col[1]]])\n",
        "      var_col <- cn[grepl('Var', cn, ignore.case=TRUE) & grepl(varname, cn, ignore.case=TRUE)]\n",
        "      if (length(var_col) > 0) return(sqrt(pmax(0, S[[var_col[1]]])))\n",
        "      rep(NA_real_, nrow(S))\n",
        "    }}\n",
        "    pick_mean <- function(varname) {{\n",
        "      m_col <- cn[grepl('Mean|Ave|Avg|LM', cn, ignore.case=TRUE) & grepl(varname, cn, ignore.case=TRUE)]\n",
        "      if (length(m_col) > 0) S[[m_col[1]]] else rep(NA_real_, nrow(S))\n",
        "    }}\n",
        "\n",
        "    dmat_sites_all <- spDists(sites, df, longlat=TRUE)\n",
        "    if (use_adaptive) {{\n",
        "      neff <- apply(dmat_sites_all, 1, function(d) {{\n",
        "        thr <- sort(d, partial=bw_adapt)[bw_adapt]\n",
        "        sum(is.finite(d) & d <= thr)\n",
        "      }})\n",
        "    }} else {{\n",
        "      neff <- apply(dmat_sites_all, 1, function(d) sum(is.finite(d) & d <= bw_fixed))\n",
        "    }}\n",
        "\n",
        "    out <- data.frame(\n",
        "      cell_id   = sites$cell_id,\n",
        "      corr      = S[[corr_col]],\n",
        "      mean_spei = pick_mean('spei'),\n",
        "      mean_fire = pick_mean('fire'),\n",
        "      sd_spei   = get_sd('spei'),\n",
        "      sd_fire   = get_sd('fire'),\n",
        "      bw_mode   = rep(if (use_adaptive) 'adaptive' else 'fixed', nrow(S)),\n",
        "      bw_used   = rep(if (use_adaptive) bw_adapt else bw_fixed, nrow(S)),\n",
        "      neff      = as.numeric(neff),\n",
        "      corr_name = rep(corr_col, nrow(S)),\n",
        "      corr_type = rep(if (grepl('^SCorr', corr_col)) 'Spearman' else 'Pearson', nrow(S))\n",
        "    )\n",
        "    write.csv(out, 'gw_corr_out.csv', row.names=FALSE)\n",
        "    \"\"\"\n",
        "    ro.r(r_code)\n",
        "\n",
        "    out = pd.read_csv(\"gw_corr_out.csv\")\n",
        "    if os.path.exists(\"gw_sites.csv\"):\n",
        "        sites = pd.read_csv(\"gw_sites.csv\")\n",
        "    else:\n",
        "        sites = pd.read_csv(\"gw_input.csv\")[['cell_id','lon','lat']].drop_duplicates()\n",
        "    out = out.merge(sites[['cell_id','lon','lat']], on='cell_id', how='left')\n",
        "    return out\n",
        "\n",
        "def _bh_fdr(pvals: np.ndarray) -> np.ndarray:\n",
        "    p = np.asarray(pvals, float); m = np.sum(~np.isnan(p))\n",
        "    order = np.argsort(np.where(np.isnan(p), np.inf, p))\n",
        "    ranks = np.empty_like(order); ranks[order] = np.arange(1, len(p)+1)\n",
        "    q = p * m / ranks; q_sorted = np.minimum.accumulate(q[order][::-1])[::-1]\n",
        "    q_final = np.empty_like(q); q_final[order] = q_sorted\n",
        "    return np.clip(q_final, 0, 1)\n",
        "\n",
        "def _compute_gw_post_metrics(df_corr: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df_corr.copy()\n",
        "    neff = pd.to_numeric(df.get('neff', np.nan), errors='coerce')\n",
        "    bw   = pd.to_numeric(df.get('bw_used', np.nan), errors='coerce')\n",
        "    df['neff'] = np.where(np.isfinite(neff), neff, bw).astype(float)\n",
        "    df['neff'] = df['neff'].fillna(30).clip(lower=5)\n",
        "    r = pd.to_numeric(df['corr'], errors='coerce').astype(float)\n",
        "    df_ = df['neff'] - 2.0\n",
        "    denom = np.clip(1.0 - r**2, 1e-12, None)\n",
        "    numer = np.clip(df_, 0.0, None)\n",
        "    tstat = r * np.sqrt(numer / denom)\n",
        "    from scipy.stats import t as tdist\n",
        "    p_arr = 2.0 * tdist.sf(np.abs(np.asarray(tstat, float)),\n",
        "                           np.asarray(df_.clip(lower=1), float))\n",
        "    df['pval'] = p_arr\n",
        "    df['qval'] = _bh_fdr(p_arr)\n",
        "    df['sig_fdr05'] = (df['qval'] <= 0.05)\n",
        "    if {'sd_spei','sd_fire'}.issubset(df.columns):\n",
        "        sd_x = pd.to_numeric(df['sd_spei'], errors='coerce')\n",
        "        sd_y = pd.to_numeric(df['sd_fire'], errors='coerce')\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            slope = r * (sd_y / sd_x)\n",
        "        slope[~np.isfinite(slope)] = np.nan\n",
        "        df['slope'] = np.clip(slope, -1.0, 1.0)\n",
        "    else:\n",
        "        df['slope'] = np.nan\n",
        "    return df\n",
        "\n",
        "def gw_correlation_yearly_panels(\n",
        "    k: int,\n",
        "    lag: int,\n",
        "    firms_csv_path: str,\n",
        "    start_year: int = 2014,\n",
        "    end_year: int   = 2024,\n",
        "    bw_neighbors: int = 480,\n",
        "    kernel: str = \"bisquare\",\n",
        "    use_spearman: bool = False,\n",
        "    nan_strategy: str = \"mask\",         # \"mask\" (omite NaN) o \"impute0\" (rellena 0)\n",
        "    enso_csv_path: str | None = None,\n",
        "    enso_phase: str = \"all\",           # \"all\",\"nino\",\"nina\",\"neutral\"\n",
        "    enso_thr: float = 0.5,\n",
        "    enso_lag: int = 0,\n",
        "    ncols: int = 4,                    # más filas que columnas\n",
        "    dot_size: float = 9.0,             # tamaño del punto en scatter\n",
        "    out_dir: str = None                # por defecto: DRIVE_DIR_SPEI\n",
        "):\n",
        "    \"\"\"\n",
        "    Ejecuta GWSS (lógica de distancia) año a año (start_year..end_year).\n",
        "    Ya no usa 'spei_agg', 'fire_metric', 'riskset' o 'balanceo'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import numpy as _np\n",
        "        import matplotlib.pyplot as _plt\n",
        "        from matplotlib.colors import TwoSlopeNorm\n",
        "        import matplotlib as _mpl\n",
        "\n",
        "        if out_dir is None:\n",
        "            out_dir = DRIVE_DIR_SPEI\n",
        "        Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        years = list(range(int(start_year), int(end_year) + 1))\n",
        "        results_per_year = []\n",
        "        kept_years, skipped_years = [], []\n",
        "\n",
        "        enso_csv_eff = enso_csv_path or f\"{DRIVE_DIR_SPEI}/ENSO.csv\"\n",
        "\n",
        "        for y in years:\n",
        "            if enso_phase != \"all\":\n",
        "                try:\n",
        "                    months_phase = _months_by_enso_phase(\n",
        "                        enso_csv_eff, phase=enso_phase, thr=float(enso_thr), enso_lag=int(enso_lag)\n",
        "                    )\n",
        "                    has_month_in_year = any(getattr(p, \"year\", None) == int(y) for p in months_phase)\n",
        "                    if not has_month_in_year:\n",
        "                        skipped_years.append(y)\n",
        "                        continue\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "            try:\n",
        "                _ = _prepare_gw_input_unified(\n",
        "                    k=int(k),\n",
        "                    lag=int(lag),\n",
        "                    firms_csv_path=firms_csv_path,\n",
        "                    years_str=str(y),\n",
        "                    phase=enso_phase,\n",
        "                    enso_csv_path=(enso_csv_eff if enso_phase != \"all\" else enso_csv_eff),\n",
        "                    enso_thr=float(enso_thr),\n",
        "                    enso_lag=int(enso_lag),\n",
        "                )\n",
        "            except ValueError as ve:\n",
        "                msg = str(ve).lower()\n",
        "                if (\"no quedan meses\" in msg) or (\"spei vacío\" in msg) or (\"spei vacio\" in msg) or (\"tras filtrar\" in msg) or (\"no se encontraron meses\" in msg) or (\"dataframe final está vacío\" in msg) or (\"no tiene varianza\" in msg):\n",
        "                    skipped_years.append(y)\n",
        "                    continue\n",
        "                else:\n",
        "                    raise\n",
        "\n",
        "            corr_df = _run_gwss_gwmodel(bw_neighbors=int(bw_neighbors), kernel=kernel, use_spearman=bool(use_spearman))\n",
        "            dfm = _compute_gw_post_metrics(corr_df).copy()\n",
        "            if dfm is None or len(dfm) == 0:\n",
        "                skipped_years.append(y)\n",
        "                continue\n",
        "            dfm[\"year\"] = y\n",
        "            results_per_year.append(dfm)\n",
        "            kept_years.append(y)\n",
        "\n",
        "        if not results_per_year:\n",
        "            raise ValueError(\"No se obtuvieron resultados: todos los años fueron omitidos por filtros ENSO/datos.\")\n",
        "\n",
        "        region_fc, _ = _get_regions()\n",
        "        gdf_regions  = geemap.ee_to_gdf(region_fc)\n",
        "        bounds = gdf_regions.total_bounds\n",
        "        xmin, ymin, xmax, ymax = bounds\n",
        "\n",
        "        n_years = len(kept_years)\n",
        "        ncols   = max(1, int(ncols))\n",
        "        nrows   = int(_np.ceil(n_years / ncols))\n",
        "        corr_norm  = TwoSlopeNorm(vmin=-1, vcenter=0.0, vmax=1)\n",
        "        slope_norm = TwoSlopeNorm(vmin=-0.5, vcenter=0.0, vmax=0.5)\n",
        "\n",
        "        # ---------- Panel 1: Correlación local ----------\n",
        "        fig_corr, axs = _plt.subplots(nrows, ncols, figsize=(3.2*ncols, 2.6*nrows), sharex=True, sharey=True, dpi=300)\n",
        "        axs = _np.atleast_2d(axs)\n",
        "        last_im = None\n",
        "        for i, y in enumerate(kept_years):\n",
        "            r = i // ncols; c = i % ncols\n",
        "            ax = axs[r, c]\n",
        "            dfy = next(d for d in results_per_year if int(d[\"year\"].iloc[0]) == y)\n",
        "            corr_vals = _np.asarray(dfy[\"corr\"], float)\n",
        "            if nan_strategy == \"impute0\":\n",
        "                corr_vals = _np.where(_np.isfinite(corr_vals), corr_vals, 0.0)\n",
        "                mask = _np.ones_like(corr_vals, bool)\n",
        "            else:\n",
        "                mask = _np.isfinite(corr_vals)\n",
        "            sc = ax.scatter(dfy.loc[mask, \"lon\"].astype(float),\n",
        "                            dfy.loc[mask, \"lat\"].astype(float),\n",
        "                            c=corr_vals[mask], s=dot_size, cmap=\"RdGy\", norm=corr_norm, linewidths=0, zorder=2)\n",
        "            last_im = sc\n",
        "            gpd.GeoSeries(unary_union(gdf_regions.boundary.geometry)).plot(ax=ax, edgecolor=\"black\", linewidth=0.7, facecolor=\"none\", zorder=5)\n",
        "            ax.set_title(f\"Año {y}\", fontsize=10, pad=2); ax.set_xlim(xmin, xmax); ax.set_ylim(ymin, ymax); ax.axis(\"off\")\n",
        "        for j in range(n_years, nrows*ncols):\n",
        "            axs[j//ncols, j % ncols].axis('off')\n",
        "        _plt.tight_layout(rect=[0, 0, 0.88, 1])\n",
        "        cax = fig_corr.add_axes([0.90, 0.15, 0.025, 0.7])\n",
        "        fig_corr.colorbar(last_im, cax=cax, label=\"Correlación local (GW)\")\n",
        "\n",
        "        # ---------- Panel 2: Significancia local ----------\n",
        "        fig_sig, axs2 = _plt.subplots(nrows, ncols, figsize=(3.2*ncols, 2.6*nrows), sharex=True, sharey=True, dpi=300)\n",
        "        axs2 = _np.atleast_2d(axs2)\n",
        "        all_logq_vals = []\n",
        "        for dfy in results_per_year:\n",
        "            q = np.asarray(dfy[\"qval\"], float)\n",
        "            logq = -np.log10(np.clip(q, 1e-12, 1.0))\n",
        "            all_logq_vals.append(logq[np.isfinite(logq)])\n",
        "        if all_logq_vals:\n",
        "            concatenated_vals = np.concatenate(all_logq_vals)\n",
        "            vmin = np.min(concatenated_vals) if concatenated_vals.size > 0 else 0\n",
        "            vmax = np.max(concatenated_vals) if concatenated_vals.size > 0 else 1\n",
        "            norm_sig = plt.Normalize(vmin=vmin, vmax=vmax)\n",
        "        else:\n",
        "            norm_sig = plt.Normalize(vmin=0, vmax=1)\n",
        "        last_im2 = None\n",
        "        for i, y in enumerate(kept_years):\n",
        "            r = i // ncols; c = i % ncols\n",
        "            ax = axs2[r, c]\n",
        "            dfy = next(d for d in results_per_year if int(d[\"year\"].iloc[0]) == y)\n",
        "            q = _np.asarray(dfy[\"qval\"], float)\n",
        "            logq = -_np.log10(_np.clip(q, 1e-12, 1.0))\n",
        "            if nan_strategy == \"impute0\":\n",
        "                logq = _np.where(_np.isfinite(logq), logq, 0.0)\n",
        "                mask = _np.ones_like(logq, bool)\n",
        "            else:\n",
        "                mask = _np.isfinite(logq)\n",
        "            sc = ax.scatter(dfy.loc[mask, \"lon\"].astype(float),\n",
        "                            dfy.loc[mask, \"lat\"].astype(float),\n",
        "                            c=logq[mask], s=dot_size, cmap=\"viridis\", linewidths=0, zorder=2, norm=norm_sig)\n",
        "            last_im2 = sc\n",
        "            sig_mask = (dfy[\"sig_fdr05\"].fillna(False).values) & mask\n",
        "            ax.scatter(dfy.loc[sig_mask, \"lon\"].astype(float),\n",
        "                       dfy.loc[sig_mask, \"lat\"].astype(float), facecolors=\"none\",\n",
        "                       edgecolors=\"k\", s=dot_size*3.0, linewidths=0.6, zorder=3)\n",
        "            gpd.GeoSeries(unary_union(gdf_regions.boundary.geometry)).plot(ax=ax, edgecolor=\"black\", linewidth=0.7, facecolor=\"none\", zorder=5)\n",
        "            ax.set_title(f\"Año {y}\", fontsize=10, pad=2); ax.set_xlim(xmin, xmax); ax.set_ylim(ymin, ymax); ax.axis(\"off\")\n",
        "        for j in range(n_years, nrows*ncols):\n",
        "            axs2[j//ncols, j % ncols].axis('off')\n",
        "        _plt.tight_layout(rect=[0, 0, 0.88, 1])\n",
        "        cax2 = fig_sig.add_axes([0.90, 0.15, 0.025, 0.7])\n",
        "        fig_sig.colorbar(last_im2, cax=cax2, label=\"-log10(q)\")\n",
        "\n",
        "        # ---------- Panel 3: Pendiente local ----------\n",
        "        fig_slope, axs3 = _plt.subplots(nrows, ncols, figsize=(3.2*ncols, 2.6*nrows), sharex=True, sharey=True, dpi=300)\n",
        "        axs3 = _np.atleast_2d(axs3)\n",
        "        last_im3 = None\n",
        "        for i, y in enumerate(kept_years):\n",
        "            r = i // ncols; c = i % ncols\n",
        "            ax = axs3[r, c]\n",
        "            dfy = next(d for d in results_per_year if int(d[\"year\"].iloc[0]) == y)\n",
        "            slope = _np.asarray(dfy[\"slope\"], float)\n",
        "            has_any = _np.isfinite(slope).any()\n",
        "            if has_any:\n",
        "                if nan_strategy == \"impute0\":\n",
        "                    slope = _np.where(_np.isfinite(slope), slope, 0.0)\n",
        "                    mask = _np.ones_like(slope, bool)\n",
        "                else:\n",
        "                    mask = _np.isfinite(slope)\n",
        "                sc = ax.scatter(dfy.loc[mask, \"lon\"].astype(float),\n",
        "                                dfy.loc[mask, \"lat\"].astype(float),\n",
        "                                c=slope[mask], s=dot_size, cmap=\"PiYG\", norm=slope_norm, linewidths=0, zorder=2)\n",
        "                last_im3 = sc\n",
        "            else:\n",
        "                ax.text(0.5, 0.5, \"Sin SD locales\", ha=\"center\", va=\"center\", fontsize=9)\n",
        "            gpd.GeoSeries(unary_union(gdf_regions.boundary.geometry)).plot(ax=ax, edgecolor=\"black\", linewidth=0.7, facecolor=\"none\", zorder=5)\n",
        "            ax.set_title(f\"Año {y}\", fontsize=10, pad=2); ax.set_xlim(xmin, xmax); ax.set_ylim(ymin, ymax); ax.axis(\"off\")\n",
        "        for j in range(n_years, nrows*ncols):\n",
        "            axs3[j//ncols, j % ncols].axis('off')\n",
        "        _plt.tight_layout(rect=[0, 0, 0.88, 1])\n",
        "        if last_im3 is not None:\n",
        "            cax3 = fig_slope.add_axes([0.90, 0.15, 0.025, 0.7])\n",
        "            fig_slope.colorbar(last_im3, cax=cax3, label=\"Pendiente local y|x\")\n",
        "\n",
        "        # ---------- Panel 4: Resumen (ECDF + Histo) ----------\n",
        "        fig_sum, (ax_ecdf, ax_hist) = _plt.subplots(1, 2, figsize=(10, 4), dpi=300, sharex=False)\n",
        "        if len(kept_years) <= 10: cmap = _mpl.cm.get_cmap(\"tab10\", len(kept_years))\n",
        "        else: cmap = _mpl.cm.get_cmap(\"tab20\", len(kept_years))\n",
        "        bins = _np.linspace(-1.0, 1.0, 41)\n",
        "        for i, y in enumerate(kept_years):\n",
        "            dfy  = next(d for d in results_per_year if int(d[\"year\"].iloc[0]) == y)\n",
        "            vals = _np.asarray(dfy[\"corr\"], float)\n",
        "            vals = vals[_np.isfinite(vals)]\n",
        "            if vals.size == 0: continue\n",
        "            color = cmap(i)\n",
        "            v = _np.sort(vals)\n",
        "            ecdf = _np.arange(1, len(v)+1)/len(v)\n",
        "            ax_ecdf.plot(v, ecdf, lw=1.8, alpha=0.95, color=color, label=str(y))\n",
        "            h, edges = _np.histogram(vals, bins=bins, density=True)\n",
        "            centers  = 0.5*(edges[:-1]+edges[1:])\n",
        "            ax_hist.plot(centers, h, lw=1.4, alpha=0.9, color=color)\n",
        "        ax_ecdf.axvline(0.0, color=\"k\", lw=0.8, ls=\"--\")\n",
        "        ax_ecdf.set_title(\"ECDF correlación (años superpuestos)\")\n",
        "        ax_ecdf.set_xlabel(\"Correlación local\"); ax_ecdf.set_ylabel(\"Proporción\")\n",
        "        ax_ecdf.grid(ls=\":\", alpha=0.5)\n",
        "        ax_hist.set_title(\"Histograma densidad (superpuestos)\")\n",
        "        ax_hist.set_xlabel(\"Correlación local\"); ax_hist.set_ylabel(\"Densidad\")\n",
        "        ax_hist.grid(ls=\":\", alpha=0.5)\n",
        "        handles, labels = ax_ecdf.get_legend_handles_labels()\n",
        "        import math\n",
        "        ncols_leg = max(1, math.ceil(len(handles) / 2))\n",
        "        fig_sum.subplots_adjust(bottom=0.25)\n",
        "        leg = fig_sum.legend(\n",
        "            handles=handles, labels=labels, title=\"Año\",\n",
        "            loc=\"upper center\", ncol=ncols_leg, frameon=False,\n",
        "            bbox_to_anchor=(0.5, 0.18), fontsize=9, title_fontsize=10,\n",
        "            columnspacing=1.2, handlelength=2.6,\n",
        "        )\n",
        "\n",
        "        # 5) Guardado + mensaje con años omitidos\n",
        "        tag = f\"TS{k}_lag{lag}_{start_year}_{end_year}\"\n",
        "        f_corr = os.path.join(out_dir, f\"GWSS_YearPanel_corr_{tag}.png\")\n",
        "        f_sig  = os.path.join(out_dir, f\"GWSS_YearPanel_sig_{tag}.png\")\n",
        "        f_slo  = os.path.join(out_dir, f\"GWSS_YearPanel_slope_{tag}.png\")\n",
        "        f_sum  = os.path.join(out_dir, f\"GWSS_YearPanel_summary_{tag}.png\")\n",
        "        save_png_trim(fig_corr, f_corr, dpi=600)\n",
        "        save_png_trim(fig_sig,  f_sig,  dpi=600)\n",
        "        save_png_trim(fig_slope,f_slo,  dpi=600)\n",
        "        save_png_trim(fig_sum,  f_sum,  dpi=600)\n",
        "        omit_txt = f\"\\nOmitidos por ENSO/datos: {', '.join(map(str, skipped_years))}\" if skipped_years else \"\"\n",
        "        msg = (f\"✔️ Paneles anuales guardados:\\n\"\n",
        "               f\"‣ {os.path.basename(f_corr)}\\n\"\n",
        "               f\"‣ {os.path.basename(f_sig)}\\n\"\n",
        "               f\"‣ {os.path.basename(f_slo)}\\n\"\n",
        "               f\"‣ {os.path.basename(f_sum)}\\n\"\n",
        "               f\"Años usados: {', '.join(map(str, kept_years))}{omit_txt}\\n\"\n",
        "               f\"Carpeta: {out_dir}\")\n",
        "        return fig_corr, fig_sig, fig_slope, fig_sum, msg\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback as _tb\n",
        "        fig_err, ax_err = plt.subplots(figsize=(6, 4), dpi=150)\n",
        "        ax_err.axis(\"off\"); ax_err.text(0.5, 0.5, f\"{e}\", ha=\"center\", va=\"center\", color=\"red\")\n",
        "        return fig_err, fig_err, fig_err, fig_err, f\"❌ {e}\\n{_tb.format_exc()}\"\n",
        "\n",
        "from matplotlib.colors import TwoSlopeNorm\n",
        "def _plot_gw_corr(df_corr, title=\"Correlación GW (SPEI vs Incendios)\", nan_strategy: str = \"mask\", draw_borders: bool = True):\n",
        "    dfp = df_corr.copy()\n",
        "    if nan_strategy == \"impute0\": dfp['corr'] = dfp['corr'].astype(float).fillna(0.0)\n",
        "    else: dfp = dfp.dropna(subset=['corr'])\n",
        "    if dfp.empty:\n",
        "        fig, ax = plt.subplots(); ax.axis('off'); ax.text(0.5,0.5,\"Sin celdas válidas\",ha='center',va='center'); return fig\n",
        "    fig, ax = plt.subplots(figsize=(8, 7)); norm = TwoSlopeNorm(vmin=-0.5, vcenter=0.0, vmax=0.5)\n",
        "    sc = ax.scatter(dfp['lon'].astype(float), dfp['lat'].astype(float), c=dfp['corr'].astype(float),\n",
        "                    s=9, cmap='RdGy', norm=norm, linewidths=0, zorder=2)\n",
        "    if draw_borders:\n",
        "        region_fc, _roi = _get_regions(); gdf_regions = geemap.ee_to_gdf(region_fc)\n",
        "        gpd.GeoSeries(unary_union(gdf_regions.boundary.geometry)).plot(ax=ax, edgecolor=\"black\", linewidth=0.7, facecolor=\"none\", zorder=5)\n",
        "        xmin, ymin, xmax, ymax = gdf_regions.total_bounds; ax.set_xlim(xmin, xmax); ax.set_ylim(ymin, ymax)\n",
        "    cb = plt.colorbar(sc, ax=ax); cb.set_label(\"ρ local (GW)\"); ax.grid(ls=\":\")\n",
        "    ax.set_xlabel(\"Longitud\"); ax.set_ylabel(\"Latitud\"); ax.set_title(title); fig.tight_layout(); return fig\n",
        "\n",
        "def _plot_gw_significance_map(df, title=\"Significancia local (FDR)\"):\n",
        "    dfp = df.copy()\n",
        "    if dfp.empty:\n",
        "        fig, ax = plt.subplots(); ax.axis('off'); ax.text(0.5,0.5,'Sin datos',ha='center'); return fig\n",
        "    fig, ax = plt.subplots(figsize=(8, 7))\n",
        "    logq = -np.log10(np.clip(dfp['qval'].astype(float), 1e-12, 1))\n",
        "    sc = ax.scatter(dfp['lon'], dfp['lat'], c=logq, s=10, cmap='viridis', zorder=2)\n",
        "    sig = dfp['sig_fdr05'].fillna(False).values\n",
        "    ax.scatter(dfp.loc[sig,'lon'], dfp.loc[sig,'lat'], facecolors='none', edgecolors='k', s=30, linewidths=0.6, zorder=3)\n",
        "    region_fc, _ = _get_regions(); gdf_regions = geemap.ee_to_gdf(region_fc)\n",
        "    gpd.GeoSeries(unary_union(gdf_regions.boundary.geometry)).plot(ax=ax, edgecolor='black', linewidth=0.7, facecolor='none', zorder=5)\n",
        "    xmin, ymin, xmax, ymax = gdf_regions.total_bounds; ax.set_xlim(xmin, xmax); ax.set_ylim(ymin, ymax)\n",
        "    cb = plt.colorbar(sc, ax=ax); cb.set_label(\"−log10(q)\"); ax.set_xlabel(\"Longitud\"); ax.set_ylabel(\"Latitud\")\n",
        "    ax.set_title(title); ax.grid(ls=':'); fig.tight_layout(); return fig\n",
        "\n",
        "def _plot_gw_slope_map(df, title=\"Sensibilidad local (pendiente y~x)\"):\n",
        "    dfp = df.copy()\n",
        "    if dfp['slope'].notna().sum() == 0:\n",
        "        fig, ax = plt.subplots(); ax.axis('off'); ax.text(0.5,0.5,'Sin SD locales → no hay pendiente',ha='center'); return fig\n",
        "    fig, ax = plt.subplots(figsize=(8, 7)); norm = TwoSlopeNorm(vmin=-0.5, vcenter=0.0, vmax=0.5)\n",
        "    sc = ax.scatter(dfp['lon'], dfp['lat'], c=dfp['slope'], s=10, cmap='PiYG', norm=norm, zorder=2)\n",
        "    region_fc, _ = _get_regions(); gdf_regions = geemap.ee_to_gdf(region_fc)\n",
        "    gpd.GeoSeries(unary_union(gdf_regions.boundary.geometry)).plot(ax=ax, edgecolor='black', linewidth=0.7, facecolor='none', zorder=5)\n",
        "    xmin, ymin, xmax, ymax = gdf_regions.total_bounds; ax.set_xlim(xmin, xmax); ax.set_ylim(ymin, ymax)\n",
        "    cb = plt.colorbar(sc, ax=ax); cb.set_label(\"Pendiente local (≈ Δprob/ΔSPEI)\")\n",
        "    ax.set_xlabel(\"Longitud\"); ax.set_ylabel(\"Latitud\"); ax.set_title(title); ax.grid(ls=':'); fig.tight_layout(); return fig\n",
        "\n",
        "def _plot_corr_hist_ecdf(df, title=\"Distribución de correlaciones locales\"):\n",
        "    vals = df['corr'].astype(float).values; sig  = df['sig_fdr05'].fillna(False).values\n",
        "    vals_sig = vals[sig]; fig = plt.figure(figsize=(10,5))\n",
        "    ax1 = fig.add_subplot(1,2,1); bins = np.linspace(-1, 1, 41)\n",
        "    ax1.hist(vals, bins=bins, color='0.7', edgecolor='0.3', label='Todas')\n",
        "    if len(vals_sig) > 0: ax1.hist(vals_sig, bins=bins, alpha=0.8, color='tab:blue', edgecolor='black', label='FDR ≤ 0.05')\n",
        "    ax1.axvline(0, color='k', lw=1); ax1.set_xlabel(\"ρ local\"); ax1.set_ylabel(\"Celdas\"); ax1.grid(ls=':')\n",
        "    med = np.nanmedian(vals); ax1.axvline(med, color='tab:red', lw=1.5, ls='--', label=f\"Mediana={med:.3f}\")\n",
        "    ax1.legend(); ax1.set_title(title)\n",
        "    ax2 = fig.add_subplot(1,2,2); v_sorted = np.sort(vals[np.isfinite(vals)]); y = np.arange(1, len(v_sorted)+1) / len(v_sorted)\n",
        "    ax2.plot(v_sorted, y, lw=2); ax2.set_xlabel(\"ρ local\"); ax2.set_ylabel(\"ECDF\"); ax2.grid(ls=':'); fig.tight_layout(); return fig\n",
        "\n",
        "def gw_correlation_unified_plus(k: int, lag: int, firms_csv_path: str, years_str: str,\n",
        "                                bw_neighbors: int, kernel: str,\n",
        "                                use_spearman: bool, nan_strategy: str,\n",
        "                                enso_csv_path: str, enso_phase: str, enso_thr: float, enso_lag: int):\n",
        "    try:\n",
        "        # Prepara los datos de entrada (versión simplificada)\n",
        "        _prepare_gw_input_unified(int(k), int(lag), firms_csv_path, years_str,\n",
        "                                  enso_phase, enso_csv_path, float(enso_thr), int(enso_lag))\n",
        "\n",
        "        # Ejecuta el análisis GWSS en R\n",
        "        corr_df = _run_gwss_gwmodel(bw_neighbors=int(bw_neighbors), kernel=kernel, use_spearman=bool(use_spearman))\n",
        "\n",
        "        # Calcula métricas de significancia\n",
        "        dfm = _compute_gw_post_metrics(corr_df)\n",
        "\n",
        "        # Genera las figuras y el resumen\n",
        "        phase_txt = f\" · ENSO={enso_phase.upper()}\" if enso_phase!='all' else \"\"\n",
        "        fig_corr = _plot_gw_corr(dfm, title=f\"Correlación GW — SPEI-{k} (lag {lag}), años={years_str if years_str.strip() else 'todos'}{phase_txt}\",\n",
        "                                 nan_strategy=nan_strategy)\n",
        "        fig_sig  = _plot_gw_significance_map(dfm, title=\"Significancia local (FDR 5%)\")\n",
        "        fig_sens = _plot_gw_slope_map(dfm, title=\"Sensibilidad local (pendiente ≈ Δprob/ΔSPEI)\")\n",
        "        fig_hist = _plot_corr_hist_ecdf(dfm)\n",
        "\n",
        "        # Prepara el texto de resumen\n",
        "        n_all = len(dfm); n_sig = int(dfm['sig_fdr05'].sum()); frac_sig = n_sig / n_all if n_all else 0\n",
        "        med_r = float(np.nanmedian(dfm['corr'])); iqr_r = np.nanpercentile(dfm['corr'], [25, 75]) if np.isfinite(dfm['corr']).any() else [np.nan, np.nan]\n",
        "        neg_sig = int(((dfm['sig_fdr05']) & (dfm['corr'] < 0)).sum()); pos_sig = int(((dfm['sig_fdr05']) & (dfm['corr'] > 0)).sum())\n",
        "\n",
        "        if dfm['slope'].notna().any():\n",
        "            med_slope = float(np.nanmedian(dfm['slope'])); p2, p98 = np.nanpercentile(dfm['slope'], [2, 98])\n",
        "            slope_txt = f\"pendiente_mediana={med_slope:.3f} · p2–p98=[{p2:.3f}, {p98:.3f}]\"\n",
        "        else:\n",
        "            slope_txt = \"pendiente: N/D\"\n",
        "\n",
        "        text_sum = (f\"celdas={n_all} · bw(adapt)≈{int(np.nanmedian(dfm.get('bw_used', pd.Series([bw_neighbors]))))} · \"\n",
        "                    f\"ρ_mediana={med_r:.3f} · IQR=[{iqr_r[0]:.3f},{iqr_r[1]:.3f}] · \"\n",
        "                    f\"FDR≤0.05: {n_sig} ({frac_sig:.1%}) → neg={neg_sig}, pos={pos_sig} · {slope_txt}\")\n",
        "\n",
        "        status_msg = (f\"✔️ GWSS OK (Distancia) · periodo={years_str if years_str.strip() else 'todos'} · \"\n",
        "                      f\"k={k} · lag={lag} · kernel={kernel} · \"\n",
        "                      f\"{'Spearman' if use_spearman else 'Pearson'} · \"\n",
        "                      f\"ENSO_phase={enso_phase} thr={enso_thr} lag_ENSO={enso_lag}\")\n",
        "\n",
        "        return fig_corr, fig_sig, fig_sens, fig_hist, text_sum, status_msg\n",
        "    except Exception as e:\n",
        "        fig, ax = plt.subplots(); ax.axis('off'); ax.text(0.5,0.5,str(e),ha='center',va='center',color='red')\n",
        "        return fig, fig, fig, fig, f\"❌ {e}\", f\"❌ {e}\"\n",
        "\n",
        "# ========= GWSS: explicación paso a paso para UNA celda (VERSIÓN SIMPLIFICADA) =========\n",
        "def explain_gwss_single_cell(\n",
        "    k: int, lag: int, firms_csv_path: str, years_str: str,\n",
        "    bw_neighbors: int, kernel: str,\n",
        "    use_spearman: bool,\n",
        "    enso_phase: str = \"all\", enso_csv_path: str = None,\n",
        "    enso_thr: float = 0.5, enso_lag: int = 0,\n",
        "    cell_id: str = \"random\", seed: int = 13,\n",
        "):\n",
        "    \"\"\"\n",
        "    Versión eficiente y SIMPLIFICADA.\n",
        "    Ya no usa 'spei_agg', 'fire_metric', 'riskset' o 'balanceo'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from matplotlib.gridspec import GridSpec\n",
        "        from scipy.stats import t as tdist\n",
        "\n",
        "        # 1. Preparar datos de entrada globales (crea /content/gw_input.csv)\n",
        "        _prepare_gw_input_unified(\n",
        "            int(k), int(lag), firms_csv_path, years_str, enso_phase, enso_csv_path,\n",
        "            float(enso_thr), int(enso_lag)\n",
        "        )\n",
        "        df_input = pd.read_csv(\"/content/gw_input.csv\", dtype={\"cell_id\": str})\n",
        "        if df_input.empty:\n",
        "            return {\"error\": \"El DataFrame de entrada está vacío después de aplicar los filtros.\"}\n",
        "\n",
        "        # Leer la lista completa de sitios (para seleccionar la celda focal ANTES de R)\n",
        "        df_sites = pd.read_csv(\"/content/gw_sites.csv\", dtype={\"cell_id\": str})\n",
        "        if df_sites.empty:\n",
        "            return {\"error\": \"No se encontró gw_sites.csv\"}\n",
        "\n",
        "        # 2. Ejecutar el análisis GWSS global con R\n",
        "        df_results_raw = _run_gwss_gwmodel(\n",
        "            bw_neighbors=int(bw_neighbors), kernel=kernel, use_spearman=bool(use_spearman)\n",
        "        )\n",
        "\n",
        "        # 3. Calcular métricas de significancia (p-value, q-value)\n",
        "        df_results_full = _compute_gw_post_metrics(df_results_raw)\n",
        "\n",
        "        # 4. Seleccionar celda focal (de la lista de sitios completa)\n",
        "        unique_cells = df_sites[\"cell_id\"].unique()\n",
        "        if cell_id == \"random\":\n",
        "            rng = np.random.default_rng()\n",
        "            cell_id = rng.choice(unique_cells)\n",
        "        elif cell_id not in unique_cells:\n",
        "            return {\"error\": f\"El cell_id '{cell_id}' no se encontró en la grilla base (gw_sites.csv).\"}\n",
        "\n",
        "        # 5. Extraer resultados FINALES para la celda focal\n",
        "        if cell_id not in df_results_full[\"cell_id\"].values:\n",
        "            return {\"error\": f\"El cell_id '{cell_id}' no tiene resultados en el análisis GWSS (probablemente no tiene datos SPEI válidos).\"}\n",
        "        focal_results = df_results_full[df_results_full[\"cell_id\"] == cell_id].iloc[0]\n",
        "        rho_local = focal_results['corr']\n",
        "        p_local = focal_results['pval']\n",
        "        q_local = focal_results['qval']\n",
        "        t_local = focal_results.get('tstat', np.nan)\n",
        "        df_local = int(focal_results['neff'] - 2)\n",
        "        signif_fdr05 = focal_results['sig_fdr05']\n",
        "\n",
        "        # 6. Recopilar datos de ENTRADA para la visualización\n",
        "        focal_row_site = df_sites[df_sites[\"cell_id\"] == cell_id].iloc[0]\n",
        "        lon_focal, lat_focal = focal_row_site[\"lon\"], focal_row_site[\"lat\"]\n",
        "\n",
        "        df_focal_data = df_input[df_input[\"cell_id\"] == cell_id]\n",
        "        if df_focal_data.empty:\n",
        "            focal_spei_val = np.nan\n",
        "            focal_fire_val = np.nan\n",
        "        else:\n",
        "            focal_spei_val = df_focal_data[\"spei\"].mean()\n",
        "            focal_fire_val = df_focal_data[\"fire\"].mean()\n",
        "\n",
        "\n",
        "        # 6.b) Vecinos y pesos\n",
        "        coords = df_input[[\"lon\", \"lat\"]].to_numpy(float)\n",
        "        from sklearn.neighbors import BallTree\n",
        "        tree = BallTree(coords, metric=\"euclidean\")\n",
        "\n",
        "        # Encontrar el índice del PUNTO DE DATOS MÁS CERCANO a la celda focal\n",
        "        focal_coord_array = np.array([[focal_row_site[\"lon\"], focal_row_site[\"lat\"]]])\n",
        "        dist_to_focal, idx_of_nearest_point = tree.query(focal_coord_array, k=1)\n",
        "\n",
        "        idx_query_point = idx_of_nearest_point[0][0]\n",
        "\n",
        "        bw = min(int(bw_neighbors), len(coords))\n",
        "        dists_deg, idxs = tree.query([coords[idx_query_point]], k=bw)\n",
        "        idxs = idxs[0]\n",
        "        dists_km = dists_deg[0] * 111.0\n",
        "        d_max_km = float(dists_km[-1])\n",
        "\n",
        "        def compute_kernel(d, h, kernel_type):\n",
        "            u = d / max(h, 1e-12)\n",
        "            k = kernel_type.lower()\n",
        "            if k == \"bisquare\":\n",
        "                w = np.where(u < 1.0, (1.0 - u**2) ** 2, 0.0)\n",
        "            elif k == \"gaussian\":\n",
        "                w = np.exp(-0.5 * u**2)\n",
        "            elif k == \"exponential\":\n",
        "                w = np.exp(-u)\n",
        "            elif k == \"tricube\":\n",
        "                w = np.where(u < 1.0, (1.0 - u**3) ** 3, 0.0)\n",
        "            elif k == \"boxcar\":\n",
        "                w = np.where(u < 1.0, 1.0, 0.0)\n",
        "            else:\n",
        "                raise ValueError(f\"Kernel desconocido: {kernel_type}\")\n",
        "            s = w.sum()\n",
        "            return w / (s + 1e-12)\n",
        "\n",
        "        weights = compute_kernel(dists_km, d_max_km, kernel)\n",
        "\n",
        "        df_neighbors = df_input.iloc[idxs].copy()\n",
        "        df_neighbors[\"dist_km\"] = dists_km\n",
        "        df_neighbors[\"weight\"]  = weights\n",
        "\n",
        "        X_neighbors = df_neighbors[\"spei\"].astype(float).values\n",
        "        Y_neighbors = df_neighbors[\"fire\"].astype(float).values\n",
        "        mask = np.isfinite(X_neighbors) & np.isfinite(Y_neighbors)\n",
        "        Xc, Yc, Wc = X_neighbors[mask], Y_neighbors[mask], weights[mask]\n",
        "\n",
        "        if not np.isfinite(t_local) and np.isfinite(rho_local) and np.isfinite(focal_results[\"neff\"]) and focal_results[\"neff\"] >= 3:\n",
        "            n_eff_pairs = int(focal_results[\"neff\"])\n",
        "            num = rho_local * np.sqrt(max(n_eff_pairs - 2, 1))\n",
        "            den = np.sqrt(max(1.0 - rho_local**2, 1e-12))\n",
        "            t_local = float(num / den)\n",
        "        df_local = int(max(int(focal_results[\"neff\"]) - 2, 1))\n",
        "\n",
        "        # 7) FIGURA MULTI-PANEL (A–F)\n",
        "        import matplotlib.pyplot as plt\n",
        "        from matplotlib.gridspec import GridSpec\n",
        "        from scipy.stats import t as tdist\n",
        "\n",
        "        fig = plt.figure(figsize=(16, 10), dpi=120)\n",
        "        gs  = GridSpec(3, 3, figure=fig, hspace=0.35, wspace=0.3)\n",
        "\n",
        "        # Panel A: Mapa vecinos\n",
        "        ax1 = fig.add_subplot(gs[0:2, 0:2])\n",
        "        ax1.set_aspect('equal')\n",
        "        sc = ax1.scatter(\n",
        "            df_neighbors[\"lon\"], df_neighbors[\"lat\"],\n",
        "            c=df_neighbors[\"weight\"], cmap=\"viridis\",\n",
        "            s=30 + 300 * df_neighbors[\"weight\"], alpha=0.75,\n",
        "            edgecolor=\"k\", linewidth=0.4\n",
        "        )\n",
        "        ax1.scatter([lon_focal], [lat_focal], c=\"crimson\", s=300, marker=\"*\",\n",
        "                    edgecolor=\"white\", linewidth=2, zorder=10, label=\"Celda focal\")\n",
        "        plt.colorbar(sc, ax=ax1, label=\"Peso (kernel)\", shrink=0.8)\n",
        "        ax1.set_title(f\"A. Vecinos adaptativos (bw={bw}, {kernel})\", fontsize=12, fontweight=\"bold\")\n",
        "        ax1.set_xlabel(\"Longitud\"); ax1.set_ylabel(\"Latitud\")\n",
        "        ax1.legend(loc=\"upper right\", fontsize=9); ax1.grid(ls=\":\", alpha=0.3)\n",
        "\n",
        "        # Panel B: Scatter ponderado\n",
        "        ax2 = fig.add_subplot(gs[0, 2])\n",
        "        ax2.scatter(Xc, Yc, c=Wc, cmap=\"plasma\", s=30, alpha=0.7, edgecolor=\"k\", linewidth=0.3)\n",
        "        ax2.scatter([focal_spei_val], [focal_fire_val], c=\"red\",\n",
        "                    s=150, marker=\"*\", edgecolor=\"white\", linewidth=1.5, zorder=10, label=\"Focal (prom.)\")\n",
        "        method = \"spearman\" if use_spearman else \"pearson\"\n",
        "        ax2.set_title(f\"B. Correlación ({method})\", fontsize=11, fontweight=\"bold\")\n",
        "        ax2.set_xlabel(f\"SPEI-{k} (mensual, lag={lag})\"); ax2.set_ylabel(f\"Incendios (Cercanía)\")\n",
        "        ax2.text(0.05, 0.95, f\"ρ = {rho_local:.3f}\", transform=ax2.transAxes, fontsize=13, fontweight=\"bold\",\n",
        "                 va=\"top\", bbox=dict(boxstyle=\"round\", facecolor=\"yellow\", alpha=0.7))\n",
        "        ax2.legend(fontsize=8); ax2.grid(ls=\":\", alpha=0.3)\n",
        "        y_min, y_max = Yc.min(), Yc.max()\n",
        "        y_min = max(0.0, y_min)\n",
        "        y_max = min(1.0, y_max)\n",
        "        padding = (y_max - y_min) * 0.1\n",
        "        ax2.set_ylim(y_min - padding - 0.01, y_max + padding + 0.01)\n",
        "\n",
        "        # Panel C: Histograma de pesos\n",
        "        ax3 = fig.add_subplot(gs[1, 2])\n",
        "        ax3.hist(df_neighbors[\"weight\"].values, bins=30, color=\"steelblue\", alpha=0.7, edgecolor=\"k\")\n",
        "        if np.isfinite(focal_fire_val): # Solo mostrar si la celda focal está en los datos\n",
        "            ax3.axvline(df_neighbors[\"weight\"].values[0], color=\"crimson\", ls=\"--\", lw=2, label=\"Vecino más cercano\")\n",
        "        ax3.set_title(\"C. Distribución de pesos\", fontsize=11, fontweight=\"bold\")\n",
        "        ax3.set_xlabel(\"Peso\"); ax3.set_ylabel(\"Frecuencia\")\n",
        "        ax3.legend(fontsize=9); ax3.grid(axis=\"y\", ls=\":\", alpha=0.3)\n",
        "\n",
        "        # Panel D: Distancia vs Peso\n",
        "        ax4 = fig.add_subplot(gs[2, 0])\n",
        "        ax4.scatter(dists_km, weights, s=20, alpha=0.6, c=\"teal\", edgecolor=\"k\", linewidth=0.3)\n",
        "        ax4.axhline(weights[0], color=\"crimson\", ls=\"--\", lw=1.5, alpha=0.7)\n",
        "        ax4.axvline(d_max_km, color=\"orange\", ls=\"--\", lw=1.5, alpha=0.7, label=f\"Bandwidth={d_max_km:.1f} km\")\n",
        "        ax4.set_title(f\"D. Kernel: {kernel}\", fontsize=11, fontweight=\"bold\")\n",
        "        ax4.set_xlabel(\"Distancia (km)\"); ax4.set_ylabel(\"Peso\")\n",
        "        ax4.legend(fontsize=8); ax4.grid(ls=\":\", alpha=0.3)\n",
        "\n",
        "        # Panel E: t-Student\n",
        "        ax5 = fig.add_subplot(gs[2, 1])\n",
        "        if df_local > 0 and np.isfinite(df_local):\n",
        "            x_t = np.linspace(-6, 6, 200)\n",
        "            y_t = tdist.pdf(x_t, df_local)\n",
        "            ax5.plot(x_t, y_t, 'k-', lw=2, label=f\"t({df_local} df)\")\n",
        "            if np.isfinite(t_local):\n",
        "                ax5.axvline(t_local, color=\"red\", lw=2.5, label=f\"t obs = {t_local:.2f}\")\n",
        "            t_crit = float(tdist.ppf(0.975, df_local))\n",
        "            ax5.axvline( t_crit,  color=\"orange\", ls=\"--\", lw=1.5, alpha=0.7)\n",
        "            ax5.axvline(-t_crit,  color=\"orange\", ls=\"--\", lw=1.5, alpha=0.7, label=f\"t crit ±{t_crit:.2f}\")\n",
        "            ax5.fill_between(x_t[x_t >  t_crit], 0, y_t[x_t >  t_crit], color=\"red\", alpha=0.2)\n",
        "            ax5.fill_between(x_t[x_t < -t_crit], 0, y_t[x_t < -t_crit], color=\"red\", alpha=0.2)\n",
        "        ax5.set_title(\"E. Test t-Student\", fontsize=11, fontweight=\"bold\")\n",
        "        ax5.set_xlabel(\"Estadístico t\"); ax5.set_ylabel(\"Densidad\")\n",
        "        ax5.legend(fontsize=7, loc=\"upper right\"); ax5.grid(ls=\":\", alpha=0.3)\n",
        "\n",
        "        # Panel F: Resumen\n",
        "        ax6 = fig.add_subplot(gs[2, 2]); ax6.axis(\"off\")\n",
        "        summary_text = f\"\"\"\n",
        "SIGNIFICANCIA\n",
        "\n",
        "{method.upper()}: ρ={rho_local:.3f}\n",
        "n_eff={int(focal_results['neff']):,} celdas\n",
        "\n",
        "t={t_local:.3f}, df={df_local}\n",
        "p={p_local:.2e}\n",
        "q(FDR)={q_local:.2e}\n",
        "\n",
        "FDR≤0.05: {'✓ SÍ' if signif_fdr05 else '✗ NO'}\n",
        "\"\"\"\n",
        "        ax6.text(0.5, 0.5, summary_text.strip(), transform=ax6.transAxes,\n",
        "                 ha=\"center\", va=\"center\", fontsize=9, family=\"monospace\",\n",
        "                 bbox=dict(boxstyle=\"round,pad=1\",\n",
        "                           facecolor=(\"lightgreen\" if signif_fdr05 else \"lightyellow\"),\n",
        "                           edgecolor=\"black\", linewidth=2, alpha=0.9))\n",
        "        ax6.set_title(\"F. Resumen\", fontsize=11, fontweight=\"bold\")\n",
        "\n",
        "        fig.suptitle(f\"GWSS Celda {cell_id} · TS-{k} lag={lag} · ({lon_focal:.4f}°, {lat_focal:.4f}°)\",\n",
        "                     fontsize=14, fontweight=\"bold\", y=0.98)\n",
        "\n",
        "        csv_out_path = f\"/content/GWSS_neighbors_{cell_id}_TS{k}_lag{lag}_bw{bw}_{kernel}.csv\"\n",
        "        df_neighbors[[\"cell_id\",\"lon\",\"lat\",\"dist_km\",\"weight\",\"spei\",\"fire\"]].to_csv(csv_out_path, index=False)\n",
        "\n",
        "        return {\n",
        "            \"mensaje\": \"Reporte GWSS completo generado\",\n",
        "            \"cell_id\": cell_id,\n",
        "            \"paso1_datos_entrada\": {\n",
        "                \"lon\": lon_focal, \"lat\": lat_focal,\n",
        "                \"spei_value\": float(focal_spei_val),\n",
        "                \"fire_value\": float(focal_fire_val),\n",
        "            },\n",
        "            \"paso2_vecinos\": {\"bw_usado\": bw, \"distancia_max_km\": d_max_km},\n",
        "            \"paso4_correlacion\": {\n",
        "                \"metodo\": method, \"rho_local\": float(rho_local),\n",
        "                \"n_eff_pairs\": int(focal_results[\"neff\"]),\n",
        "                \"t_stat\": float(t_local), \"df\": int(df_local), \"p_value\": float(p_local),\n",
        "            },\n",
        "            \"paso5_significancia\": {\n",
        "                \"q_value\": float(q_local), \"significativo_fdr05\": bool(signif_fdr05),\n",
        "                \"interpretacion\": \"Correlación significativa\" if signif_fdr05 else \"No significativa\",\n",
        "            },\n",
        "            \"figura\": fig,\n",
        "            \"csv_vecinos\": csv_out_path,\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        return {\"error\": f\"Error: {str(e)}\\n\\n{traceback.format_exc()}\"}\n",
        "\n",
        "# ========= Wrapper para UI Gradio =========\n",
        "def explain_gwss_ui(k, lag, firms_csv, years, bw, kernel,\n",
        "                    spearman, enso_phase, enso_csv,\n",
        "                    enso_thr, enso_lag, cell_id_input):\n",
        "    try:\n",
        "        res = explain_gwss_single_cell(\n",
        "            int(k), int(lag), firms_csv, years,\n",
        "            int(bw), kernel, bool(spearman),\n",
        "            enso_phase, enso_csv if enso_phase != \"all\" else None,\n",
        "            float(enso_thr), int(enso_lag),\n",
        "            cell_id=cell_id_input.strip() if cell_id_input.strip() else \"random\"\n",
        "        )\n",
        "\n",
        "        if \"error\" in res:\n",
        "            fig_err, ax = plt.subplots(figsize=(10, 8)); ax.axis(\"off\")\n",
        "            ax.text(0.05, 0.95, f\"❌ Error:\\n\\n{res['error']}\", ha=\"left\", va=\"top\", color=\"red\",\n",
        "                    fontsize=8, family=\"monospace\", bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8))\n",
        "            return fig_err, f\"❌ {res['error']}\"\n",
        "\n",
        "        texto = f\"\"\"\n",
        "GWSS — Análisis de celda única (lógica de distancia)\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "CONFIGURACIÓN\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "Celda: {res['cell_id']} ({res['paso1_datos_entrada']['lon']:.4f}°, {res['paso1_datos_entrada']['lat']:.4f}°)\n",
        "Periodo: {years if years.strip() else \"Todos los años\"}\n",
        "SPEI: k={k}, lag={lag} (mensual)\n",
        "Incendios: Métrica de Cercanía (1 / (dist_km + 1))\n",
        "Kernel: {kernel}, BW={res['paso2_vecinos']['bw_usado']} vecinos ({res['paso2_vecinos']['distancia_max_km']:.1f} km)\n",
        "ENSO: {enso_phase if enso_phase != 'all' else 'sin filtro'}\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "DATOS DE ENTRADA (promedio de la celda focal en el período)\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "SPEI (promedio): {res['paso1_datos_entrada']['spei_value']:.3f}\n",
        "Incendios (Cercanía, promedio): {res['paso1_datos_entrada']['fire_value']:.3f}\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "RESULTADOS\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "Método: {res['paso4_correlacion']['metodo'].upper()}\n",
        "ρ (correlación local): {res['paso4_correlacion']['rho_local']:.4f}\n",
        "n_eff (celdas vecinas): {res['paso4_correlacion']['n_eff_pairs']:,}\n",
        "t: {res['paso4_correlacion']['t_stat']:.3f}, df: {res['paso4_correlacion']['df']}\n",
        "p-value: {res['paso4_correlacion']['p_value']:.2e}\n",
        "q-value (FDR): {res['paso5_significancia']['q_value']:.3e}\n",
        "Significativo FDR≤0.05: {'✓ SÍ' if res['paso5_significancia']['significativo_fdr05'] else '✗ NO'}\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "La figura muestra 6 paneles explicativos. CSV de vecinos en: {res['csv_vecinos']}\n",
        "\"\"\"\n",
        "        return res[\"figura\"], texto.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        fig_err, ax = plt.subplots(figsize=(10, 8)); ax.axis(\"off\")\n",
        "        ax.text(0.05, 0.95, f\"❌ Error en UI:\\n\\n{traceback.format_exc()}\",\n",
        "                ha=\"left\", va=\"top\", color=\"red\", fontsize=7, family=\"monospace\",\n",
        "                bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8))\n",
        "        return fig_err, f\"❌ {str(e)}\"\n",
        "\n",
        "# =================== 14) UI GRADIO ===========================\n",
        "with gr.Blocks(title=\"Suite CHIRPS + MODIS — SPI + SPEI + Análisis\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "### **Hidro-Suite (CHIRPS + MODIS)** — SPI + SPEI + ENSO + FIRMS + GWSS\n",
        "\n",
        "\n",
        "**Requisitos**: EE, Google Drive, R (paquete **SPEI**) y **GWmodel** (R) para GWSS.\n",
        "**Nota**: Todo queda alineado a la grilla **CHIRPS** (0.05°). Archivos largos `SPI_k_month.csv` y `SPEI_k_month.csv` se guardan en sus carpetas de Drive.\n",
        "\"\"\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        # -------------------- TAB 1: SPI + NIFT --------------------\n",
        "        with gr.TabItem(\"SPI + NIFT (CHIRPS)\"):\n",
        "            with gr.Row():\n",
        "                sd_spi = gr.Textbox(\"1985-01-01\", label=\"Inicio (AAAA-MM-DD)\")\n",
        "                ed_spi = gr.Textbox(\"2024-12-31\", label=\"Fin (AAAA-MM-DD)\")\n",
        "            btn_e1 = gr.Button(\"Paso 1 → Exportar PR (CHIRPS)\")\n",
        "            out_e1 = gr.Textbox(label=\"Estado exportación PR\")\n",
        "\n",
        "            k_spi  = gr.Textbox(\"1,3,6,12\", label=\"k (meses)\")\n",
        "            btn_e2 = gr.Button(\"Paso 2 → Calcular SPI (R)\")\n",
        "            out_e2 = gr.Textbox(label=\"Estado SPI\")\n",
        "\n",
        "            with gr.Row():\n",
        "                k_plot  = gr.Dropdown([\"1\",\"3\",\"6\",\"12\"], value=\"3\", label=\"k\")\n",
        "                date_sp = gr.Textbox(\"2021-01\", label=\"Mes (AAAA-MM)\")\n",
        "            btn_e3 = gr.Button(\"Paso 3 → Rasterizar + Plot SPI\")\n",
        "            fig_spi = gr.Plot(label=\"Mapa SPI\")\n",
        "            msg_sp  = gr.Textbox(label=\"Estado SPI map\")\n",
        "\n",
        "            gr.Markdown(\"### Índice **NIFT** y parámetros P1–P10\")\n",
        "\n",
        "            k_nift = gr.Textbox(\"1,3,6,12\", label=\"k para NIFT\")\n",
        "\n",
        "            with gr.Accordion(\"Ponderaciones NIFT (por defecto: Brasil Neto)\", open=False):\n",
        "                preset_w = gr.Dropdown(\n",
        "                    [\"Brasil Neto (2024)\", \"Uniforme (10% c/u)\", \"Personalizado\"],\n",
        "                    value=\"Brasil Neto (2024)\", label=\"Preset\"\n",
        "                )\n",
        "                with gr.Row():\n",
        "                    wP1 = gr.Slider(0.0, 1.0, value=0.125, step=0.001, label=\"P1 (N)\")\n",
        "                    wP2 = gr.Slider(0.0, 1.0, value=0.125, step=0.001, label=\"P2 (MDI)\")\n",
        "                    wP3 = gr.Slider(0.0, 1.0, value=0.009, step=0.001, label=\"P3 (%Mild)\")\n",
        "                    wP4 = gr.Slider(0.0, 1.0, value=0.034, step=0.001, label=\"P4 (%Mod)\")\n",
        "                    wP5 = gr.Slider(0.0, 1.0, value=0.071, step=0.001, label=\"P5 (%Sev)\")\n",
        "                with gr.Row():\n",
        "                    wP6 = gr.Slider(0.0, 1.0, value=0.136, step=0.001, label=\"P6 (%Ext)\")\n",
        "                    wP7 = gr.Slider(0.0, 1.0, value=0.100, step=0.001, label=\"P7 (BT) NEG\")\n",
        "                    wP8 = gr.Slider(0.0, 1.0, value=0.075, step=0.001, label=\"P8 (DT)\")\n",
        "                    wP9 = gr.Slider(0.0, 1.0, value=0.075, step=0.001, label=\"P9 (ST)\")\n",
        "                    wP10= gr.Slider(0.0, 1.0, value=0.250, step=0.001, label=\"P10 (Prec) NEG\")\n",
        "\n",
        "                renorm = gr.Checkbox(True, label=\"Renormalizar a suma=1.0\")\n",
        "                sum_box = gr.Markdown(\"**Suma actual:** 1.000\")\n",
        "\n",
        "            def _preset_vals(name: str):\n",
        "                if name.startswith(\"Brasil\"): w = DEFAULT_WEIGHTS_BRASIL_NETO\n",
        "                elif name.startswith(\"Uniforme\"): w = DEFAULT_WEIGHTS_UNIFORME\n",
        "                else: return None\n",
        "                return [w[f\"P{i}\"] for i in range(1, 11)]\n",
        "            def _weights_from_ui(preset, p1,p2,p3,p4,p5,p6,p7,p8,p9,p10):\n",
        "                if preset.startswith(\"Brasil\"): return DEFAULT_WEIGHTS_BRASIL_NETO\n",
        "                if preset.startswith(\"Uniforme\"): return DEFAULT_WEIGHTS_UNIFORME\n",
        "                return {f\"P{i}\": v for i, v in enumerate([p1,p2,p3,p4,p5,p6,p7,p8,p9,p10], start=1)}\n",
        "            def _sum_text(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10):\n",
        "                s = float(p1+p2+p3+p4+p5+p6+p7+p8+p9+p10); return f\"**Suma actual:** {s:.3f}\"\n",
        "            def _apply_preset(preset):\n",
        "                vals = _preset_vals(preset);\n",
        "                if vals is None: return (gr.update(),)*10\n",
        "                return tuple(gr.update(value=v) for v in vals)\n",
        "            preset_w.change(_apply_preset, [preset_w], [wP1,wP2,wP3,wP4,wP5,wP6,wP7,wP8,wP9,wP10])\n",
        "            for s in (wP1,wP2,wP3,wP4,wP5,wP6,wP7,wP8,wP9,wP10):\n",
        "                s.change(_sum_text, [wP1,wP2,wP3,wP4,wP5,wP6,wP7,wP8,wP9,wP10], [sum_box])\n",
        "            def _do_nift_with_weights(ks, preset, p1,p2,p3,p4,p5,p6,p7,p8,p9,p10, renormalize):\n",
        "                w = _weights_from_ui(preset, p1,p2,p3,p4,p5,p6,p7,p8,p9,p10)\n",
        "                return compute_nift(ks, run_min=3, weights=w, renormalize=bool(renormalize))\n",
        "            btn_e4 = gr.Button(\"Paso 4 → Calcular NIFT\")\n",
        "            out_e4 = gr.Textbox(label=\"Estado NIFT\")\n",
        "            def _exp_pr(a,b): return export_chirps_pr(a,b)\n",
        "            def _do_spi(ks):   return compute_spi_from_csv(ks)\n",
        "            def _plot_spi(k, d): return plot_index_map(DRIVE_DIR_SPI, \"SPI\", k, d, \"spi\", f\"SPI-{k} · {d}\", DRIVE_DIR_SPI)\n",
        "            btn_e1.click(_exp_pr,  [sd_spi, ed_spi], out_e1)\n",
        "            btn_e2.click(_do_spi,  [k_spi], out_e2)\n",
        "            btn_e3.click(_plot_spi, [k_plot, date_sp], [fig_spi, msg_sp])\n",
        "            btn_e4.click(_do_nift_with_weights, inputs=[k_nift, preset_w, wP1,wP2,wP3,wP4,wP5,wP6,wP7,wP8,wP9,wP10, renorm], outputs=[out_e4])\n",
        "            gr.Markdown(\"### Mapas NIFT y parámetros P1–P10\")\n",
        "            with gr.Row():\n",
        "                k_plot_all_spi = gr.Textbox(\"1,3,6,12\", label=\"k para mapas (coma)\")\n",
        "                norm_chk_spi   = gr.Checkbox(False, label=\"Normalizar P1–P10\")\n",
        "            plot_btn_spi  = gr.Button(\"Paso 5A → Mapas NIFT + P1–P10\")\n",
        "            param_plot_spi = gr.Plot(label=\"Parámetros P1–P10 (completo)\")\n",
        "            nift_plot_spi  = gr.Plot(label=\"Mapas NIFT\")\n",
        "            plot_msg_spi   = gr.Textbox(label=\"Estado mapas (SPI/NIFT)\")\n",
        "            gr.Markdown(\"### Figuras compactas (4 imágenes: conjuntos G1–G4)\")\n",
        "            plot_grp_btn_spi = gr.Button(\"Paso 5B → Figuras compactas (G1–G4)\")\n",
        "            with gr.Row():\n",
        "                g1_plot_spi = gr.Plot(label=\"Grupo 1\"); g2_plot_spi = gr.Plot(label=\"Grupo 2\")\n",
        "            with gr.Row():\n",
        "                g3_plot_spi = gr.Plot(label=\"Grupo 3\"); g4_plot_spi = gr.Plot(label=\"Grupo 4\")\n",
        "            plot_grp_msg_spi = gr.Textbox(label=\"Estado grupos (SPI/NIFT)\")\n",
        "            plot_btn_spi.click(_spi_plot_maps, inputs=[k_plot_all_spi, norm_chk_spi], outputs=[param_plot_spi, nift_plot_spi, plot_msg_spi])\n",
        "            plot_grp_btn_spi.click(_spi_plot_maps_grouped, inputs=[k_plot_all_spi, norm_chk_spi], outputs=[g1_plot_spi, g2_plot_spi, g3_plot_spi, g4_plot_spi, plot_grp_msg_spi])\n",
        "            gr.Markdown(\"### Exportar NIFT a GeoTIFF (uno por k)\")\n",
        "            btn_nift_tif  = gr.Button(\"Paso 5C → Exportar NIFT a GeoTIFFs\")\n",
        "            out_nift_tif  = gr.Textbox(label=\"Estado export GeoTIFF (NIFT)\")\n",
        "            btn_nift_tif.click(lambda ks: export_nift_geotiffs(ks, DRIVE_DIR_SPI), inputs=[k_plot_all_spi], outputs=[out_nift_tif])\n",
        "\n",
        "        # -------------------- TAB 2: SPEI + Análisis --------------\n",
        "        with gr.TabItem(\"SPEI (CHIRPS − MODIS) + Análisis\"):\n",
        "            with gr.Row():\n",
        "                sd_sp = gr.Textbox(\"2001-01-01\", label=\"Inicio (AAAA-MM-DD)\")\n",
        "                ed_sp = gr.Textbox(\"2024-12-31\", label=\"Fin (AAAA-MM-DD)\")\n",
        "            btn_s1 = gr.Button(\"Paso 1 → Exportar PR − PET (mensual)\")\n",
        "            out_s1 = gr.Textbox(label=\"Estado exportación balance\")\n",
        "\n",
        "            k_spei = gr.Textbox(\"1,3,6,12\", label=\"k (meses)\")\n",
        "            btn_s2 = gr.Button(\"Paso 2 → Calcular SPEI (R)\")\n",
        "            out_s2 = gr.Textbox(label=\"Estado SPEI\")\n",
        "\n",
        "            with gr.Row():\n",
        "                k_plot2  = gr.Dropdown([\"1\",\"3\",\"6\",\"12\"], value=\"3\", label=\"k\")\n",
        "                date_sp2 = gr.Textbox(\"2021-01\", label=\"Mes (AAAA-MM)\")\n",
        "            btn_s3 = gr.Button(\"Paso 3 → Rasterizar + Plot SPEI\")\n",
        "            fig_spei = gr.Plot(label=\"Mapa SPEI\")\n",
        "            msg_spei = gr.Textbox(label=\"Estado SPEI map\")\n",
        "\n",
        "            gr.Markdown(\"#### Validación SPEI (GEE CSIC vs Local)\")\n",
        "            val_k_list = gr.Textbox(\"1,3,6,12\", label=\"k (coma)\")\n",
        "            with gr.Row():\n",
        "                val_sd = gr.Textbox(\"1985-01-01\", label=\"Inicio gráfico (opcional)\")\n",
        "                val_ed = gr.Textbox(\"2024-12-31\", label=\"Fin gráfico (opcional)\")\n",
        "            val_btn  = gr.Button(\"Comparar series\")\n",
        "            val_plot = gr.Plot(label=\"GEE vs Local\")\n",
        "            val_msg  = gr.Textbox(label=\"Estado validación\")\n",
        "\n",
        "            gr.Markdown(\"#### Comparar SPEI (Local) vs ENSO mensual (ONI)\")\n",
        "            with gr.Row():\n",
        "                enso_csv_in = gr.Textbox(f\"{DRIVE_DIR_SPEI}/ENSO.csv\", label=\"Ruta ENSO.csv (ONI 3-meses)\")\n",
        "                enso_lag_dd = gr.Dropdown([str(i) for i in range(-3,4)], value=\"-1\", label=\"Lag ENSO (meses, −3…+3)\")\n",
        "            with gr.Row():\n",
        "                enso_sd = gr.Textbox(\"1985-01-01\", label=\"Inicio gráfico (opcional)\")\n",
        "                enso_ed = gr.Textbox(\"2024-12-31\", label=\"Fin gráfico (opcional)\")\n",
        "            enso_btn    = gr.Button(\"Comparar con ENSO mensual\")\n",
        "            enso_plot   = gr.Plot(label=\"SPEI (Local) vs ENSO (ONI)\")\n",
        "            enso_msg    = gr.Textbox(label=\"Estado ENSO\")\n",
        "\n",
        "            gr.Markdown(\"#### FIRMS mensual (VIIRS 375 m)\")\n",
        "            with gr.Row():\n",
        "                f_start = gr.Textbox(\"2014-01-01\", label=\"Inicio (AAAA-MM-DD)\")\n",
        "                f_end   = gr.Textbox(\"2024-12-31\", label=\"Fin (AAAA-MM-DD)\")\n",
        "                firms_btn = gr.Button(\"Exportar FIRMS → CSV (grilla CHIRPS)\")\n",
        "            firms_out = gr.Textbox(label=\"Estado FIRMS (export)\")\n",
        "\n",
        "            gr.Markdown(\"**Visualizar incendios (presencia) desde CSV**\")\n",
        "            with gr.Row():\n",
        "                fires_csv_map   = gr.Textbox(f\"{DRIVE_DIR_SPEI}/FIRMS_MONTH_20140101_20241231.csv\", label=\"Ruta CSV FIRMS_MONTH…\")\n",
        "            with gr.Row():\n",
        "                fires_start_map = gr.Textbox(\"2014-01\", label=\"Mes inicio (AAAA-MM)\")\n",
        "                fires_end_map   = gr.Textbox(\"2024-12\", label=\"Mes fin (AAAA-MM)\")\n",
        "            fires_map_btn  = gr.Button(\"Mostrar mapa (CSV vs GEE)\")\n",
        "            fires_map_plot = gr.Plot(label=\"Mapa incendios (comparación)\")\n",
        "            fires_map_msg  = gr.Textbox(label=\"Estado mapa\")\n",
        "\n",
        "            # ---- GWSS PERÍODO COMPLETO ----\n",
        "            gr.Markdown(\"#### Correlación GWSS para Período Completo — Lógica de Distancia\")\n",
        "            gr.Markdown(\"Genera un mapa de correlación local para todo el período seleccionado (ej. 2014-2024).\")\n",
        "            with gr.Row():\n",
        "                gw_firms_csv = gr.Textbox(value=f\"{DRIVE_DIR_SPEI}/FIRMS_MONTH_20140101_20241231.csv\", label=\"Ruta CSV FIRMS_MONTH…\")\n",
        "            with gr.Row():\n",
        "                gw_k      = gr.Dropdown([\"1\",\"3\",\"6\",\"12\"], value=\"3\", label=\"k SPEI\")\n",
        "                gw_lag    = gr.Dropdown([\"0\",\"1\"], value=\"0\", label=\"lag SPEI (meses)\")\n",
        "                gw_years  = gr.Textbox(value=\"2014-2024\", label=\"Período: AAAA-AAAA | AAAA | AAAA,AAAA (vacío=todo)\")\n",
        "            with gr.Row():\n",
        "                gw_bw         = gr.Slider(minimum=10, maximum=1500, value=1080, step=10, label=\"BW adaptativo (nº obs. celda-mes)\")\n",
        "                gw_kernel     = gr.Dropdown([\"bisquare\",\"gaussian\",\"exponential\",\"tricube\",\"boxcar\"], value=\"bisquare\", label=\"Kernel\")\n",
        "                gw_spearman   = gr.Checkbox(value=False, label=\"Usar Spearman (rango)\")\n",
        "                gw_nan_strategy = gr.Dropdown([\"mask\",\"impute0\"], value=\"mask\", label=\"NaN locales: 'mask' o 'impute0'\")\n",
        "\n",
        "            gr.Markdown(\"**Filtro ENSO opcional dentro del periodo:**\")\n",
        "            with gr.Row():\n",
        "                gw_enso_csv = gr.Textbox(f\"{DRIVE_DIR_SPEI}/ENSO.csv\", label=\"Ruta ENSO.csv\")\n",
        "                gw_enso_phase = gr.Dropdown([\"all\",\"nino\",\"neutral\",\"nina\"], value=\"all\", label=\"Fase ENSO (all = sin filtro)\")\n",
        "                gw_enso_thr   = gr.Slider(0.3, 1.0, value=0.5, step=0.1, label=\"Umbral |ONI| (fase ENSO)\")\n",
        "                gw_enso_lag   = gr.Dropdown([str(i) for i in range(-3,4)], value=\"0\", label=\"Lag ENSO (meses, −3…+3)\")\n",
        "\n",
        "            gw_btn   = gr.Button(\"Calcular GWSS para el Período Seleccionado\")\n",
        "            gw_plot  = gr.Plot(label=\"Mapa de correlación GW (ρ local) - Período Completo\")\n",
        "            gw_msg   = gr.Textbox(label=\"Estado / Resumen breve\")\n",
        "            gw_sig_plot  = gr.Plot(label=\"Significancia local (−log10 q) - Período Completo\")\n",
        "            gw_sens_plot = gr.Plot(label=\"Sensibilidad local (pendiente) - Período Completo\")\n",
        "            gw_hist_plot = gr.Plot(label=\"Distribución de ρ locales (hist + ECDF) - Período Completo\")\n",
        "            gw_stats_box = gr.Textbox(label=\"Resumen estadístico (mapa GW)\", lines=4)\n",
        "\n",
        "            # --- Wiring SPEI tab ---\n",
        "            def _exp_bal(a,b): return export_pr_minus_pet(a,b)\n",
        "            def _do_spei(ks):  return compute_spei_from_csv(ks)\n",
        "            def _plot_spei(k, d): return plot_index_map(DRIVE_DIR_SPEI, \"SPEI\", k, d, \"spei\", f\"SPEI-{k} · {d}\", DRIVE_DIR_SPEI)\n",
        "            btn_s1.click(_exp_bal,  [sd_sp, ed_sp], out_s1)\n",
        "            btn_s2.click(_do_spei,  [k_spei], out_s2)\n",
        "            btn_s3.click(_plot_spei, [k_plot2, date_sp2], [fig_spei, msg_spei])\n",
        "            val_btn.click(lambda ks, sd, ed: _plot_spei_vs_spei_clc(ks, sd, ed), inputs=[val_k_list, val_sd, val_ed], outputs=[val_plot, val_msg])\n",
        "            enso_btn.click(lambda ks, sd, ed, p, lag: _plot_spei_vs_enso(ks, sd, ed, p, int(lag)), inputs=[val_k_list, enso_sd, enso_ed, enso_csv_in, enso_lag_dd], outputs=[enso_plot, enso_msg])\n",
        "            firms_btn.click(_export_firms_monthly, [f_start, f_end], firms_out)\n",
        "            fires_map_btn.click(_plot_fires, inputs=[fires_csv_map, fires_start_map, fires_end_map], outputs=[fires_map_plot, fires_map_msg])\n",
        "\n",
        "            gw_btn.click(\n",
        "                lambda fcsv, k, lag, yrs, bw, ker, sp, ns, epath, phase, thr, elag:\n",
        "                    gw_correlation_unified_plus( # Llama a la misma función que antes\n",
        "                        int(k), int(lag), fcsv, yrs, int(bw), ker, bool(sp),\n",
        "                        ns, epath, phase, float(thr), int(elag)\n",
        "                    ),\n",
        "                inputs=[gw_firms_csv, gw_k, gw_lag, gw_years,\n",
        "                        gw_bw, gw_kernel, gw_spearman, gw_nan_strategy,\n",
        "                        gw_enso_csv, gw_enso_phase, gw_enso_thr, gw_enso_lag],\n",
        "                outputs=[gw_plot, gw_sig_plot, gw_sens_plot, gw_hist_plot, gw_stats_box, gw_msg]\n",
        "            )\n",
        "\n",
        "            # ---- Panel anual ----\n",
        "            gr.Markdown(\"#### Panel anual GWSS (Año por Año) — 4 figuras compactas\")\n",
        "            with gr.Row():\n",
        "                yrs_start = gr.Number(value=2014, label=\"Año inicio\")\n",
        "                yrs_end   = gr.Number(value=2024, label=\"Año fin\")\n",
        "                ncols_v   = gr.Slider(minimum=1, maximum=4, value=4, step=1, label=\"Columnas del panel\")\n",
        "            bw_panel_anual = gr.Slider(minimum=10, maximum=500, value=120, step=5, label=\"BW Anual (nº obs. celda-mes)\")\n",
        "            btn_yearpan = gr.Button(\"Generar Panel Anual (Año por Año)\")\n",
        "            yr_corr = gr.Plot(label=\"Panel: Correlación local por año\")\n",
        "            yr_sig  = gr.Plot(label=\"Panel: Significancia local por año\")\n",
        "            yr_slo  = gr.Plot(label=\"Panel: Pendiente local por año\")\n",
        "            yr_sum  = gr.Plot(label=\"Panel: Resumen (ECDF + Histo)\")\n",
        "            yr_msg  = gr.Textbox(label=\"Estado GWSS anual\")\n",
        "            def _run_yearpanels_ui(\n",
        "                firms_csv: str, k: int, lag: int,\n",
        "                years_start: int, years_end: int, ncols: int,\n",
        "                bw_neighbors: int, kernel: str, use_spearman: bool,\n",
        "                nan_strategy: str,\n",
        "                enso_csv: str, enso_phase: str, enso_thr: float, enso_lag: int,\n",
        "            ):\n",
        "                return gw_correlation_yearly_panels(\n",
        "                    k=int(k), lag=int(lag), firms_csv_path=firms_csv,\n",
        "                    start_year=int(years_start), end_year=int(years_end),\n",
        "                    bw_neighbors=int(bw_neighbors), kernel=kernel, use_spearman=bool(use_spearman),\n",
        "                    nan_strategy=nan_strategy,\n",
        "                    enso_csv_path=(enso_csv or f\"{DRIVE_DIR_SPEI}/ENSO.csv\"),\n",
        "                    enso_phase=enso_phase, enso_thr=float(enso_thr), enso_lag=int(enso_lag),\n",
        "                    ncols=int(ncols)\n",
        "                )\n",
        "            btn_yearpan.click(\n",
        "                _run_yearpanels_ui,\n",
        "                inputs=[\n",
        "                    gw_firms_csv, gw_k, gw_lag,\n",
        "                    yrs_start, yrs_end, ncols_v,\n",
        "                    bw_panel_anual,\n",
        "                    gw_kernel, gw_spearman,\n",
        "                    gw_nan_strategy,\n",
        "                    gw_enso_csv, gw_enso_phase, gw_enso_thr, gw_enso_lag,\n",
        "                ],\n",
        "                outputs=[yr_corr, yr_sig, yr_slo, yr_sum, yr_msg]\n",
        "            )\n",
        "\n",
        "        # -------------------- TAB 3: GWSS Ejemplo Celda Única ----\n",
        "        with gr.TabItem(\"GWSS — Celda ejemplo\"):\n",
        "            gr.Markdown(\"\"\"\n",
        "#### Ejemplo paso a paso GWSS para UNA celda (Lógica de Distancia)\n",
        "Genera un reporte detallado (ρ, vecinos, pesos, t-value, p-value, q-value) para una celda aleatoria o indicada.\n",
        "\"\"\")\n",
        "            with gr.Row():\n",
        "                k_ex = gr.Dropdown([\"1\",\"3\",\"6\",\"12\"], value=\"3\", label=\"k (meses)\")\n",
        "                lag_ex = gr.Slider(-6, 6, value=1, step=1, label=\"lag SPEI\")\n",
        "                years_ex = gr.Textbox(\"2015-2023\", label=\"Años (AAAA-AAAA)\")\n",
        "            with gr.Row():\n",
        "                bw_ex = gr.Slider(10, 1500, value=400, step=10, label=\"BW (vecinos celda-mes)\")\n",
        "                kernel_ex = gr.Dropdown([\"bisquare\",\"gaussian\",\"exponential\",\"tricube\",\"boxcar\"], value=\"exponential\", label=\"Kernel\")\n",
        "                spearman_ex = gr.Checkbox(True, label=\"Spearman\")\n",
        "            gr.Markdown(\"**Filtros ENSO:**\")\n",
        "            with gr.Row():\n",
        "                enso_phase_ex = gr.Dropdown([\"all\",\"nino\",\"neutral\",\"nina\"], value=\"all\", label=\"Fase ENSO\")\n",
        "                enso_csv_ex = gr.Textbox(f\"{DRIVE_DIR_SPEI}/ENSO.csv\", label=\"ENSO.csv\")\n",
        "                enso_thr_ex = gr.Slider(0.3, 1.0, value=0.5, step=0.1, label=\"Umbral |ONI|\")\n",
        "                enso_lag_ex = gr.Dropdown([str(i) for i in range(-3,4)], value=\"0\", label=\"Lag ENSO\")\n",
        "            firms_csv_ex = gr.Textbox(f\"{DRIVE_DIR_SPEI}/FIRMS_MONTH_20140101_20241231.csv\", label=\"FIRMS mensual (CSV)\")\n",
        "            cell_id_ex = gr.Textbox(\"\", label=\"cell_id (vacío=aleatorio)\")\n",
        "            btn_explain = gr.Button(\"Ejecutar análisis (celda única)\")\n",
        "            fig_explain = gr.Plot(label=\"Análisis GWSS completo (6 paneles)\")\n",
        "            txt_explain = gr.Textbox(label=\"Reporte detallado\", lines=15)\n",
        "            gr.Markdown(\"\"\"\n",
        "**Interpretación:**\n",
        "- **ρ (rho)**: coeficiente de correlación local (Pearson o Spearman)\n",
        "- **t**: estadístico t de Student para probar H₀: ρ=0\n",
        "- **df**: grados de libertad efectivos (n_eff - 2)\n",
        "- **p**: p-value bilateral\n",
        "- **q**: q-value ajustado por FDR (Benjamini-Hochberg)\n",
        "- **FDR≤0.05**: si es significativa tras corrección por comparaciones múltiples\n",
        "\"\"\")\n",
        "            btn_explain.click(\n",
        "                explain_gwss_ui,\n",
        "                inputs=[k_ex, lag_ex, firms_csv_ex, years_ex,\n",
        "                        bw_ex, kernel_ex, spearman_ex,\n",
        "                        enso_phase_ex, enso_csv_ex, enso_thr_ex, enso_lag_ex,\n",
        "                        cell_id_ex],\n",
        "                outputs=[fig_explain, txt_explain]\n",
        "            )\n",
        "    gr.Markdown(\n",
        "        \"> **Tips**\\n\"\n",
        "        \"> • MODIS **PET** proviene de *MOD16A2* (8-días); se agrega a **mm/mes** aplicando escala 0.1.\\n\"\n",
        "        \"> • Alineación 1:1 a CHIRPS: sin reproyección en rasterización local.\\n\"\n",
        "        \"> • `SPI_k_month.csv` y `SPEI_k_month.csv` quedan en tus carpetas de Drive.\\n\"\n",
        "        \"> • La pestaña **GWSS — Celda ejemplo** genera reportes detallados para una única celda.\"\n",
        "    )\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5PHmvxRXrRvGQxGd95kkF"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}